# # performance
model_params:
  hidden_size: 256
  num_hidden_layers: 4        # 保持轻量 backbone，靠大 batch 吃满 GPU
  num_attention_heads: 8
  intermediate_size: 1024
  max_position_embeddings: 512
  dropout_rate: 0.1           # 降低正则，配合大 batch 加快收敛
  diffusion_steps: 4
  lambda_div: 0             # 多樣性懲罰適度，避免過度壓制高頻合法項

token_params:
  vocab_size: 166
  pad_token_id: 0
  cls_token_id: 163
  sep_token_id: 164
  mask_token_id: 165

training_params:
  lr: 0.0001                   # 按 8x 大 batch 放大学习率，提升吞吐
  weight_decay: 0.01
  batch_size: 2048            
  num_epochs: 100              # 减少总轮次，加快整体训练周期
  early_stop: 10
  eval_interval: 5            # 更频繁验证以触发早停
  num_workers: 32             # 高并发加载，避免 CPU 成瓶颈

evaluation_params:
  beam_size: 50               # 略降 beam，配合大 batch 保持速度
  batch_size: 128             # 利用大显存批量推理，加速评估
  diffusion_steps: 4          # 减少去噪步数，缩短推理耗时
  sampling_temperature: 0.1   # 略高溫度，配合 trie 約束提升有效樣本率
  topk_sampling: 10
  use_prefix_trie: true       # 啟用前綴約束，防止拼接非法 code
  alpha_diversity: 0.5
  diversity_topk: 10   # 或 0 关闭去重


