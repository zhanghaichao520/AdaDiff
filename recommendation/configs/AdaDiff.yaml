model_params:
  hidden_size: 256                 # BERT hidden dim
  num_hidden_layers: 4             # Encoder depth
  num_attention_heads: 8           # Attention heads
  intermediate_size: 1024          # FFN hidden dim
  max_position_embeddings: 128     # Positional limit
  dropout_rate: 0.1                # Dropout for attention/hidden
  diffusion_steps: 4               # Training forward diffusion steps (hierarchical mask)
  layer_mask_weights: [1.2, 1.1, 1.0, 1.0]  # Coarse->fine mask scaling per depth
  history_mask_prob: 0.2          # Hist token mask prob during training

training_params:
  lr: 0.001                      # Optimizer LR
  weight_decay: 0.01               # L2 weight decay
  batch_size: 1024                 # Train batch size
  num_epochs: 100                  # Max training epochs
  early_stop: 5                   # Early stop patience (epochs)
  eval_interval: 5                 # Eval every N epochs
  num_workers: 32                  # Dataloader workers

evaluation_params:
  diffusion_steps: 6               # Reverse denoising steps at inference
  beam_size: 50                    # Beam width
  top_k: 100                       # Sampling top-k (>= beam*2 to avoid auto bump)
  temperature: 0.8                 # Sampling temperature
  batch_size: 128                  # Eval batch size
  lambda_div: 0                  # Diversity penalty for rerank/final logits
  lambda_div_sampling: 28       # Base energy guidance scale (annealed over time)
  time_anneal_guidance: false       # Use time decay for lambda_div_sampling
  use_prefix_trie: true            # Enforce prefix-tree validity
  alpha_diversity: 0.5             # Alpha-NDCG weighting
  diversity_topk: 0               # Enforce distinct categories in top-k beams (0 to disable)
  debug_logit_stats: true         # If true, log logits/guidance stats for scale sanity-check

# evaluation_params:
#   diffusion_steps: 6               # Reverse denoising steps at inference
#   beam_size: 50                    # Beam width
#   top_k: 100                       # Sampling top-k (>= beam*2 to avoid auto bump)
#   temperature: 0.8                 # Sampling temperature
#   batch_size: 128                  # Eval batch size
#   lambda_div: 0                  # Diversity penalty for rerank/final logits
#   lambda_div_sampling: 0       # Base energy guidance scale (annealed over time)
#   time_anneal_guidance: false       # Use time decay for lambda_div_sampling
#   use_prefix_trie: true            # Enforce prefix-tree validity
#   alpha_diversity: 0.5             # Alpha-NDCG weighting
#   diversity_topk: 0               # Enforce distinct categories in top-k beams (0 to disable)
#   debug_logit_stats: true         # If true, log logits/guidance stats for scale sanity-check

