# RPG 模型獨有的架構參數
model_params:
  max_len: 50
  n_embd: 448
  n_layer: 2
  n_head: 4
  n_inner: 1024
  activation_function: gelu_new
  # --- 关键修正 1: 降低 Dropout ---
  resid_pdrop: 0.1
  embd_pdrop: 0.1
  attn_pdrop: 0.1
  # ---
  layer_norm_epsilon: 1e-12
  initializer_range: 0.02
  temperature: 0.07

# ✨ 範例：如果 RPG 需要不同的 batch size，可以在這裡覆蓋 base.yaml
training_params:
  batch_size: 1024
  # --- 关键修正 2: 提高学习率 ---
  lr: 0.0001  # (从 1e-5 提高到 1e-4)
  # ---
  early_stop: 15

evaluation_params:
  batch_size: 512