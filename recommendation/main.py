# main.py

import argparse
import logging
import torch
import torch.optim as optim
import os

from dataset import GenRecDataset
from dataloader import GenRecDataLoader
from trainer import train_one_epoch, evaluate
from utils import load_and_process_config, setup_logging, set_seed, get_model_class


def main():
    # 1) å‚æ•°ï¼šç°åœ¨åªéœ€è¦æ¨¡å‹ã€æ•°æ®é›†å’Œé‡åŒ–æ–¹æ³•
    parser = argparse.ArgumentParser(description="GenRec Universal Training Pipeline")
    parser.add_argument('--model', type=str, required=True, help='æ¨¡å‹åç§° (e.g., TIGER)')
    parser.add_argument('--dataset', type=str, required=True, help='æ•°æ®é›†åç§° (e.g., Beauty)')
    parser.add_argument('--quant_method', type=str, required=True, choices=['rkmeans', 'rvq', 'rqvae'],
                        help='é‡åŒ–æ–¹æ³• (rkmeans / rvq / rqvae)')
    args = parser.parse_args()

    # 2) åŠ è½½å¹¶å¤„ç†æ‰€æœ‰é…ç½®
    config = load_and_process_config(args.model, args.dataset, args.quant_method)

    # 3) åˆå§‹åŒ–
    setup_logging(config['log_path'])
    set_seed(config['training_params']['seed'])
    logging.info(f"Configuration loaded for {args.model} on {args.dataset} with {args.quant_method}.")

    # 4) è®¾å¤‡
    device = torch.device(config['training_params']['device'] if torch.cuda.is_available() else 'cpu')
    logging.info(f"Using device: {device}")

    # 5) æ•°æ®é›†ï¼ˆç›´æ¥ç”¨ utils é‡Œè‡ªåŠ¨æ‹¼å¥½çš„ jsonl è·¯å¾„ï¼‰
    train_dataset = GenRecDataset(
        dataset_path=config['train_json'],
        code_path=config['code_path'], mode='train', max_len=config['model_params']['max_len'],
        vocab_sizes=config['vocab_sizes'], bases=config['bases']
    )
    validation_dataset = GenRecDataset(
        dataset_path=config['valid_json'],
        code_path=config['code_path'], mode='evaluation', max_len=config['model_params']['max_len'],
        vocab_sizes=config['vocab_sizes'], bases=config['bases']
    )
    test_dataset = GenRecDataset(
        dataset_path=config['test_json'],
        code_path=config['code_path'], mode='evaluation', max_len=config['model_params']['max_len'],
        vocab_sizes=config['vocab_sizes'], bases=config['bases']
    )
    train_loader = GenRecDataLoader(train_dataset, batch_size=config['training_params']['batch_size'], shuffle=True)
    validation_loader = GenRecDataLoader(validation_dataset, batch_size=config['training_params']['infer_size'], shuffle=False)
    test_loader = GenRecDataLoader(test_dataset, batch_size=config['training_params']['infer_size'], shuffle=False)

    # 6) æ¨¡å‹ä¸ä¼˜åŒ–å™¨
    # 6) âœ¨ æ¨¡å‹èˆ‡å„ªåŒ–å™¨ (å‹•æ…‹è¼‰å…¥) âœ¨
    logging.info(f"Dynamically loading model: {args.model}")
    # é€éè¼”åŠ©å‡½æ•¸ï¼Œç”¨å­—ä¸²ç²å–æ¨¡å‹ Class
    ModelClass = get_model_class(args.model)
    # å¯¦ä¾‹åŒ–æ¨¡å‹
    model = ModelClass(config)
    
    # èª¿æ•´è©åµŒå…¥å±¤å¤§å° (ä½¿ç”¨ self.t5)
    model.t5.resize_token_embeddings(config['token_params']['vocab_size'])
    model.to(device)
    logging.info(model.n_parameters)
    optimizer = optim.Adam(model.parameters(), lr=config['training_params']['lr'])


    # 7) è¨“ç·´-è©•ä¼°å¾ªç’°
    best_ndcg = 0.0
    early_stop_counter = 0
    
    # --- æ–°å¢ï¼šç”¨æ–¼å„²å­˜æœ€ä½³çµæœçš„è®Šæ•¸ ---
    best_epoch = 0
    best_val_results = None
    best_test_results = None
    # ------------------------------------

    for epoch in range(config['training_params']['num_epochs']):
        logging.info(f"--- Epoch {epoch + 1}/{config['training_params']['num_epochs']} ---")
        train_loss = train_one_epoch(model, train_loader, optimizer, device)
        logging.info(f"Training loss: {train_loss:.4f}")

        val_recalls, val_ndcgs = evaluate(
            model, validation_loader, config['evaluation_params']['topk_list'],
            config['evaluation_params']['beam_size'], config['code_len'], device
        )
        logging.info(f"Validation Recalls: {val_recalls}")
        logging.info(f"Validation NDCGs: {val_ndcgs}")

        current_ndcg = val_ndcgs.get('NDCG@20', 0)
        if current_ndcg > best_ndcg:
            best_ndcg = current_ndcg
            early_stop_counter = 0
            logging.info(f"ğŸš€ New best NDCG@20 on validation: {best_ndcg:.4f}")

            test_recalls, test_ndcgs = evaluate(
                model, test_loader, config['evaluation_params']['topk_list'],
                config['evaluation_params']['beam_size'], config['code_len'], device
            )
            logging.info(f"Test Recalls: {test_recalls}")
            logging.info(f"Test NDCGs: {test_ndcgs}")

            # --- æ–°å¢ï¼šæ›´æ–°æœ€ä½³çµæœ ---
            best_epoch = epoch + 1
            best_val_results = {'recalls': val_recalls, 'ndcgs': val_ndcgs}
            best_test_results = {'recalls': test_recalls, 'ndcgs': test_ndcgs}
            # --------------------------

            torch.save(model.state_dict(), config['save_path'])
            logging.info(f"Best model saved to {config['save_path']}")
        else:
            early_stop_counter += 1
            logging.info(f"No improvement in NDCG@20. Early stop counter: {early_stop_counter}/{config['training_params']['early_stop']}")
            if early_stop_counter >= config['training_params']['early_stop']:
                logging.info("Early stopping triggered.")
                break
    
    # --- æ–°å¢ï¼šåœ¨è¨“ç·´çµæŸå¾Œæ‰“å°æœ€çµ‚ç¸½çµ ---
    logging.info("="*50)
    logging.info("ğŸ Training Finished!")
    if best_test_results:
        logging.info(f"ğŸ† Best performance found at Epoch {best_epoch}")
        logging.info(f"  - Best Validation Results: Recalls={best_val_results['recalls']}, NDCGs={best_val_results['ndcgs']}")
        logging.info(f"  - Corresponding Test Results: Recalls={best_test_results['recalls']}, NDCGs={best_test_results['ndcgs']}")
        logging.info(f"  - Best model checkpoint saved at: {config['save_path']}")
    else:
        logging.info("No improvement was observed during training. No model was saved.")
    logging.info("="*50)
    # ---------------------------------------


if __name__ == "__main__":
    main()
