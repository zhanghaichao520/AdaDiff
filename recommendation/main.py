import argparse
import logging
import torch
import torch.optim as optim
import os
import pprint
from typing import Optional # âœ… (æ–°å¢) å¯¼å…¥ Optional

# âœ… 1. å¾ torch.utils.data ç›´æ¥å°å…¥ DataLoader
from torch.utils.data import DataLoader 
from dataset import GenRecDataset, item2code
from tokenizer import get_tokenizer       
from trainer import train_one_epoch, evaluate
from utils import load_and_process_config, setup_logging, set_seed, get_model_class

import sys
from pathlib import Path

# è·å–é¡¹ç›®æ ¹ç›®å½•è·¯å¾„ï¼ˆmain.py çš„ä¸Šä¸€çº§ç›®å½•çš„ä¸Šä¸€çº§ï¼‰
root = Path(__file__).resolve().parent  # recommendation/
root_parent = root.parent               # é¡¹ç›®æ ¹ç›®å½•

if str(root_parent) not in sys.path:
    sys.path.insert(0, str(root_parent))

from recommendation.models.generation.prefix_tree import Trie, build_trie_from_codebook


def main():
    # === 1. è§£æå‘½ä»¤åˆ—åƒæ•¸ ===
    parser = argparse.ArgumentParser(description="GenRec Universal Training Pipeline")
    parser.add_argument('--model', type=str, required=True, help='æ¨¡å‹åç¨± (e.g., TIGER, GPT2, RPG)')
    parser.add_argument('--dataset', type=str, required=True, help='æ•°æ®é›†åç¨± (e.g., Beauty)')
    parser.add_argument('--quant_method', type=str, required=True, choices=['rkmeans', 'rvq', 'rqvae', 'opq', 'pq', 'vqvae', 'mm_rqvae'], help='é‡åŒ–æ–¹æ³•')
    parser.add_argument('--embedding_modality', type=str, default='text', choices=['text', 'image', 'fused', 'lfused'], help='é‡åŒ–æ¨¡æ€ç±»å‹ï¼Œå¯¹åº”ä¸åŒçš„ codebook (é»˜è®¤ text)')

    
    # âœ… (å·²ç§»é™¤) åˆ é™¤äº† --no_trie å‘½ä»¤è¡Œå‚æ•°
    
    args = parser.parse_args()

    # === 2. è¼‰å…¥ä¸¦è™•ç†è¨­å®šæª” ===
    config = load_and_process_config(
        args.model, 
        args.dataset, 
        args.quant_method,
        embedding_modality=args.embedding_modality
    )


    # === 3. åˆå§‹åŒ– (æ—¥èªŒ, éš¨æ©Ÿç¨®å­) ===
    setup_logging(config['log_path'])
    set_seed(config['training_params']['seed'])
    logging.info(f"Configuration loaded for {args.model} on {args.dataset} with {args.quant_method}.")
    logging.info("=" * 50)
    config_str = pprint.pformat(config)
    logging.info("\n" + config_str)
    logging.info("=" * 50)

    # === 4. è¨­å®šè¨­å‚™ ===
    device = torch.device(config['training_params']['device'] if torch.cuda.is_available() else 'cpu')
    logging.info(f"Using device: {device}")
    num_workers = config['training_params'].get('num_workers', 4)
    
    # === 5. âœ… (é¡ºåºè°ƒæ•´) è½½å…¥ item_to_code æ˜ å°„ ===
    # (å¿…é¡»åœ¨åˆ›å»ºæ¨¡å‹ä¹‹å‰å®Œæˆï¼Œå› ä¸ºTrieä¾èµ–å®ƒ)
    logging.info("Loading item to code mapping...")
    item_to_code_map, _ = item2code(
        config['code_path'],
        config['vocab_sizes'],
        config['bases']
    )
    logging.info(f"Item to code map loaded. Total items mapped: {len(item_to_code_map)}")

    # === 6. âœ… (ä¿®æ”¹) æ ¹æ® config æ„å»ºå‰ç¼€æ ‘ ===
    prefix_trie: Optional[Trie] = None
    
    # æ£€æŸ¥ config['evaluation_params'] ä¸­çš„ 'use_prefix_trie' æ ‡å¿—
    # é»˜è®¤å€¼ä¸º False (å¦‚æœæ‚¨å¸Œæœ›é»˜è®¤ä¸ä½¿ç”¨)
    use_trie = config.get('evaluation_params', {}).get('use_prefix_trie', False) 
    
    if use_trie and build_trie_from_codebook is not None:
        logging.info("Building Prefix Trie (enabled in config)...")
        
        # è·å–æ‰€æœ‰åˆæ³•çš„ code token åºåˆ—
        all_token_sequences = list(item_to_code_map.values())
        
        # è·å– EOS token ID
        eos_token_id = config['token_params']['eos_token_id']
        
        prefix_trie = build_trie_from_codebook(
            token_sequences=all_token_sequences,
            eos_token_id=eos_token_id
        )
    elif use_trie:
        logging.warning("Config requested Prefix Trie, but 'utils.prefix_trie' module was not found.")
    else:
        logging.info("Prefix Trie is DISABLED (default or as per config).")


    # === 7. âœ… (ä¿®æ”¹) åˆ›å»ºæ¨¡å‹ (å°†Trieæ³¨å…¥) ===
    logging.info(f"Dynamically loading model: {args.model}")
    ModelClass = get_model_class(args.model)
    
    # âœ… (ä¿®æ”¹) å°† config å’Œ prefix_trie (å¯èƒ½æ˜¯ None) ä¼ é€’ç»™æ¨¡å‹
    #    (æˆ‘ä»¬å‡è®¾ ModelClass çš„ __init__ æ¥å— prefix_trie=None)
    model = ModelClass(config, prefix_trie=prefix_trie) 
    
    model.to(device)
    logging.info(model.n_parameters)
    logging.info("=" * 50)
    optimizer = optim.Adam(model.parameters(), lr=float(config['training_params']['lr']))

    # === 8. (é¡ºåºè°ƒæ•´) åˆå§‹åŒ–æ¨¡å‹ä¸“å±çš„ Tokenizer ===
    logging.info(f"Initializing tokenizer for model: {args.model}")
    tokenizer_collate_fn = get_tokenizer(
        model_name=args.model,
        config=config,
        item_to_code_map=item_to_code_map
    )
    logging.info("Tokenizer initialized.")

    # === 9. (é¡ºåºè°ƒæ•´) å‰µå»ºæ•¸æ“šé›†èˆ‡ DataLoader ===
    logging.info("Creating Datasets...")
    train_dataset = GenRecDataset(config=config, mode='train')
    validation_dataset = GenRecDataset(config=config, mode='valid')
    test_dataset = GenRecDataset(config=config, mode='test')

    logging.info("Creating DataLoaders...")
    
    is_gpu_training = (torch.cuda.is_available() and num_workers > 0)
    loader_kwargs = {
        "num_workers": num_workers,
        "collate_fn": tokenizer_collate_fn, # å‚³å…¥ tokenizer
        "pin_memory": is_gpu_training,
        "persistent_workers": is_gpu_training if num_workers > 0 else False
    }

    train_loader = DataLoader(
        train_dataset,
        batch_size=config['training_params']['batch_size'],
        shuffle=True, 
        **loader_kwargs
    )
    validation_loader = DataLoader(
        validation_dataset,
        batch_size=config['evaluation_params']['batch_size'],
        shuffle=False, 
        **loader_kwargs
    )
    test_loader = DataLoader(
        test_dataset,
        batch_size=config['evaluation_params']['batch_size'],
        shuffle=False, 
        **loader_kwargs
    )

    # === 10. (é¡ºåºè°ƒæ•´) è®­ç»ƒ-è¯„ä¼°å¾ªç¯ (å·²ä¿®æ”¹) ===
    best_ndcg = 0.0
    early_stop_counter = 0
    best_epoch = 0
    best_val_results = None
    best_test_results = None
    
    eval_interval = config['training_params'].get('eval_interval', 1) 
    logging.info(f"Evaluation interval set to: {eval_interval} epoch(s)")

    for epoch in range(config['training_params']['num_epochs']):
        epoch_num = epoch + 1 
        logging.info(f"--- Epoch {epoch_num}/{config['training_params']['num_epochs']} ---")
        
        # --- è®­ç»ƒ ---
        train_loss = train_one_epoch(model, train_loader, optimizer, device)
        logging.info(f"Training loss: {train_loss:.4f}")

        # --- è¯„ä¼° (æ ¹æ® eval_interval) ---
        if epoch_num % eval_interval == 0:
            logging.info(f"--- Evaluating at Epoch {epoch_num} ---")
            val_results = evaluate(
                model,
                validation_loader,
                config['evaluation_params']['topk_list'],
                device
            )
            logging.info(f"Validation Results: {val_results}")

            current_ndcg = val_results.get('NDCG@10', val_results.get('NDCG@20', 0.0))

            # --- æ£€æŸ¥æ€§èƒ½æå‡å’Œ Early Stopping ---
            if current_ndcg > best_ndcg:
                best_ndcg = current_ndcg
                early_stop_counter = 0 
                logging.info(f"ğŸš€ New best NDCG on validation: {best_ndcg:.4f} at Epoch {epoch_num}")

                test_results = evaluate(
                    model,
                    test_loader,
                    config['evaluation_params']['topk_list'],
                    device
                )
                logging.info(f"Test Results: {test_results}")

                best_epoch = epoch_num
                best_val_results = val_results
                best_test_results = test_results

                torch.save(model.state_dict(), config['save_path'])
                logging.info(f"Best model saved to {config['save_path']}")
            
            else:
                early_stop_counter += eval_interval 
                logging.info(f"No improvement since Epoch {best_epoch}. Early stop counter: {early_stop_counter}/{config['training_params']['early_stop'] * eval_interval}")
                if early_stop_counter >= config['training_params']['early_stop'] * eval_interval:
                    logging.info("Early stopping triggered.")
                    break
        else:
             logging.info(f"Skipping evaluation for Epoch {epoch_num}.")

    # === 11. (é¡ºåºè°ƒæ•´) è¨“ç·´çµæŸç¸½çµ ===
    logging.info("="*50)
    logging.info("ğŸ Training Finished!")
    if best_test_results:
        logging.info(f"ğŸ† Best performance found at Epoch {best_epoch}")
        logging.info(f"  - Best Validation Results: {best_val_results}")
        logging.info(f"  - Corresponding Test Results: {best_test_results}")
    else:
        logging.info("No improvement was observed.")
    logging.info("="*50)


if __name__ == "__main__":
    main()