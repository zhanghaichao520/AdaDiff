import argparse
import logging
import torch
import torch.optim as optim
import os
import pprint

# âœ… 1. å¾ torch.utils.data ç›´æ¥å°å…¥ DataLoader
from torch.utils.data import DataLoader 
from dataset import GenRecDataset, item2code
# from dataloader import GenRecDataLoader  # <-- å·²åˆªé™¤
from tokenizer import get_tokenizer       
from trainer import train_one_epoch, evaluate
from utils import load_and_process_config, setup_logging, set_seed, get_model_class

def main():
    # === 1. è§£æå‘½ä»¤åˆ—åƒæ•¸ ===
    parser = argparse.ArgumentParser(description="GenRec Universal Training Pipeline")
    parser.add_argument('--model', type=str, required=True, help='æ¨¡å‹åç¨± (e.g., TIGER, GPT2, RPG)')
    parser.add_argument('--dataset', type=str, required=True, help='æ•°æ®é›†åç¨± (e.g., Beauty)')
    parser.add_argument('--quant_method', type=str, required=True, choices=['rkmeans', 'rvq', 'rqvae', 'opq', 'pq', 'vqvae'],
                        help='é‡åŒ–æ–¹æ³•')
    args = parser.parse_args()

    # === 2. è¼‰å…¥ä¸¦è™•ç†è¨­å®šæª” ===
    config = load_and_process_config(args.model, args.dataset, args.quant_method)

    # === 3. åˆå§‹åŒ– (æ—¥èªŒ, éš¨æ©Ÿç¨®å­) ===
    setup_logging(config['log_path'])
    set_seed(config['training_params']['seed'])
    logging.info(f"Configuration loaded for {args.model} on {args.dataset} with {args.quant_method}.")
    logging.info("=" * 50)
    config_str = pprint.pformat(config)
    logging.info("\n" + config_str)
    logging.info("=" * 50)

    # === 4. è¨­å®šè¨­å‚™ ===
    device = torch.device(config['training_params']['device'] if torch.cuda.is_available() else 'cpu')
    logging.info(f"Using device: {device}")
    num_workers = config['training_params'].get('num_workers', 4)

    # === 5. å‰µå»ºæ¨¡å‹èˆ‡å„ªåŒ–å™¨ ===
    logging.info(f"Dynamically loading model: {args.model}")
    ModelClass = get_model_class(args.model)
    model = ModelClass(config)
    model.to(device)
    logging.info(model.n_parameters)
    logging.info("=" * 50)
    optimizer = optim.Adam(model.parameters(), lr=float(config['training_params']['lr']))

    # === 6. è¼‰å…¥ item_to_code æ˜ å°„ ===
    logging.info("Loading item to code mapping...")
    item_to_code_map, _ = item2code(
        config['code_path'],
        config['vocab_sizes'],
        config['bases']
    )
    logging.info(f"Item to code map loaded. Total items mapped: {len(item_to_code_map)}")

    # === 7. åˆå§‹åŒ–æ¨¡å‹å°ˆå±¬çš„ Tokenizer ===
    logging.info(f"Initializing tokenizer for model: {args.model}")
    tokenizer_collate_fn = get_tokenizer(
        model_name=args.model,
        config=config,
        item_to_code_map=item_to_code_map
    )
    logging.info("Tokenizer initialized.")

    # === 8. å‰µå»ºæ•¸æ“šé›†èˆ‡ DataLoader ===
    logging.info("Creating Datasets...")
    train_dataset = GenRecDataset(config=config, mode='train')
    validation_dataset = GenRecDataset(config=config, mode='valid')
    test_dataset = GenRecDataset(config=config, mode='test')

    logging.info("Creating DataLoaders...")
    
    # âœ… 2. æº–å‚™é€šç”¨çš„ DataLoader åƒæ•¸
    is_gpu_training = (torch.cuda.is_available() and num_workers > 0)
    loader_kwargs = {
        "num_workers": num_workers,
        "collate_fn": tokenizer_collate_fn, # å‚³å…¥ tokenizer
        "pin_memory": is_gpu_training,
        "persistent_workers": is_gpu_training if num_workers > 0 else False
    }

    # âœ… 3. ç›´æ¥ä½¿ç”¨ PyTorch å®˜æ–¹çš„ DataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['training_params']['batch_size'],
        shuffle=True, 
        **loader_kwargs
    )
    validation_loader = DataLoader(
        validation_dataset,
        batch_size=config['evaluation_params']['batch_size'],
        shuffle=False, 
        **loader_kwargs
    )
    test_loader = DataLoader(
        test_dataset,
        batch_size=config['evaluation_params']['batch_size'],
        shuffle=False, 
        **loader_kwargs
    )

    # === 9. è®­ç»ƒ-è¯„ä¼°å¾ªç¯ (å·²ä¿®æ”¹) ===
    best_ndcg = 0.0
    early_stop_counter = 0
    best_epoch = 0
    best_val_results = None
    best_test_results = None
    
    # ä»é…ç½®ä¸­è·å–è¯„ä¼°é—´éš”
    eval_interval = config['training_params'].get('eval_interval', 1) # é»˜è®¤ä¸º 1 (æ…¢é€Ÿæ¨¡å¼)
    logging.info(f"Evaluation interval set to: {eval_interval} epoch(s)")

    for epoch in range(config['training_params']['num_epochs']):
        epoch_num = epoch + 1 # å½“å‰ epoch ç¼–å· (ä» 1 å¼€å§‹)
        logging.info(f"--- Epoch {epoch_num}/{config['training_params']['num_epochs']} ---")
        
        # --- è®­ç»ƒ ---
        train_loss = train_one_epoch(model, train_loader, optimizer, device)
        logging.info(f"Training loss: {train_loss:.4f}")

        # --- è¯„ä¼° (æ ¹æ® eval_interval) ---
        # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾è¯„ä¼°çš„ epoch
        if epoch_num % eval_interval == 0:
            logging.info(f"--- Evaluating at Epoch {epoch_num} ---")
            val_results = evaluate(
                model,
                validation_loader,
                config['evaluation_params']['topk_list'],
                device
            )
            logging.info(f"Validation Results: {val_results}")

            current_ndcg = val_results.get('NDCG@10', val_results.get('NDCG@20', 0.0))

            # --- æ£€æŸ¥æ€§èƒ½æå‡å’Œ Early Stopping ---
            if current_ndcg > best_ndcg:
                best_ndcg = current_ndcg
                early_stop_counter = 0 # é‡ç½®è®¡æ•°å™¨
                logging.info(f"ğŸš€ New best NDCG on validation: {best_ndcg:.4f} at Epoch {epoch_num}")

                # --- åªæœ‰åœ¨éªŒè¯é›†æ€§èƒ½æå‡æ—¶ï¼Œæ‰è¯„ä¼°æµ‹è¯•é›† ---
                test_results = evaluate(
                    model,
                    test_loader,
                    config['evaluation_params']['topk_list'],
                    device
                )
                logging.info(f"Test Results: {test_results}")

                # æ›´æ–°æœ€ä½³ç»“æœè®°å½•
                best_epoch = epoch_num
                best_val_results = val_results
                best_test_results = test_results

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                torch.save(model.state_dict(), config['save_path'])
                logging.info(f"Best model saved to {config['save_path']}")
            
            else:
                # éªŒè¯é›†æ€§èƒ½æ²¡æœ‰æå‡
                early_stop_counter += eval_interval # <--- æ³¨æ„ï¼šæ¯æ¬¡æ£€æŸ¥æ—¶å¢åŠ  interval çš„å€¼
                logging.info(f"No improvement since Epoch {best_epoch}. Early stop counter: {early_stop_counter}/{config['training_params']['early_stop'] * eval_interval}")
                # <--- ä¿®æ”¹ Early Stopping æ¡ä»¶ï¼šå½“ç´¯è®¡æœªæå‡çš„ epoch æ•°ï¼ˆè€ƒè™‘äº† intervalï¼‰è¶…è¿‡é˜ˆå€¼æ—¶åœæ­¢
                if early_stop_counter >= config['training_params']['early_stop'] * eval_interval:
                    logging.info("Early stopping triggered.")
                    break
        else:
             # å¦‚æœä¸æ˜¯è¯„ä¼° epochï¼Œåªæ‰“å°è®­ç»ƒæŸå¤±ä¿¡æ¯
             logging.info(f"Skipping evaluation for Epoch {epoch_num}.")

    # === 10. è¨“ç·´çµæŸç¸½çµ ===
    logging.info("="*50)
    logging.info("ğŸ Training Finished!")
    if best_test_results:
        logging.info(f"ğŸ† Best performance found at Epoch {best_epoch}")
        logging.info(f"  - Best Validation Results: {best_val_results}")
        logging.info(f"  - Corresponding Test Results: {best_test_results}")
    else:
        logging.info("No improvement was observed.")
    logging.info("="*50)


if __name__ == "__main__":
    main()