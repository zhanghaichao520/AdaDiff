# utils.py
# -*- coding: utf-8 -*-

import os
import sys
import yaml
import numpy as np
import random
import torch
import logging
from pathlib import Path
import importlib
from collections.abc import Mapping
from collections import Counter
import json

VALID_QUANT_METHODS = {"rkmeans", "rvq", "rqvae", "opq", "pq", 'vqvae', 'mm_rqvae'}

def _ensure_dir_exists(dir_path: Path):
    """ç¡®ä¿ç›®å½•å­˜åœ¨"""
    if dir_path:
        dir_path.mkdir(parents=True, exist_ok=True)

def _recursive_update(base_dict: dict, new_dict: dict) -> dict:
    """
    éè¿´åœ°æ›´æ–°å­—å…¸ã€‚
    å¦‚æœ new_dict ä¸­çš„éµåœ¨ base_dict ä¸­ä¹Ÿå­˜åœ¨ä¸”å°æ‡‰çš„å€¼éƒ½æ˜¯å­—å…¸ï¼Œ
    å‰‡éè¿´åœ°åˆä½µå®ƒå€‘ï¼Œå¦å‰‡ç›´æ¥ç”¨ new_dict çš„å€¼è¦†è“‹ base_dict çš„å€¼ã€‚
    """
    for key, value in new_dict.items():
        if isinstance(value, Mapping) and key in base_dict and isinstance(base_dict[key], Mapping):
            base_dict[key] = _recursive_update(base_dict[key], value)
        else:
            base_dict[key] = value
    return base_dict


def _load_quant_details(path: str, quant_method: str) -> dict:
    """
    å¾æŒ‡å®šçš„ YAML æª”æ¡ˆä¸­ï¼Œæ ¹æ“š quant_method è¼‰å…¥å°æ‡‰çš„åƒæ•¸ã€‚
    """
    if not Path(path).is_file():
        raise FileNotFoundError(f"[Config] æ ¹æ“šç´„å®šï¼Œæœªæ‰¾åˆ°é‡åŒ–è¨­å®šæª”: {path}")
    
    with open(path, 'r') as f:
        cfg = yaml.safe_load(f)
        
    if quant_method not in cfg or 'model_params' not in cfg[quant_method]:
        raise ValueError(f"[Config] åœ¨ {path} ä¸­ç¼ºå°‘ '{quant_method}.model_params' ç¯€é»")
    
    mp = cfg[quant_method]['model_params']
    required_keys = ['codebook_size', 'num_levels']
    if not all(key in mp for key in required_keys):
         raise ValueError(f"[Config] åœ¨ {path} çš„ model_params ä¸­ç¼ºå°‘ 'codebook_size' æˆ– 'num_levels'")

    return mp


def load_and_process_config(model_name: str, dataset_name: str, quant_method: str, embedding_modality: str = 'text') -> dict:
    """
    é€šç”¨é…ç½®åŠ è½½å™¨ (V6 - æ”¯æ´ base.yaml ç¹¼æ‰¿èˆ‡è¦†è“‹)ã€‚
    """
    # === 1. âœ… é—œéµæ”¹å‹• 2ï¼šä¾åºè¼‰å…¥ base å’Œ model-specific è¨­å®šæª” ===
    # è¼‰å…¥åŸºç¤è¨­å®šæª”
    base_config_path = Path("configs/base.yaml")
    if not base_config_path.is_file():
        raise FileNotFoundError(f"åŸºç¤è¨­å®šæª”æœªæ‰¾åˆ°: {base_config_path}")
    with open(base_config_path, 'r') as f:
        config = yaml.safe_load(f)

    # è¼‰å…¥ç‰¹å®šæ¨¡å‹è¨­å®šæª”
    model_config_path = Path(f"configs/{model_name}.yaml")
    if not model_config_path.is_file():
        raise FileNotFoundError(f"æ¨¡å‹é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {model_config_path}")
    with open(model_config_path, 'r') as f:
        model_config = yaml.safe_load(f)
        
    # âœ¨ ä½¿ç”¨éè¿´æ›´æ–°ï¼Œè®“ model_config è¦†è“‹ base_config
    config = _recursive_update(config, model_config)

    # === å¾ŒçºŒæµç¨‹å®Œå…¨ä¸è®Šï¼Œå®ƒå€‘ç¾åœ¨æ“ä½œçš„æ˜¯å·²ç¶“åˆä½µå¥½çš„ config ===
    
    config['model_name'] = model_name
    config['dataset_name'] = dataset_name
    config['quant_method'] = quant_method.lower()
    config['embedding_modality'] = embedding_modality.lower()
    
    if config['quant_method'] not in VALID_QUANT_METHODS:
        raise ValueError(f"ä¸æ”¯æŒçš„é‡åŒ–æ–¹æ³•: {quant_method}ã€‚å¯é€‰: {VALID_QUANT_METHODS}")

    # 2. ç¨ç«‹åœ°è¼‰å…¥é‡åŒ–è¨­å®šæª”
    quant_config_path = Path(f"../quantization/configs/{quant_method}_config.yaml")
    quant_details = _load_quant_details(quant_config_path, config['quant_method'])
    
    # === 3. æ ¼å¼åŒ–å’Œæ´¾ç”Ÿã€Œæ•¸æ“šã€èˆ‡ã€Œè¼¸å‡ºã€çš„è·¯å¾‘ ===
    # é€™éƒ¨åˆ†çš„è·¯å¾‘æ¨¡æ¿ä»ç„¶ä¾†è‡ª TIGER.yamlï¼Œæ˜¯åˆç†çš„ï¼Œå› ç‚ºå®ƒå®šç¾©äº†æ•¸æ“šå­˜æ”¾æ ¼å¼
    # 3. æ ¼å¼åŒ–è·¯å¾‘
    paths = config['paths']
    # âœ… å°‡ model_name åŠ å…¥å­—å…¸
    format_args = {
        'dataset_name': dataset_name, 
        'quant_method': config['quant_method'], 
        'model_name': model_name
    }
    dataset_root = Path(paths['dataset_root'].format(**format_args))
    output_root = Path(paths['output_root'].format(**format_args))

        # === 4. è‡ªåŠ¨æ„é€  codebook è·¯å¾„ ===
    dataset_root = Path(f"../datasets/{dataset_name}")
    codebook_dir = dataset_root / "codebooks"

    mod_tag = embedding_modality.lower()
    quant_tag = config['quant_method'].lower()

    # ä¸¥æ ¼åŒ¹é…æŒ‡å®šæ¨¡æ€å’Œé‡åŒ–æ–¹æ³•
    codebook_path = codebook_dir / f"{dataset_name}.{mod_tag}.{quant_tag}.npy"

    if not codebook_path.exists():
        raise FileNotFoundError(
            f"[FATAL] æœªæ‰¾åˆ°æŒ‡å®šæ¨¡æ€ '{mod_tag}' çš„ codebook æ–‡ä»¶ï¼\n"
            f"æœŸæœ›è·¯å¾„: {codebook_path}\n"
            f"è¯·ç¡®è®¤è·¯å¾„åŠæ–‡ä»¶åä¸ä¿å­˜æ—¶ä¸€è‡´ã€‚"
        )

    config['code_path'] = str(codebook_path)
    logging.info(f"ğŸ“¦ [Config] æˆåŠŸåŠ è½½ Codebook: {config['code_path']}")

    config['log_path'] = output_root / "training.log"
    config['save_path'] = output_root / "best_model.pth"
    config['train_json'] = dataset_root / f"{dataset_name}.train.jsonl"
    config['valid_json'] = dataset_root / f"{dataset_name}.valid.jsonl"
    config['test_json'] = dataset_root / f"{dataset_name}.test.jsonl"
    _ensure_dir_exists(output_root)

    # === 4. æ ¹æ“šè¼‰å…¥çš„é‡åŒ–ç´°ç¯€ï¼Œè¨ˆç®—è©è¡¨åƒæ•¸ ===
    K = int(quant_details['codebook_size'])
    num_semantic_levels = int(quant_details['num_levels'])
    has_dup_layer = quant_details.get('has_dup_layer', True) 
    
    config['codebook_size'] = K
    config['num_semantic_levels'] = num_semantic_levels

    # === 5. æ ¡éªŒ codebook æª”æ¡ˆ ===
    if not Path(config['code_path']).is_file():
        raise FileNotFoundError(f"[FATAL] æœªæ‰¾åˆ° codebook: {config['code_path']}")
    codes_arr = np.load(config['code_path'], allow_pickle=True)
    codes_mat = np.vstack(codes_arr) if codes_arr.dtype == object else codes_arr
    
    expected_code_len = num_semantic_levels + 1 if has_dup_layer else num_semantic_levels
    config['code_len'] = expected_code_len

    if codes_mat.ndim != 2 or codes_mat.shape[1] != expected_code_len:
        raise ValueError(f"[FATAL] Codebook {config['code_path']} çš„æœŸæœ›å½¢çŠ¶ç‚º (N, {expected_code_len})ï¼Œå¯¦éš›ç‚º {codes_mat.shape}")

    # === 6. è¨ˆç®—æœ€çµ‚è©è¡¨åƒæ•¸ ===
    if has_dup_layer:
        dup_max = int(codes_mat[:, -1].max()) if codes_mat.size > 0 else 0
        dup_vocab_size = dup_max + 1
        config['dup_vocab_size'] = dup_vocab_size
        vocab_sizes = [K] * num_semantic_levels + [dup_vocab_size]
    else:
        vocab_sizes = [K] * num_semantic_levels
    config['vocab_sizes'] = vocab_sizes

    bases = np.cumsum([0] + vocab_sizes[:-1]).tolist()
    config['bases'] = [int(b) for b in bases]

    # === 6. å®šç¾©ç‰¹æ®Š Token (çµ±ä¸€æ”¾ç½®åœ¨è©è¡¨çš„å°¾ç«¯ï¼Œé¿å…èˆ‡ Code è¡çª) ===
    base_vocab = sum(config['vocab_sizes'])  # èªç¾© token æ•¸é‡
    # è¦ç¯„ï¼šPAD å›ºå®šç‚º 0ï¼Œèªç¾© token å…¨éƒ¨åç§» +1ï¼Œç‰¹æ®Š token è¿½åŠ åœ¨è©è¡¨å°¾éƒ¨
    pad_id = 0
    mask_id = base_vocab + 1
    cls_id = base_vocab + 2
    sep_id = base_vocab + 3
    eos_id = base_vocab + 4  # ä¿ç•™ EOS
    vocab_size = eos_id + 1  # ID èŒƒåœ 0..eos_id

    config['token_params'] = {
        'pad_token_id': pad_id,
        'cls_token_id': cls_id,
        'sep_token_id': sep_id,
        'mask_token_id': mask_id,
        'eos_token_id': eos_id,
        'vocab_size': vocab_size
    }
    
    return config


# --- æ—¥å¿—å’Œéšæœºç§å­å‡½æ•°ä¿æŒä¸å˜ ---
def setup_logging(log_path: Path):
    """é…ç½®æ—¥å¿—è®°å½•å™¨"""
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    if logger.hasHandlers():
        logger.handlers.clear()
    fmt = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

    sh = logging.StreamHandler(sys.stdout)
    sh.setFormatter(fmt)
    logger.addHandler(sh)

    fh = logging.FileHandler(str(log_path), mode='a', encoding='utf-8')
    fh.setFormatter(fmt)
    logger.addHandler(fh)

def get_model_class(model_name: str):
    """
    å¢å¼ºç‰ˆï¼šæ ¹æ®æ¨¡å‹åç§°å­—ç¬¦ä¸²ï¼ŒåŠ¨æ€åœ°ä» 'models' ç›®å½•åŠå…¶æ‰€æœ‰å­ç›®å½•ä¸­
    æœç´¢å¹¶å¯¼å…¥å¯¹åº”çš„æ¨¡å‹ç±»ã€‚

    Args:
        model_name (str): æ¨¡å‹çš„åç§° (e.g., "TIGER")ã€‚
                          çº¦å®šï¼šæ–‡ä»¶åå’Œç±»åéƒ½åº”ä¸ model_name ä¸€è‡´ (TIGER.py, class TIGER)ã€‚
    """
    models_root_dir = "models"
    model_file_name = f"{model_name}.py"
    model_module_path = None

    # os.walk ä¼šéå†æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶å¤¹å’Œæ–‡ä»¶
    for root, dirs, files in os.walk(models_root_dir):
        if model_file_name in files:
            # æ‰¾åˆ°äº†æ–‡ä»¶ï¼ç°åœ¨æ„å»º Python çš„å¯¼å…¥è·¯å¾„
            # ä¾‹å¦‚, root = "models/encoder_decoder"
            
            # 1. å°†æ–‡ä»¶ç³»ç»Ÿè·¯å¾„ ('/') æ›¿æ¢ä¸º Python å¯¼å…¥è·¯å¾„ ('.')
            # "models/encoder_decoder" -> "models.encoder_decoder"
            base_path = root.replace(os.sep, '.')
            
            # 2. æ‹¼æ¥æˆæœ€ç»ˆçš„æ¨¡å—è·¯å¾„
            # "models.encoder_decoder.TIGER"
            model_module_path = f"{base_path}.{model_name}"
            break # æ‰¾åˆ°åç«‹åˆ»åœæ­¢æœç´¢

    # å¦‚æœéå†å®Œéƒ½æ²¡æ‰¾åˆ°æ–‡ä»¶
    if not model_module_path:
        raise ImportError(
            f"é”™è¯¯ï¼šæ— æ³•åœ¨ '{models_root_dir}' ç›®å½•æˆ–å…¶ä»»ä½•å­ç›®å½•ä¸­æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ '{model_file_name}'ã€‚\n"
            f"è¯·æ£€æŸ¥ä½ çš„æ–‡ä»¶ç»“æ„å’Œ --model å‚æ•°æ˜¯å¦æ­£ç¡®ã€‚"
        )

    try:
        # ä½¿ç”¨åŠ¨æ€æ„å»ºçš„è·¯å¾„æ¥å¯¼å…¥æ¨¡å—
        logging.info(f"Found model module at: {model_module_path}")
        model_module = importlib.import_module(model_module_path)
        # ä»æ¨¡å—ä¸­è·å–ä¸æ¨¡å‹åŒåçš„ç±»
        model_class = getattr(model_module, model_name)
        return model_class
    except (ImportError, AttributeError) as e:
        raise ImportError(
            f"é”™è¯¯ï¼šæˆåŠŸæ‰¾åˆ°æ–‡ä»¶ï¼Œä½†åœ¨ä» '{model_module_path}' å¯¼å…¥ç±» '{model_name}' æ—¶å¤±è´¥ã€‚\n"
            f"è¯·ç¡®ä¿ä½ çš„ Python æ–‡ä»¶å†… class çš„åç§° ({model_name}) ä¸æ–‡ä»¶åå’Œ --model å‚æ•°å®Œå…¨ä¸€è‡´ã€‚\n"
            f"åŸå§‹é”™è¯¯: {e}"
        )

def set_seed(seed: int):
    """è®¾ç½®å…¨å±€éšæœºç§å­"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def load_item_category_map(
    dataset_root: Path,
    dataset_name: str,
    return_cate_names: bool = False,
    min_items_per_cate: int = 5,
    max_categories: int = 0,
):
    """
    å¾é è™•ç†ç”Ÿæˆçš„ item.json ä¸­æå–é¡åˆ¥ä¿¡æ¯ï¼Œè¿”å› {item_id(1-based): category_id} æ˜ å°„ã€‚
    - è‡ªå‹•æª¢æ¸¬ä¸¦è·³éä½”æ¯”éé«˜çš„æ ¹é¡ï¼ˆå¦‚ã€ŒMusical Instrumentsã€ï¼‰ï¼Œå„ªå…ˆå–ä¸‹ä¸€ç´šã€‚
    - è‹¥ç¼ºå¤±å‰‡å›é€€åˆ° genres[0]ï¼ˆMovieLensï¼‰ã€‚
    - å†ç¼ºå‰‡ç”¨ brand ä½œç‚ºå¼±é¡åˆ¥ã€‚
    - å¦‚æœä»åªæœ‰æ ¹é¡ï¼Œå˜—è©¦ç”¨ codebook å‰å…©ç´šä½œç‚ºå‚™ç”¨é¡åˆ¥ï¼Œé¿å…å…¨éƒ¨è½åœ¨åŒä¸€é¡ã€‚
    - return_cate_names=True æ™‚ï¼ŒåŒæ™‚è¿”å› {cate_id: cate_name} ä»¥ä¾¿å¾ŒçºŒçµ±è¨ˆã€‚
    """
    item_file = dataset_root / f"{dataset_name}.item.json"
    if not item_file.is_file():
        logging.warning(f"[Diversity] item metadata not found at {item_file}, skip category map.")
        return {}

    try:
        with open(item_file, "r", encoding="utf-8") as f:
            item_meta = json.load(f)
    except Exception as exc:
        logging.warning(f"[Diversity] failed to load item metadata ({item_file}): {exc}")
        return {}

    cate2id: dict = {}
    item_to_cate: dict = {}
    missing = 0
    parsed_categories: dict = {}

    # å…ˆè§£æå‡º categories çš„å±¤ç´šï¼Œæ–¹ä¾¿å¾ŒçºŒçµ±è¨ˆ/è·³éæ ¹é¡
    for item_str, info in item_meta.items():
        categories = info.get("categories")
        tokens = []
        if isinstance(categories, list):
            categories = categories[0] if categories else None
        if isinstance(categories, str) and categories.strip():
            tokens = [c.strip() for c in categories.split(",") if c.strip()]
        parsed_categories[item_str] = tokens

    # æª¢æ¸¬æ˜¯å¦å­˜åœ¨ä½”æ¯”æ¥µé«˜çš„æ ¹é¡ï¼ˆä¾‹å¦‚å…¨éƒ¨éƒ½æ˜¯ Musical Instrumentsï¼‰
    first_level_counter = Counter(tokens[0] for tokens in parsed_categories.values() if tokens)
    dominant_root = None
    total_with_cate = sum(1 for tokens in parsed_categories.values() if tokens)
    if first_level_counter and total_with_cate:
        root_candidate, cnt = first_level_counter.most_common(1)[0]
        if cnt / total_with_cate >= 0.9:  # 90% ä»¥ä¸Šè¦–ç‚ºéœ€è¦è·³éçš„æ ¹é¡
            dominant_root = root_candidate
            logging.info(
                f"[Diversity] Detected dominant root category '{dominant_root}' "
                f"({cnt}/{total_with_cate}); will skip it when possible."
            )
    root_norm = dominant_root.lower() if dominant_root else None

    # æ§‹å»ºå»é™¤æ ¹é¡çš„å±¤ç´šåˆ—è¡¨ï¼Œä¸¦æ ¹æ“šçµ±è¨ˆæ±ºå®šä½¿ç”¨æ·ºå±¤é‚„æ˜¯æœ€ç´°ç²’åº¦
    parsed_no_root = {}
    max_depth = 0
    for item_str, tokens in parsed_categories.items():
        cleaned = tokens
        if tokens and root_norm and tokens[0].lower() == root_norm:
            cleaned = tokens[1:]
        parsed_no_root[item_str] = cleaned
        max_depth = max(max_depth, len(cleaned))

    depth_counters = []
    depth_dominance = []
    total_items = len(parsed_no_root)
    depth0_coverage = (
        sum(1 for toks in parsed_no_root.values() if len(toks) > 0) / total_items
    ) if total_items else 0.0
    for d in range(max_depth):
        ctr = Counter(toks[d] for toks in parsed_no_root.values() if len(toks) > d)
        depth_counters.append(ctr)
        total_d = sum(ctr.values())
        dom_ratio = (ctr.most_common(1)[0][1] / total_d) if total_d else None
        depth_dominance.append(dom_ratio)

    # å¦‚æœç¬¬ä¸€å±¤éæ ¹é¡ä¾ç„¶é«˜åº¦é›†ä¸­ä¸”è¦†è“‹é«˜ï¼Œæ‰åˆ‡åˆ°æ›´æ·±çš„ç´°ç²’åº¦
    use_deepest = False
    if max_depth > 0:
        dom0 = depth_dominance[0]
        if dom0 is not None and dom0 > 0.8 and max_depth > 1 and depth0_coverage >= 0.8:
            use_deepest = True
            logging.info(
                f"[Diversity] First non-root category dominance={dom0:.2f}, "
                f"coverage={depth0_coverage:.2f}; switch to deepest level."
            )

    # é å‚™ codebook å‰å…©ç´šä½œç‚ºå‚™ç”¨é¡åˆ¥ï¼ˆåƒ…åœ¨å­˜åœ¨å–®ä¸€æ ¹é¡æ™‚å•Ÿç”¨ï¼‰
    codebook_prefixes = {}
    if dominant_root:
        codebook_dir = dataset_root / "codebooks"
        preferred = codebook_dir / f"{dataset_name}.text.rqvae.codebook.json"
        codebook_file = preferred if preferred.is_file() else None
        if not codebook_file and codebook_dir.is_dir():
            candidates = sorted(codebook_dir.glob("*.codebook.json"))
            if candidates:
                codebook_file = candidates[0]
        if codebook_file:
            try:
                with open(codebook_file, "r", encoding="utf-8") as f:
                    codebook_data = json.load(f)
                for key, value in codebook_data.items():
                    try:
                        idx = int(key)
                    except (TypeError, ValueError):
                        continue
                    parts = [p.strip("<>") for p in str(value).split() if p]
                    if len(parts) >= 2:
                        codebook_prefixes[idx] = f"{parts[0]}|{parts[1]}"
                    elif parts:
                        codebook_prefixes[idx] = parts[0]
                logging.info(
                    f"[Diversity] Loaded codebook prefixes from {codebook_file} "
                    f"for category fallback (entries={len(codebook_prefixes)})."
                )
            except Exception as exc:
                logging.warning(f"[Diversity] failed to load codebook prefixes: {exc}")

    # ç¬¬ä¸€è¼ªï¼šç¢ºå®šåˆå§‹é¡åˆ¥
    item_to_raw_cate = {}
    item_tokens_no_root = {}
    for item_str, info in item_meta.items():
        cat = None
        tokens = parsed_categories.get(item_str, [])
        tokens_no_root = parsed_no_root.get(item_str, [])
        if tokens_no_root:
            # æ ¹æ“šçµ±è¨ˆé¸æ“‡é¡åˆ¥å±¤ç´šï¼šéæ ¹æœ€ç´°æˆ–ç¬¬ä¸€ç´š
            cat = tokens_no_root[-1] if use_deepest else tokens_no_root[0]
        elif tokens:
            # åªæœ‰æ ¹é¡ï¼ŒæŒ‰ç­–ç•¥æ±ºå®šæ˜¯å¦ä¿ç•™æ ¹
            cat = tokens[-1] if use_deepest else tokens[0]

        if not cat:
            genres = info.get("genres")
            if isinstance(genres, list) and len(genres) > 0:
                cat = str(genres[0]).strip()

        if not cat:
            brand = info.get("brand", "").strip()
            if brand:
                cat = f"brand::{brand}"

        # å¦‚æœä»ç„¶åªæœ‰æ ¹é¡æˆ–ç¼ºå¤±ï¼Œå˜—è©¦ç”¨ codebook å‰å…©ç´šåšå‚™ç”¨é¡åˆ¥
        try:
            iid = int(item_str)
        except ValueError:
            missing += 1
            continue
        if use_deepest and (not cat or (root_norm and cat.lower() == root_norm)) and codebook_prefixes:
            cb = codebook_prefixes.get(iid)
            if cb:
                cat = f"code::{cb}"

        if not cat:
            missing += 1
            continue

        # æ¨¡å‹å…§éƒ¨ item_id æ˜¯ 1-based
        item_to_raw_cate[iid + 1] = cat
        item_tokens_no_root[iid + 1] = tokens_no_root

    total_items_with_cate = len(item_to_raw_cate)

    # ç¬¬äºŒè¼ªï¼šå°å æ¯”éé«˜çš„é¡åˆ¥å˜—è©¦ç´°åˆ†ï¼ˆé€²ä¸€æ­¥ä½¿ç”¨æ›´æ·±å±¤çš„é¡åˆ¥ï¼‰
    split_threshold = 0.20
    if total_items_with_cate > 0:
        updated_global = True
        while updated_global:
            updated_global = False
            raw_counter = Counter(item_to_raw_cate.values())
            for cate, cnt in list(raw_counter.items()):
                if cnt / total_items_with_cate <= split_threshold:
                    continue
                # å°è©²é¡åˆ¥çš„ item å˜—è©¦ç”¨ã€Œç•¶å‰å±¤ç´šçš„ä¸‹ä¸€å±¤ã€é€²è¡Œç´°åˆ†
                for iid, cur_cate in list(item_to_raw_cate.items()):
                    if cur_cate != cate:
                        continue
                    toks_nr = item_tokens_no_root.get(iid, [])
                    new_cate = None
                    if cate in toks_nr:
                        idx = toks_nr.index(cate)
                        if idx + 1 < len(toks_nr):
                            new_cate = toks_nr[idx + 1]
                        elif len(toks_nr) > 1:
                            new_cate = toks_nr[-1]
                    elif len(toks_nr) >= 2:
                        new_cate = toks_nr[1]
                    elif len(toks_nr) == 1:
                        new_cate = toks_nr[0]
                    if new_cate and new_cate != cate:
                        item_to_raw_cate[iid] = new_cate
                        updated_global = True
            if not updated_global:
                break

    # ç¬¬ä¸‰è¼ªï¼šå¼·åˆ¶é¡åˆ¥è‡³å°‘åŒ…å« min_items_per_cateï¼Œä¸”é™åˆ¶ç¸½é¡åˆ¥æ•¸
    raw_counter = Counter(item_to_raw_cate.values())
    small_cates = {c for c, cnt in raw_counter.items() if cnt < min_items_per_cate}
    if max_categories and len(raw_counter) > max_categories:
        for idx, (cate, _) in enumerate(raw_counter.most_common()):
            if idx >= max_categories:
                small_cates.add(cate)
    if small_cates:
        other_name = "other"
        for iid, cate in list(item_to_raw_cate.items()):
            if cate in small_cates:
                item_to_raw_cate[iid] = other_name
        raw_counter = Counter(item_to_raw_cate.values())

    # é‡æ–°ç·¨ç¢¼ cate_id
    for cate in raw_counter.keys():
        cate_id = cate2id.setdefault(cate, len(cate2id))
    for iid, cate in item_to_raw_cate.items():
        item_to_cate[iid] = cate2id[cate]

    logging.info(
        f"[Diversity] Loaded categories for {len(item_to_cate)} items "
        f"(distinct categories={len(cate2id)}, missing={missing})."
    )
    if return_cate_names:
        id_to_cate = {cid: cname for cname, cid in cate2id.items()}
        return item_to_cate, id_to_cate
    return item_to_cate
