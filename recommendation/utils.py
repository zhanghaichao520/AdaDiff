# utils.py
# -*- coding: utf-8 -*-

import os
import sys
import yaml
import numpy as np
import random
import torch
import logging
from pathlib import Path
import importlib
from collections.abc import Mapping

VALID_QUANT_METHODS = {"rkmeans", "rvq", "rqvae", "opq", "pq", 'vqvae', 'mm_rqvae'}

def _ensure_dir_exists(dir_path: Path):
    """ç¡®ä¿ç›®å½•å­˜åœ¨"""
    if dir_path:
        dir_path.mkdir(parents=True, exist_ok=True)

def _recursive_update(base_dict: dict, new_dict: dict) -> dict:
    """
    éè¿´åœ°æ›´æ–°å­—å…¸ã€‚
    å¦‚æœ new_dict ä¸­çš„éµåœ¨ base_dict ä¸­ä¹Ÿå­˜åœ¨ä¸”å°æ‡‰çš„å€¼éƒ½æ˜¯å­—å…¸ï¼Œ
    å‰‡éè¿´åœ°åˆä½µå®ƒå€‘ï¼Œå¦å‰‡ç›´æ¥ç”¨ new_dict çš„å€¼è¦†è“‹ base_dict çš„å€¼ã€‚
    """
    for key, value in new_dict.items():
        if isinstance(value, Mapping) and key in base_dict and isinstance(base_dict[key], Mapping):
            base_dict[key] = _recursive_update(base_dict[key], value)
        else:
            base_dict[key] = value
    return base_dict


def _load_quant_details(path: str, quant_method: str) -> dict:
    """
    å¾æŒ‡å®šçš„ YAML æª”æ¡ˆä¸­ï¼Œæ ¹æ“š quant_method è¼‰å…¥å°æ‡‰çš„åƒæ•¸ã€‚
    """
    if not Path(path).is_file():
        raise FileNotFoundError(f"[Config] æ ¹æ“šç´„å®šï¼Œæœªæ‰¾åˆ°é‡åŒ–è¨­å®šæª”: {path}")
    
    with open(path, 'r') as f:
        cfg = yaml.safe_load(f)
        
    if quant_method not in cfg or 'model_params' not in cfg[quant_method]:
        raise ValueError(f"[Config] åœ¨ {path} ä¸­ç¼ºå°‘ '{quant_method}.model_params' ç¯€é»")
    
    mp = cfg[quant_method]['model_params']
    required_keys = ['codebook_size', 'num_levels']
    if not all(key in mp for key in required_keys):
         raise ValueError(f"[Config] åœ¨ {path} çš„ model_params ä¸­ç¼ºå°‘ 'codebook_size' æˆ– 'num_levels'")

    return mp


def load_and_process_config(model_name: str, dataset_name: str, quant_method: str, embedding_modality: str = 'text') -> dict:
    """
    é€šç”¨é…ç½®åŠ è½½å™¨ (V6 - æ”¯æ´ base.yaml ç¹¼æ‰¿èˆ‡è¦†è“‹)ã€‚
    """
    # === 1. âœ… é—œéµæ”¹å‹• 2ï¼šä¾åºè¼‰å…¥ base å’Œ model-specific è¨­å®šæª” ===
    # è¼‰å…¥åŸºç¤è¨­å®šæª”
    base_config_path = Path("configs/base.yaml")
    if not base_config_path.is_file():
        raise FileNotFoundError(f"åŸºç¤è¨­å®šæª”æœªæ‰¾åˆ°: {base_config_path}")
    with open(base_config_path, 'r') as f:
        config = yaml.safe_load(f)

    # è¼‰å…¥ç‰¹å®šæ¨¡å‹è¨­å®šæª”
    model_config_path = Path(f"configs/{model_name}.yaml")
    if not model_config_path.is_file():
        raise FileNotFoundError(f"æ¨¡å‹é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {model_config_path}")
    with open(model_config_path, 'r') as f:
        model_config = yaml.safe_load(f)
        
    # âœ¨ ä½¿ç”¨éè¿´æ›´æ–°ï¼Œè®“ model_config è¦†è“‹ base_config
    config = _recursive_update(config, model_config)

    # === å¾ŒçºŒæµç¨‹å®Œå…¨ä¸è®Šï¼Œå®ƒå€‘ç¾åœ¨æ“ä½œçš„æ˜¯å·²ç¶“åˆä½µå¥½çš„ config ===
    
    config['model_name'] = model_name
    config['dataset_name'] = dataset_name
    config['quant_method'] = quant_method.lower()
    config['embedding_modality'] = embedding_modality.lower()
    
    if config['quant_method'] not in VALID_QUANT_METHODS:
        raise ValueError(f"ä¸æ”¯æŒçš„é‡åŒ–æ–¹æ³•: {quant_method}ã€‚å¯é€‰: {VALID_QUANT_METHODS}")

    # 2. ç¨ç«‹åœ°è¼‰å…¥é‡åŒ–è¨­å®šæª”
    quant_config_path = Path(f"../quantization/configs/{quant_method}_config.yaml")
    quant_details = _load_quant_details(quant_config_path, config['quant_method'])
    
    # === 3. æ ¼å¼åŒ–å’Œæ´¾ç”Ÿã€Œæ•¸æ“šã€èˆ‡ã€Œè¼¸å‡ºã€çš„è·¯å¾‘ ===
    # é€™éƒ¨åˆ†çš„è·¯å¾‘æ¨¡æ¿ä»ç„¶ä¾†è‡ª TIGER.yamlï¼Œæ˜¯åˆç†çš„ï¼Œå› ç‚ºå®ƒå®šç¾©äº†æ•¸æ“šå­˜æ”¾æ ¼å¼
    # 3. æ ¼å¼åŒ–è·¯å¾‘
    paths = config['paths']
    # âœ… å°‡ model_name åŠ å…¥å­—å…¸
    format_args = {
        'dataset_name': dataset_name, 
        'quant_method': config['quant_method'], 
        'model_name': model_name
    }
    dataset_root = Path(paths['dataset_root'].format(**format_args))
    output_root = Path(paths['output_root'].format(**format_args))

        # === 4. è‡ªåŠ¨æ„é€  codebook è·¯å¾„ ===
    dataset_root = Path(f"../datasets/{dataset_name}")
    codebook_dir = dataset_root / "codebooks"

    mod_tag = embedding_modality.lower()
    quant_tag = config['quant_method'].lower()

    # ä¸¥æ ¼åŒ¹é…æŒ‡å®šæ¨¡æ€å’Œé‡åŒ–æ–¹æ³•
    codebook_path = codebook_dir / f"{dataset_name}.{mod_tag}.{quant_tag}.npy"

    if not codebook_path.exists():
        raise FileNotFoundError(
            f"[FATAL] æœªæ‰¾åˆ°æŒ‡å®šæ¨¡æ€ '{mod_tag}' çš„ codebook æ–‡ä»¶ï¼\n"
            f"æœŸæœ›è·¯å¾„: {codebook_path}\n"
            f"è¯·ç¡®è®¤è·¯å¾„åŠæ–‡ä»¶åä¸ä¿å­˜æ—¶ä¸€è‡´ã€‚"
        )

    config['code_path'] = str(codebook_path)
    logging.info(f"ğŸ“¦ [Config] æˆåŠŸåŠ è½½ Codebook: {config['code_path']}")

    config['log_path'] = output_root / "training.log"
    config['save_path'] = output_root / "best_model.pth"
    config['train_json'] = dataset_root / f"{dataset_name}.train.jsonl"
    config['valid_json'] = dataset_root / f"{dataset_name}.valid.jsonl"
    config['test_json'] = dataset_root / f"{dataset_name}.test.jsonl"
    _ensure_dir_exists(output_root)

    # === 4. æ ¹æ“šè¼‰å…¥çš„é‡åŒ–ç´°ç¯€ï¼Œè¨ˆç®—è©è¡¨åƒæ•¸ ===
    K = int(quant_details['codebook_size'])
    num_semantic_levels = int(quant_details['num_levels'])
    has_dup_layer = quant_details.get('has_dup_layer', True) 
    
    config['codebook_size'] = K
    config['num_semantic_levels'] = num_semantic_levels

    # === 5. æ ¡éªŒ codebook æª”æ¡ˆ ===
    if not Path(config['code_path']).is_file():
        raise FileNotFoundError(f"[FATAL] æœªæ‰¾åˆ° codebook: {config['code_path']}")
    codes_arr = np.load(config['code_path'], allow_pickle=True)
    codes_mat = np.vstack(codes_arr) if codes_arr.dtype == object else codes_arr
    
    expected_code_len = num_semantic_levels + 1 if has_dup_layer else num_semantic_levels
    config['code_len'] = expected_code_len

    if codes_mat.ndim != 2 or codes_mat.shape[1] != expected_code_len:
        raise ValueError(f"[FATAL] Codebook {config['code_path']} çš„æœŸæœ›å½¢çŠ¶ç‚º (N, {expected_code_len})ï¼Œå¯¦éš›ç‚º {codes_mat.shape}")

    # === 6. è¨ˆç®—æœ€çµ‚è©è¡¨åƒæ•¸ ===
    if has_dup_layer:
        dup_max = int(codes_mat[:, -1].max()) if codes_mat.size > 0 else 0
        dup_vocab_size = dup_max + 1
        config['dup_vocab_size'] = dup_vocab_size
        vocab_sizes = [K] * num_semantic_levels + [dup_vocab_size]
    else:
        vocab_sizes = [K] * num_semantic_levels
    config['vocab_sizes'] = vocab_sizes

    bases = np.cumsum([0] + vocab_sizes[:-1]).tolist()
    config['bases'] = [int(b) for b in bases]

    max_token_id = sum(config['vocab_sizes'])
    eos_id = max_token_id + 1
    
    config['token_params'] = {
        'pad_token_id': 0, 'eos_token_id': eos_id, 'vocab_size': eos_id + 5 
    }
    
    return config


# --- æ—¥å¿—å’Œéšæœºç§å­å‡½æ•°ä¿æŒä¸å˜ ---
def setup_logging(log_path: Path):
    """é…ç½®æ—¥å¿—è®°å½•å™¨"""
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    if logger.hasHandlers():
        logger.handlers.clear()
    fmt = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

    sh = logging.StreamHandler(sys.stdout)
    sh.setFormatter(fmt)
    logger.addHandler(sh)

    fh = logging.FileHandler(str(log_path), mode='a', encoding='utf-8')
    fh.setFormatter(fmt)
    logger.addHandler(fh)

def get_model_class(model_name: str):
    """
    å¢å¼ºç‰ˆï¼šæ ¹æ®æ¨¡å‹åç§°å­—ç¬¦ä¸²ï¼ŒåŠ¨æ€åœ°ä» 'models' ç›®å½•åŠå…¶æ‰€æœ‰å­ç›®å½•ä¸­
    æœç´¢å¹¶å¯¼å…¥å¯¹åº”çš„æ¨¡å‹ç±»ã€‚

    Args:
        model_name (str): æ¨¡å‹çš„åç§° (e.g., "TIGER")ã€‚
                          çº¦å®šï¼šæ–‡ä»¶åå’Œç±»åéƒ½åº”ä¸ model_name ä¸€è‡´ (TIGER.py, class TIGER)ã€‚
    """
    models_root_dir = "models"
    model_file_name = f"{model_name}.py"
    model_module_path = None

    # os.walk ä¼šéå†æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶å¤¹å’Œæ–‡ä»¶
    for root, dirs, files in os.walk(models_root_dir):
        if model_file_name in files:
            # æ‰¾åˆ°äº†æ–‡ä»¶ï¼ç°åœ¨æ„å»º Python çš„å¯¼å…¥è·¯å¾„
            # ä¾‹å¦‚, root = "models/encoder_decoder"
            
            # 1. å°†æ–‡ä»¶ç³»ç»Ÿè·¯å¾„ ('/') æ›¿æ¢ä¸º Python å¯¼å…¥è·¯å¾„ ('.')
            # "models/encoder_decoder" -> "models.encoder_decoder"
            base_path = root.replace(os.sep, '.')
            
            # 2. æ‹¼æ¥æˆæœ€ç»ˆçš„æ¨¡å—è·¯å¾„
            # "models.encoder_decoder.TIGER"
            model_module_path = f"{base_path}.{model_name}"
            break # æ‰¾åˆ°åç«‹åˆ»åœæ­¢æœç´¢

    # å¦‚æœéå†å®Œéƒ½æ²¡æ‰¾åˆ°æ–‡ä»¶
    if not model_module_path:
        raise ImportError(
            f"é”™è¯¯ï¼šæ— æ³•åœ¨ '{models_root_dir}' ç›®å½•æˆ–å…¶ä»»ä½•å­ç›®å½•ä¸­æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ '{model_file_name}'ã€‚\n"
            f"è¯·æ£€æŸ¥ä½ çš„æ–‡ä»¶ç»“æ„å’Œ --model å‚æ•°æ˜¯å¦æ­£ç¡®ã€‚"
        )

    try:
        # ä½¿ç”¨åŠ¨æ€æ„å»ºçš„è·¯å¾„æ¥å¯¼å…¥æ¨¡å—
        logging.info(f"Found model module at: {model_module_path}")
        model_module = importlib.import_module(model_module_path)
        # ä»æ¨¡å—ä¸­è·å–ä¸æ¨¡å‹åŒåçš„ç±»
        model_class = getattr(model_module, model_name)
        return model_class
    except (ImportError, AttributeError) as e:
        raise ImportError(
            f"é”™è¯¯ï¼šæˆåŠŸæ‰¾åˆ°æ–‡ä»¶ï¼Œä½†åœ¨ä» '{model_module_path}' å¯¼å…¥ç±» '{model_name}' æ—¶å¤±è´¥ã€‚\n"
            f"è¯·ç¡®ä¿ä½ çš„ Python æ–‡ä»¶å†… class çš„åç§° ({model_name}) ä¸æ–‡ä»¶åå’Œ --model å‚æ•°å®Œå…¨ä¸€è‡´ã€‚\n"
            f"åŸå§‹é”™è¯¯: {e}"
        )

def set_seed(seed: int):
    """è®¾ç½®å…¨å±€éšæœºç§å­"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)