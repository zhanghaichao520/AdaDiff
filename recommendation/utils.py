# utils.py
# -*- coding: utf-8 -*-

import os
import sys
import yaml
import numpy as np
import random
import torch
import logging
from pathlib import Path
import importlib
from collections.abc import Mapping
from collections import Counter
import json

VALID_QUANT_METHODS = {"rkmeans", "rvq", "rqvae", "opq", "pq", 'vqvae', 'mm_rqvae'}

def _ensure_dir_exists(dir_path: Path):
    """ç¡®ä¿ç›®å½•å­˜åœ¨"""
    if dir_path:
        dir_path.mkdir(parents=True, exist_ok=True)

def _recursive_update(base_dict: dict, new_dict: dict) -> dict:
    """
    éè¿´åœ°æ›´æ–°å­—å…¸ã€‚
    å¦‚æœ new_dict ä¸­çš„éµåœ¨ base_dict ä¸­ä¹Ÿå­˜åœ¨ä¸”å°æ‡‰çš„å€¼éƒ½æ˜¯å­—å…¸ï¼Œ
    å‰‡éè¿´åœ°åˆä½µå®ƒå€‘ï¼Œå¦å‰‡ç›´æ¥ç”¨ new_dict çš„å€¼è¦†è“‹ base_dict çš„å€¼ã€‚
    """
    for key, value in new_dict.items():
        if isinstance(value, Mapping) and key in base_dict and isinstance(base_dict[key], Mapping):
            base_dict[key] = _recursive_update(base_dict[key], value)
        else:
            base_dict[key] = value
    return base_dict


def _load_quant_details(path: str, quant_method: str) -> dict:
    """
    å¾æŒ‡å®šçš„ YAML æª”æ¡ˆä¸­ï¼Œæ ¹æ“š quant_method è¼‰å…¥å°æ‡‰çš„åƒæ•¸ã€‚
    """
    if not Path(path).is_file():
        raise FileNotFoundError(f"[Config] æ ¹æ“šç´„å®šï¼Œæœªæ‰¾åˆ°é‡åŒ–è¨­å®šæª”: {path}")
    
    with open(path, 'r') as f:
        cfg = yaml.safe_load(f)
        
    if quant_method not in cfg or 'model_params' not in cfg[quant_method]:
        raise ValueError(f"[Config] åœ¨ {path} ä¸­ç¼ºå°‘ '{quant_method}.model_params' ç¯€é»")
    
    mp = cfg[quant_method]['model_params']
    required_keys = ['codebook_size', 'num_levels']
    if not all(key in mp for key in required_keys):
         raise ValueError(f"[Config] åœ¨ {path} çš„ model_params ä¸­ç¼ºå°‘ 'codebook_size' æˆ– 'num_levels'")

    return mp


def load_and_process_config(model_name: str, dataset_name: str, quant_method: str, embedding_modality: str = 'text') -> dict:
    """
    é€šç”¨é…ç½®åŠ è½½å™¨ (V6 - æ”¯æ´ base.yaml ç¹¼æ‰¿èˆ‡è¦†è“‹)ã€‚
    """
    # === 1. âœ… é—œéµæ”¹å‹• 2ï¼šä¾åºè¼‰å…¥ base å’Œ model-specific è¨­å®šæª” ===
    # è¼‰å…¥åŸºç¤è¨­å®šæª”
    base_config_path = Path("configs/base.yaml")
    if not base_config_path.is_file():
        raise FileNotFoundError(f"åŸºç¤è¨­å®šæª”æœªæ‰¾åˆ°: {base_config_path}")
    with open(base_config_path, 'r') as f:
        config = yaml.safe_load(f)

    # è¼‰å…¥ç‰¹å®šæ¨¡å‹è¨­å®šæª”
    model_config_path = Path(f"configs/{model_name}.yaml")
    if not model_config_path.is_file():
        raise FileNotFoundError(f"æ¨¡å‹é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {model_config_path}")
    with open(model_config_path, 'r') as f:
        model_config = yaml.safe_load(f)
        
    # âœ¨ ä½¿ç”¨éè¿´æ›´æ–°ï¼Œè®“ model_config è¦†è“‹ base_config
    config = _recursive_update(config, model_config)

    # === å¾ŒçºŒæµç¨‹å®Œå…¨ä¸è®Šï¼Œå®ƒå€‘ç¾åœ¨æ“ä½œçš„æ˜¯å·²ç¶“åˆä½µå¥½çš„ config ===
    
    config['model_name'] = model_name
    config['dataset_name'] = dataset_name
    config['quant_method'] = quant_method.lower()
    config['embedding_modality'] = embedding_modality.lower()
    
    if config['quant_method'] not in VALID_QUANT_METHODS:
        raise ValueError(f"ä¸æ”¯æŒçš„é‡åŒ–æ–¹æ³•: {quant_method}ã€‚å¯é€‰: {VALID_QUANT_METHODS}")

    # 2. ç¨ç«‹åœ°è¼‰å…¥é‡åŒ–è¨­å®šæª”
    quant_config_path = Path(f"../quantization/configs/{quant_method}_config.yaml")
    quant_details = _load_quant_details(quant_config_path, config['quant_method'])
    
    # === 3. æ ¼å¼åŒ–å’Œæ´¾ç”Ÿã€Œæ•¸æ“šã€èˆ‡ã€Œè¼¸å‡ºã€çš„è·¯å¾‘ ===
    # é€™éƒ¨åˆ†çš„è·¯å¾‘æ¨¡æ¿ä»ç„¶ä¾†è‡ª TIGER.yamlï¼Œæ˜¯åˆç†çš„ï¼Œå› ç‚ºå®ƒå®šç¾©äº†æ•¸æ“šå­˜æ”¾æ ¼å¼
    # 3. æ ¼å¼åŒ–è·¯å¾‘
    paths = config['paths']
    # âœ… å°‡ model_name åŠ å…¥å­—å…¸
    format_args = {
        'dataset_name': dataset_name, 
        'quant_method': config['quant_method'], 
        'model_name': model_name
    }
    dataset_root = Path(paths['dataset_root'].format(**format_args))
    output_root = Path(paths['output_root'].format(**format_args))

        # === 4. è‡ªåŠ¨æ„é€  codebook è·¯å¾„ ===
    dataset_root = Path(f"../datasets/{dataset_name}")
    codebook_dir = dataset_root / "codebooks"

    mod_tag = embedding_modality.lower()
    quant_tag = config['quant_method'].lower()

    # ä¸¥æ ¼åŒ¹é…æŒ‡å®šæ¨¡æ€å’Œé‡åŒ–æ–¹æ³•
    codebook_path = codebook_dir / f"{dataset_name}.{mod_tag}.{quant_tag}.npy"

    if not codebook_path.exists():
        raise FileNotFoundError(
            f"[FATAL] æœªæ‰¾åˆ°æŒ‡å®šæ¨¡æ€ '{mod_tag}' çš„ codebook æ–‡ä»¶ï¼\n"
            f"æœŸæœ›è·¯å¾„: {codebook_path}\n"
            f"è¯·ç¡®è®¤è·¯å¾„åŠæ–‡ä»¶åä¸ä¿å­˜æ—¶ä¸€è‡´ã€‚"
        )

    config['code_path'] = str(codebook_path)
    logging.info(f"ğŸ“¦ [Config] æˆåŠŸåŠ è½½ Codebook: {config['code_path']}")

    config['log_path'] = output_root / "training.log"
    config['save_path'] = output_root / "best_model.pth"
    config['train_json'] = dataset_root / f"{dataset_name}.train.jsonl"
    config['valid_json'] = dataset_root / f"{dataset_name}.valid.jsonl"
    config['test_json'] = dataset_root / f"{dataset_name}.test.jsonl"
    _ensure_dir_exists(output_root)

    # === 4. æ ¹æ“šè¼‰å…¥çš„é‡åŒ–ç´°ç¯€ï¼Œè¨ˆç®—è©è¡¨åƒæ•¸ ===
    K = int(quant_details['codebook_size'])
    num_semantic_levels = int(quant_details['num_levels'])
    has_dup_layer = quant_details.get('has_dup_layer', True) 
    
    config['codebook_size'] = K
    config['num_semantic_levels'] = num_semantic_levels

    # === 5. æ ¡éªŒ codebook æª”æ¡ˆ ===
    if not Path(config['code_path']).is_file():
        raise FileNotFoundError(f"[FATAL] æœªæ‰¾åˆ° codebook: {config['code_path']}")
    codes_arr = np.load(config['code_path'], allow_pickle=True)
    codes_mat = np.vstack(codes_arr) if codes_arr.dtype == object else codes_arr
    
    expected_code_len = num_semantic_levels + 1 if has_dup_layer else num_semantic_levels
    config['code_len'] = expected_code_len

    if codes_mat.ndim != 2 or codes_mat.shape[1] != expected_code_len:
        raise ValueError(f"[FATAL] Codebook {config['code_path']} çš„æœŸæœ›å½¢çŠ¶ç‚º (N, {expected_code_len})ï¼Œå¯¦éš›ç‚º {codes_mat.shape}")

    # === 6. è¨ˆç®—æœ€çµ‚è©è¡¨åƒæ•¸ ===
    if has_dup_layer:
        dup_max = int(codes_mat[:, -1].max()) if codes_mat.size > 0 else 0
        dup_vocab_size = dup_max + 1
        config['dup_vocab_size'] = dup_vocab_size
        vocab_sizes = [K] * num_semantic_levels + [dup_vocab_size]
    else:
        vocab_sizes = [K] * num_semantic_levels
    config['vocab_sizes'] = vocab_sizes

    bases = np.cumsum([0] + vocab_sizes[:-1]).tolist()
    config['bases'] = [int(b) for b in bases]

    # === 6. å®šç¾©ç‰¹æ®Š Token (çµ±ä¸€æ”¾ç½®åœ¨è©è¡¨çš„å°¾ç«¯ï¼Œé¿å…èˆ‡ Code è¡çª) ===
    base_vocab = sum(config['vocab_sizes'])  # èªç¾© token æ•¸é‡
    # è¦ç¯„ï¼šPAD å›ºå®šç‚º 0ï¼Œèªç¾© token å…¨éƒ¨åç§» +1ï¼Œç‰¹æ®Š token è¿½åŠ åœ¨è©è¡¨å°¾éƒ¨
    pad_id = 0
    mask_id = base_vocab + 1
    cls_id = base_vocab + 2
    sep_id = base_vocab + 3
    eos_id = base_vocab + 4  # ä¿ç•™ EOS
    vocab_size = eos_id + 1  # ID èŒƒåœ 0..eos_id

    config['token_params'] = {
        'pad_token_id': pad_id,
        'cls_token_id': cls_id,
        'sep_token_id': sep_id,
        'mask_token_id': mask_id,
        'eos_token_id': eos_id,
        'vocab_size': vocab_size
    }
    
    return config


# --- æ—¥å¿—å’Œéšæœºç§å­å‡½æ•°ä¿æŒä¸å˜ ---
def setup_logging(log_path: Path):
    """é…ç½®æ—¥å¿—è®°å½•å™¨"""
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    if logger.hasHandlers():
        logger.handlers.clear()
    fmt = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

    sh = logging.StreamHandler(sys.stdout)
    sh.setFormatter(fmt)
    logger.addHandler(sh)

    fh = logging.FileHandler(str(log_path), mode='a', encoding='utf-8')
    fh.setFormatter(fmt)
    logger.addHandler(fh)

def get_model_class(model_name: str):
    """
    å¢å¼ºç‰ˆï¼šæ ¹æ®æ¨¡å‹åç§°å­—ç¬¦ä¸²ï¼ŒåŠ¨æ€åœ°ä» 'models' ç›®å½•åŠå…¶æ‰€æœ‰å­ç›®å½•ä¸­
    æœç´¢å¹¶å¯¼å…¥å¯¹åº”çš„æ¨¡å‹ç±»ã€‚

    Args:
        model_name (str): æ¨¡å‹çš„åç§° (e.g., "TIGER")ã€‚
                          çº¦å®šï¼šæ–‡ä»¶åå’Œç±»åéƒ½åº”ä¸ model_name ä¸€è‡´ (TIGER.py, class TIGER)ã€‚
    """
    models_root_dir = "models"
    model_file_name = f"{model_name}.py"
    model_module_path = None

    # os.walk ä¼šéå†æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶å¤¹å’Œæ–‡ä»¶
    for root, dirs, files in os.walk(models_root_dir):
        if model_file_name in files:
            # æ‰¾åˆ°äº†æ–‡ä»¶ï¼ç°åœ¨æ„å»º Python çš„å¯¼å…¥è·¯å¾„
            # ä¾‹å¦‚, root = "models/encoder_decoder"
            
            # 1. å°†æ–‡ä»¶ç³»ç»Ÿè·¯å¾„ ('/') æ›¿æ¢ä¸º Python å¯¼å…¥è·¯å¾„ ('.')
            # "models/encoder_decoder" -> "models.encoder_decoder"
            base_path = root.replace(os.sep, '.')
            
            # 2. æ‹¼æ¥æˆæœ€ç»ˆçš„æ¨¡å—è·¯å¾„
            # "models.encoder_decoder.TIGER"
            model_module_path = f"{base_path}.{model_name}"
            break # æ‰¾åˆ°åç«‹åˆ»åœæ­¢æœç´¢

    # å¦‚æœéå†å®Œéƒ½æ²¡æ‰¾åˆ°æ–‡ä»¶
    if not model_module_path:
        raise ImportError(
            f"é”™è¯¯ï¼šæ— æ³•åœ¨ '{models_root_dir}' ç›®å½•æˆ–å…¶ä»»ä½•å­ç›®å½•ä¸­æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ '{model_file_name}'ã€‚\n"
            f"è¯·æ£€æŸ¥ä½ çš„æ–‡ä»¶ç»“æ„å’Œ --model å‚æ•°æ˜¯å¦æ­£ç¡®ã€‚"
        )

    try:
        # ä½¿ç”¨åŠ¨æ€æ„å»ºçš„è·¯å¾„æ¥å¯¼å…¥æ¨¡å—
        logging.info(f"Found model module at: {model_module_path}")
        model_module = importlib.import_module(model_module_path)
        # ä»æ¨¡å—ä¸­è·å–ä¸æ¨¡å‹åŒåçš„ç±»
        model_class = getattr(model_module, model_name)
        return model_class
    except (ImportError, AttributeError) as e:
        raise ImportError(
            f"é”™è¯¯ï¼šæˆåŠŸæ‰¾åˆ°æ–‡ä»¶ï¼Œä½†åœ¨ä» '{model_module_path}' å¯¼å…¥ç±» '{model_name}' æ—¶å¤±è´¥ã€‚\n"
            f"è¯·ç¡®ä¿ä½ çš„ Python æ–‡ä»¶å†… class çš„åç§° ({model_name}) ä¸æ–‡ä»¶åå’Œ --model å‚æ•°å®Œå…¨ä¸€è‡´ã€‚\n"
            f"åŸå§‹é”™è¯¯: {e}"
        )

def set_seed(seed: int):
    """è®¾ç½®å…¨å±€éšæœºç§å­"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


import json
import logging
from pathlib import Path
from collections import Counter
from typing import Dict, Any, List, Tuple, Optional

def load_item_category_map(
    dataset_root: Path,
    dataset_name: str,
    return_cate_names: bool = False,
    min_items_per_cate: int = 5,
    max_categories: int = 0,
    split_threshold: float = 0.01,  # [ä¿®æ”¹] é»˜èªé–¾å€¼é™ç‚º 1%ï¼Œè®“é¡åˆ¥æ›´å®¹æ˜“è¢«æ‹†åˆ†
    use_composite_keys: bool = True # [ä¿®æ”¹] æ–°å¢åƒæ•¸ï¼Œæ˜¯å¦çµ„åˆæœ€å¾Œå…©ç´š (e.g. "Guitars > Electric")
):
    """
    å¾é è™•ç†ç”Ÿæˆçš„ item.json ä¸­æå–é¡åˆ¥ä¿¡æ¯ï¼Œè¿”å› {item_id(1-based): category_id} æ˜ å°„ã€‚
    [ä¿®æ”¹ç‰ˆç‰¹æ€§]:
    - é»˜èªæ¡ç”¨ã€Œè‘‰å­å„ªå…ˆã€(Leaf-first) ç­–ç•¥ï¼Œç›´æ¥å–æœ€ç´°ç²’åº¦çš„åˆ†é¡ã€‚
    - æ”¯æŒçµ„åˆéµ (Parent > Child) ä»¥å€åˆ†ä¸åŒå¤§é¡ä¸‹çš„åŒåå­é¡ã€‚
    - é™ä½æ‹†åˆ†é–¾å€¼ï¼Œå¤§å¹…å¢åŠ é¡åˆ¥æ•¸é‡ï¼Œè§£æ±ºå¤šæ¨£æ€§æŒ‡æ¨™éšèºå•é¡Œã€‚
    """
    item_file = dataset_root / f"{dataset_name}.item.json"
    if not item_file.is_file():
        logging.warning(f"[Diversity] item metadata not found at {item_file}, skip category map.")
        return {}

    try:
        with open(item_file, "r", encoding="utf-8") as f:
            item_meta = json.load(f)
    except Exception as exc:
        logging.warning(f"[Diversity] failed to load item metadata ({item_file}): {exc}")
        return {}

    cate2id: dict = {}
    item_to_cate: dict = {}
    missing = 0
    parsed_categories: dict = {}

    # 1. è§£æ Categories å±¤ç´š
    for item_str, info in item_meta.items():
        categories = info.get("categories")
        tokens = []
        if isinstance(categories, list):
            categories = categories[0] if categories else None
        if isinstance(categories, str) and categories.strip():
            tokens = [c.strip() for c in categories.split(",") if c.strip()]
        parsed_categories[item_str] = tokens

    # 2. æª¢æ¸¬ä¸¦è·³éä½”æ¯”æ¥µé«˜çš„æ ¹é¡ï¼ˆä¾‹å¦‚ Amazon æ•¸æ“šé›†ä¸­çš„ "Musical Instruments"ï¼‰
    first_level_counter = Counter(tokens[0] for tokens in parsed_categories.values() if tokens)
    dominant_root = None
    total_with_cate = sum(1 for tokens in parsed_categories.values() if tokens)
    
    if first_level_counter and total_with_cate:
        root_candidate, cnt = first_level_counter.most_common(1)[0]
        # [ä¿®æ”¹] å°‡é–¾å€¼å¾ 0.9 é™ç‚º 0.5ï¼Œæ›´æ¿€é€²åœ°å»é™¤ç„¡æ•ˆçš„æ ¹ç¯€é»
        if cnt / total_with_cate >= 0.5: 
            dominant_root = root_candidate
            logging.info(
                f"[Diversity] Detected dominant root category '{dominant_root}' "
                f"({cnt}/{total_with_cate}); will skip it when possible."
            )
    root_norm = dominant_root.lower() if dominant_root else None

    # æ§‹å»ºå»é™¤æ ¹é¡çš„å±¤ç´šåˆ—è¡¨
    parsed_no_root = {}
    for item_str, tokens in parsed_categories.items():
        cleaned = tokens
        if tokens and root_norm and tokens[0].lower() == root_norm:
            cleaned = tokens[1:]
        parsed_no_root[item_str] = cleaned

    # 3. é å‚™ Codebook ä½œç‚ºå‚™ç”¨ (ä¿æŒåŸé‚è¼¯)
    codebook_prefixes = {}
    if dominant_root or missing > 0: # ç¨å¾®æ”¾å¯¬æ¢ä»¶ï¼Œè®“ missing çš„æ™‚å€™ä¹Ÿèƒ½åŠ è¼‰
        codebook_dir = dataset_root / "codebooks"
        preferred = codebook_dir / f"{dataset_name}.text.rqvae.codebook.json"
        codebook_file = preferred if preferred.is_file() else None
        if not codebook_file and codebook_dir.is_dir():
            candidates = sorted(codebook_dir.glob("*.codebook.json"))
            if candidates: codebook_file = candidates[0]
        
        if codebook_file:
            try:
                with open(codebook_file, "r", encoding="utf-8") as f:
                    codebook_data = json.load(f)
                for key, value in codebook_data.items():
                    try:
                        idx = int(key)
                        parts = [p.strip("<>") for p in str(value).split() if p]
                        if len(parts) >= 2:
                            codebook_prefixes[idx] = f"{parts[0]}|{parts[1]}"
                        elif parts:
                            codebook_prefixes[idx] = parts[0]
                    except: continue
                logging.info(f"[Diversity] Loaded codebook prefixes for fallback ({len(codebook_prefixes)} entries).")
            except Exception as exc:
                logging.warning(f"[Diversity] failed to load codebook prefixes: {exc}")

    # 4. ç¬¬ä¸€è¼ªï¼šç¢ºå®šåˆå§‹é¡åˆ¥ (æ ¸å¿ƒä¿®æ”¹ï¼šè‘‰å­å„ªå…ˆ)
    item_to_raw_cate = {}
    item_tokens_no_root = {} # ä¿å­˜ tokens ä¾›å¾ŒçºŒç´°åˆ†ä½¿ç”¨

    for item_str, info in item_meta.items():
        cat = None
        tokens = parsed_categories.get(item_str, [])
        tokens_no_root = parsed_no_root.get(item_str, [])
        
        # [ä¿®æ”¹] å„ªå…ˆç­–ç•¥ï¼šç›´æ¥ä½¿ç”¨å»é™¤æ ¹ç¯€é»å¾Œçš„æœ€æ·±å±¤ç´š
        candidate_tokens = tokens_no_root if tokens_no_root else tokens
        
        if candidate_tokens:
            if use_composite_keys and len(candidate_tokens) >= 2:
                # [ç­–ç•¥ A] çµ„åˆéµ: "Parent > Child" (å€åˆ†åº¦æœ€é«˜)
                cat = f"{candidate_tokens[-2]} > {candidate_tokens[-1]}"
            else:
                # [ç­–ç•¥ B] è‘‰å­ç¯€é»: å–æœ€å¾Œä¸€ç´š
                cat = candidate_tokens[-1]

        # é™ç´šç­–ç•¥ 1: Genre (é€šå¸¸æ¯” Brand å¯¬æ³›ï¼Œä½†ä¹Ÿå¯ç”¨)
        if not cat:
            genres = info.get("genres")
            if isinstance(genres, list) and len(genres) > 0:
                cat = str(genres[0]).strip()

        # é™ç´šç­–ç•¥ 2: Brand (ä½œç‚ºå¼±é¡åˆ¥è£œå……)
        if not cat:
            brand = info.get("brand", "").strip()
            if brand:
                cat = f"Brand: {brand}"

        # é™ç´šç­–ç•¥ 3: Codebook
        try:
            iid = int(item_str)
        except ValueError:
            missing += 1
            continue

        if not cat and iid in codebook_prefixes:
            cat = f"Code: {codebook_prefixes[iid]}"

        if not cat:
            missing += 1
            continue

        # æ¨¡å‹å…§éƒ¨ item_id æ˜¯ 1-basedï¼Œé€™è£¡è¦åšè½‰æ›
        item_to_raw_cate[iid + 1] = cat
        item_tokens_no_root[iid + 1] = candidate_tokens

    total_items_with_cate = len(item_to_raw_cate)

    # 5. ç¬¬äºŒè¼ªï¼šå°éæ–¼é¾å¤§çš„é¡åˆ¥é€²è¡Œå¼·åˆ¶ç´°åˆ† (ä½¿ç”¨ split_threshold)
    if total_items_with_cate > 0:
        updated_global = True
        loop_cnt = 0
        # å…è¨±è¿­ä»£ 3 æ¬¡ï¼Œæ‡‰å°å±¤ç´šå¾ˆæ·±çš„æƒ…æ³
        while updated_global and loop_cnt < 3: 
            updated_global = False
            loop_cnt += 1
            raw_counter = Counter(item_to_raw_cate.values())
            
            for cate, cnt in list(raw_counter.items()):
                # [ä¿®æ”¹] ä½¿ç”¨å‚³å…¥çš„ä½é–¾å€¼ (e.g. 0.01)
                if cnt / total_items_with_cate <= split_threshold:
                    continue
                
                # å°è©²é¡åˆ¥é€²è¡Œç´°åˆ†å˜—è©¦
                for iid, cur_cate in list(item_to_raw_cate.items()):
                    if cur_cate != cate: continue
                    
                    toks = item_tokens_no_root.get(iid, [])
                    if not toks: continue

                    # å˜—è©¦æ‹¼æ¥æ›´å¤šå±¤ç´šä¾†å€åˆ†
                    # é‚è¼¯ï¼šå¦‚æœç•¶å‰å·²ç¶“ç”¨äº† "A > B"ï¼Œå˜—è©¦è®Šæˆ "A > B > C" (å¦‚æœ C å­˜åœ¨)
                    # é€™è£¡ç°¡åŒ–è™•ç†ï¼šç›´æ¥å–æœ€å¾Œ 3 ç´š
                    new_cate = None
                    if len(toks) >= 3:
                         new_cate = " > ".join(toks[-3:])
                    elif len(toks) == 2 and cur_cate != " > ".join(toks):
                         new_cate = " > ".join(toks)
                    elif len(toks) == 1 and cur_cate != toks[0]:
                         new_cate = toks[0]
                    
                    if new_cate and new_cate != cate:
                        item_to_raw_cate[iid] = new_cate
                        updated_global = True

    # 6. ç¬¬ä¸‰è¼ªï¼šåˆä½µéå°é¡åˆ¥ (Tail Merging)
    raw_counter = Counter(item_to_raw_cate.values())
    small_cates = {c for c, cnt in raw_counter.items() if cnt < min_items_per_cate}
    
    if max_categories and len(raw_counter) > max_categories:
        for idx, (cate, _) in enumerate(raw_counter.most_common()):
            if idx >= max_categories:
                small_cates.add(cate)
                
    if small_cates:
        other_name = "other"
        for iid, cate in list(item_to_raw_cate.items()):
            if cate in small_cates:
                item_to_raw_cate[iid] = other_name
        raw_counter = Counter(item_to_raw_cate.values())

    # 7. é‡æ–°ç·¨ç¢¼ cate_id
    # æŒ‰æ•¸é‡é™åºæ’åˆ— IDï¼Œæ–¹ä¾¿å¾ŒçºŒè§€å¯Ÿ
    sorted_cates = sorted(raw_counter.keys(), key=lambda x: raw_counter[x], reverse=True)
    for cate in sorted_cates:
        cate2id[cate] = len(cate2id)
        
    for iid, cate in item_to_raw_cate.items():
        item_to_cate[iid] = cate2id[cate]

    logging.info(
        f"[Diversity] Loaded fine-grained categories for {len(item_to_cate)} items "
        f"(distinct={len(cate2id)}, missing={missing}, split_thre={split_threshold:.2%})."
    )
    
    if return_cate_names:
        id_to_cate = {cid: cname for cname, cid in cate2id.items()}
        return item_to_cate, id_to_cate
    return item_to_cate