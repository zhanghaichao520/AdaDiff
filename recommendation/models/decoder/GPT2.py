# models/tiger_gpt2.py
from typing import Any, Dict, List
import torch
import transformers
from ..abstract_model import AbstractModel

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))
from metrics import recall_at_k, ndcg_at_k

GPT2LMHeadModel = transformers.GPT2LMHeadModel
GPT2Config = transformers.GPT2Config


class GPT2(AbstractModel):
    """
    Decoder-only ç‰ˆæœ¬çš„ TIGERï¼ŒåŸºäºŽ GPT-2ï¼ˆæ— é¢„è®­ç»ƒæƒé‡ï¼‰ã€‚
    çº¦å®šï¼š
      - batch åŒ…å« input_ids / attention_mask / labels
      - code_len ä»Ž config['code_len'] è¯»å–
      - è¯„ä¼°ä½¿ç”¨ä¸Ž T5 ç‰ˆä¸€è‡´çš„ beam search å’Œ pos_index é€»è¾‘
    """

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)

        model_params = config["model_params"]          # GPT-2 ç»“æž„è¶…å‚ï¼ˆn_layer/n_head/n_embdç­‰ï¼‰
        token_params = config["token_params"]          # è¯è¡¨ã€ç‰¹æ®Šç¬¦å·ç­‰
        vocab_size = token_params["vocab_size"]
        bos_token_id = token_params.get("bos_token_id", 1)
        eos_token_id = token_params.get("eos_token_id", 2)
        pad_token_id = token_params.get("pad_token_id", 0)

        # âš ï¸ GPT-2 é»˜è®¤æ²¡æœ‰ pad/bos/eosï¼Œè¿™é‡Œæ˜¾å¼é…ç½®
        gpt2cfg = GPT2Config(
            **model_params,
            vocab_size=vocab_size,
            bos_token_id=bos_token_id,
            eos_token_id=eos_token_id,
            n_positions=token_params.get("n_positions", 1024),
            n_ctx=token_params.get("n_positions", 1024),
        )
        self.gpt2 = GPT2LMHeadModel(config=gpt2cfg)

        # ç¡®ä¿è¯è¡¨å¤§å°
        self.gpt2.resize_token_embeddings(vocab_size)

        # ä¿å­˜ä¸€ä¸ªå­—ç¬¦ä¸²ç‰ˆå‚æ•°ç»Ÿè®¡
        self.n_params_str = self._calculate_n_parameters()

        # ä¿å­˜ pad/eos ä»¥ä¾¿ generate ä½¿ç”¨
        self._pad_id = pad_token_id
        self._eos_id = eos_token_id

    @property
    def task_type(self) -> str:
        return "generative"

    @property
    def n_parameters(self) -> str:
        return self.n_params_str

    def _calculate_n_parameters(self) -> str:
        num_params = lambda ps: sum(p.numel() for p in ps if p.requires_grad)
        total_params = num_params(self.parameters())
        emb_params = num_params(self.gpt2.get_input_embeddings().parameters())
        return (
            f"# Embedding parameters: {emb_params:,}\n"
            f"# Non-embedding parameters: {total_params - emb_params:,}\n"
            f"# Total trainable parameters: {total_params:,}\n"
        )

    # --- è®­ç»ƒ/å‰å‘ ---
    def forward(self, batch: Dict) -> Dict:
        """
        é€šç”¨ forwardï¼š
        - è‹¥ labels é•¿åº¦ < input_idsï¼Œè‡ªåŠ¨æ‰©å±•å¹¶åœ¨åŽ†å²æ®µ mask æŽ‰ (-100)
        - å…¼å®¹ decoder-only GPT2
        """
        known = {"input_ids", "attention_mask", "labels"}
        inputs = {k: v for k, v in batch.items() if k in known}

        # ðŸ” è‡ªåŠ¨æ£€æµ‹æ˜¯å¦ä¸º GPT-2 decoder-only æ¨¡åž‹
        if isinstance(self.gpt2, transformers.GPT2LMHeadModel):
            input_ids = inputs["input_ids"]
            labels = inputs.get("labels")

            if labels is not None:
                # case: labels shape ä¸åŒ¹é… input_ids
                if labels.shape[1] < input_ids.shape[1]:
                    B, seq_len = input_ids.shape
                    new_labels = torch.full_like(input_ids, -100)

                    # æŠŠç›®æ ‡ code_len æ®µè´´åœ¨åºåˆ—æœ«å°¾
                    code_len = labels.shape[1]
                    new_labels[:, -code_len:] = labels
                    inputs["labels"] = new_labels

        # âœ… æ­£å¸¸å‰å‘
        return self.gpt2(**inputs)


    # --- ç”Ÿæˆ ---
    def generate(self, **kwargs: Any) -> torch.Tensor:
        """
        è°ƒç”¨ GPT-2 çš„æ ‡å‡† generateã€‚éœ€è¦æ³¨æ„ï¼š
          - decoder-only ä¸éœ€è¦ encoder è¾“å…¥
          - éœ€æä¾› pad_token_id/eos_token_id
        """
        kwargs.setdefault("eos_token_id", self._eos_id)
        kwargs.setdefault("pad_token_id", self._pad_id)
        return self.gpt2.generate(**kwargs)

    # --- è¯„ä¼°ï¼ˆä¸Ž T5 ç‰ˆä¿æŒä¸€è‡´çš„åº¦é‡å£å¾„ï¼‰ ---
    def evaluate_step(self, batch: Dict[str, torch.Tensor], topk_list: List[int]) -> Dict[str, float]:
        beam_size = self.config["evaluation_params"]["beam_size"]
        code_len = self.config["code_len"]

        input_ids = batch["input_ids"]
        attention_mask = batch["attention_mask"]
        labels = batch["labels"]
        device = input_ids.device

        # 1) ç”Ÿæˆå¤šæ ·æœ¬ï¼ˆbeamï¼‰
        preds = self.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            num_beams=beam_size,
            num_return_sequences=beam_size,
            max_new_tokens=code_len,
            do_sample=False,
            early_stopping=False,
            eos_token_id=self._eos_id,
            pad_token_id=self._pad_id,
        )
        # 2) å¯¹é½å½¢çŠ¶ï¼šå–æ–°ç”Ÿæˆçš„ code_len æ®µï¼ˆè¿™é‡Œå‡è®¾ input å·²ç»åŒ…å« BOS æˆ–åŽ†å²ï¼‰
        #    GPT-2 generate çš„è¾“å‡ºæ˜¯ [prompt + new_tokens]ï¼Œå–æœ«å°¾ code_len ä¸ª token
        preds = preds[:, -code_len:].contiguous().view(input_ids.shape[0], beam_size, -1)

        # 3) å‘½ä¸­è®¡ç®—ï¼ˆä¸Ž T5 ç‰ˆä¸€è‡´ï¼šå‰ L-1 å…¨ç›¸ç­‰ï¼Œæœ€åŽä¸€ä½ >= çœŸå€¼ï¼‰
        pos_index = self._calculate_pos_index(preds, labels, maxk=beam_size).to(device)

        # 4) æŒ‡æ ‡
        out = {}
        for k in topk_list:
            out[f"Recall@{k}"] = recall_at_k(pos_index, k).mean().item()
            out[f"NDCG@{k}"]   = ndcg_at_k(pos_index, k).mean().item()
        return out

    @staticmethod
    def _calculate_pos_index(preds: torch.Tensor, labels: torch.Tensor, maxk: int) -> torch.Tensor:
        """
        preds: (B, maxk, L)
        labels: (B, L)
        å‘½ä¸­ï¼šå‰ L-1 å®Œå…¨ä¸€è‡´ && æœ€åŽä¸€ä½ (dup) é¢„æµ‹ >= çœŸå®ž
        """
        preds = preds.detach().cpu()
        labels = labels.detach().cpu()
        B, _, L = preds.shape
        assert L == labels.shape[1], f"Code length mismatch: preds {L} vs labels {labels.shape[1]}"

        pos_index = torch.zeros((B, maxk), dtype=torch.bool)
        for i in range(B):
            gt = labels[i]
            gt_sem, gt_dup = gt[:-1].tolist(), int(gt[-1].item())
            for j in range(maxk):
                pj = preds[i, j]
                pj_sem, pj_dup = pj[:-1].tolist(), int(pj[-1].item())
                if pj_sem == gt_sem and pj_dup >= gt_dup:
                    pos_index[i, j] = True
                    break
        return pos_index
