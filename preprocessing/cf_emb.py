import argparse
import os
import random
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
from tqdm import tqdm
import json
from utils import load_json, set_device # å‡è®¾ utils.py ä¸­æœ‰ load_json, set_device


# =============== 1. æ•°æ®é›†å’Œ DataLoader (CF ä¸“æœ‰) ===============

class CFDataset(Dataset):
    """ç”¨äº SASRec è®­ç»ƒçš„ Item ID åºåˆ—æ•°æ®é›†"""
    def __init__(self, data_path, max_len):
        self.max_len = max_len
        self.sequences = []
        self.n_items = 0
        
        data_entries = []
        try:
            with open(data_path, 'r', encoding='utf-8') as f:
                # æ ¸å¿ƒä¿®æ­£ï¼šé€è¡Œè¯»å– JSONL
                for line in f:
                    line = line.strip()
                    if line:
                        # ç¡®ä¿æ¯è¡Œéƒ½æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ JSON å¯¹è±¡
                        data_entries.append(json.loads(line))
        except FileNotFoundError:
            raise FileNotFoundError(f"æ•°æ®é›†æ–‡ä»¶æœªæ‰¾åˆ°: {data_path}")
        except json.JSONDecodeError as e:
            # æ•è· JSONL æ–‡ä»¶ä¸­æŸä¸€è¡Œçš„ JSON é”™è¯¯
            raise ValueError(f"JSONL æ–‡ä»¶è§£æé”™è¯¯ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼ã€‚é”™è¯¯ä¿¡æ¯: {e}")

        # ç°åœ¨ä½¿ç”¨ data_entries (åˆ—è¡¨) æ›¿ä»£åŸæ¥çš„ data
        for entry in data_entries:
            history = [int(x) for x in entry.get("history", [])]
            target = int(entry.get("target"))
            # ä½¿ç”¨å®Œæ•´çš„äº¤äº’åºåˆ— [i1, i2, ..., it]
            sequence = history + [target] 
            
            # è®°å½•æœ€å¤§çš„ Item IDï¼Œç”¨äºç¡®å®šåµŒå…¥çŸ©é˜µå¤§å°
            self.n_items = max(self.n_items, max(sequence) if sequence else 0)
            
            # å°†åºåˆ—åˆ†è§£ä¸ºå¤šä¸ª (è¾“å…¥, ç›®æ ‡) æ ·æœ¬
            for t in range(1, len(sequence)):
                # X: [i1, ..., it-1] (å– max_len)
                input_seq = sequence[:t]
                # Y: it
                target_item = sequence[t]

                # å¯¹è¾“å…¥åºåˆ—è¿›è¡Œå·¦ä¾§ Padding/æˆªæ–­ (ä¿æŒä¸€è‡´)
                if len(input_seq) > max_len:
                    input_seq = input_seq[-max_len:]
                else:
                    input_seq = [0] * (max_len - len(input_seq)) + input_seq

                self.sequences.append({
                    'input_ids': torch.tensor(input_seq, dtype=torch.long),
                    'labels': torch.tensor(target_item, dtype=torch.long)
                })

        # æœ€ç»ˆ Item ID (ä» 1 å¼€å§‹ï¼Œ0 ç•™ç»™ Padding)
        self.n_items += 1 

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        return self.sequences[idx]

    @staticmethod
    def collate_fn(batch):
        input_ids = torch.stack([item['input_ids'] for item in batch])
        labels = torch.stack([item['labels'] for item in batch])
        return {
            'input_ids': input_ids,
            'labels': labels
        }


# =============== 2. SASRec æ¨¡å‹ (ç®€åŒ–ç‰ˆ) ===============

class SimplifiedSASRec(nn.Module):
    """ä¸€ä¸ªç®€åŒ–çš„ SASRec æ¨¡å‹ï¼Œç›®æ ‡æ˜¯è®­ç»ƒ Item åµŒå…¥çŸ©é˜µ E_cf"""
    def __init__(self, n_items, max_len, d_model, n_layers, n_heads, dropout_rate):
        super().__init__()
        
        # Item åµŒå…¥çŸ©é˜µ (E_cf)
        self.item_embeddings = nn.Embedding(n_items, d_model, padding_idx=0)
        # ä½ç½®ç¼–ç 
        self.position_embeddings = nn.Embedding(max_len + 1, d_model)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, 
            nhead=n_heads, 
            dim_feedforward=4 * d_model, 
            dropout=dropout_rate,
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)
        self.dropout = nn.Dropout(dropout_rate)
        
        # æœ€ç»ˆé¢„æµ‹å±‚ (å¯ç”¨äº Logits åŒ¹é…)
        self.output_layer = nn.Linear(d_model, d_model)
        
    def forward(self, input_ids):
        seq_len = input_ids.shape[1]
        
        # 1. Item å’Œä½ç½®åµŒå…¥
        item_emb = self.item_embeddings(input_ids)
        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)
        pos_emb = self.position_embeddings(positions)
        
        input_trans = self.dropout(item_emb + pos_emb)
        
        # 2. Attention Mask
        key_padding_mask = (input_ids == 0) # True æ„å‘³ç€å¿½ç•¥
        
        # 3. Transformer ç¼–ç 
        outputs = self.transformer_encoder(input_trans, src_key_padding_mask=key_padding_mask)
        
        # 4. æå–æœ€åä¸€ä¸ªæœ‰æ•ˆ Item çš„çŠ¶æ€ (ç®€åŒ–å¤„ç†ï¼šç›´æ¥å–æœ€åä¸€ä¸ªï¼ŒLoss æ—¶å¤„ç† Padding)
        
        return outputs 
    
    def get_final_state(self, outputs, input_ids):
        """æå–æ¯ä¸ªåºåˆ—çš„æœ€åä¸€ä¸ªæœ‰æ•ˆ Item çš„çŠ¶æ€"""
        # è®¡ç®—åºåˆ—é•¿åº¦ (æ’é™¤ padding)
        seq_lens = (input_ids != 0).sum(dim=1) 
        
        # æå–æœ€åä¸€ä¸ªæœ‰æ•ˆæ—¶é—´æ­¥çš„ hidden state
        # ç¡®ä¿ seq_lens > 0ï¼Œå¦åˆ™ç´¢å¼• seq_lens - 1 ä¼šå¯¼è‡´ -1
        last_hidden_states = outputs[torch.arange(outputs.size(0)), seq_lens - 1]
        return last_hidden_states

    def calculate_loss(self, outputs, labels, input_ids):
        # 1. æå–æœ€åä¸€ä¸ªæœ‰æ•ˆçŠ¶æ€
        final_states = self.get_final_state(outputs, input_ids) # (B, D)
        
        # 2. é¢„æµ‹å¤´ (å¯é€‰)
        final_states = self.output_layer(final_states)
        
        # 3. è®¡ç®—ä¸æ‰€æœ‰ Item åµŒå…¥çš„ Logits (å…¨é‡ Logits)
        # æ’é™¤ Padding Item (ç´¢å¼• 0) çš„åµŒå…¥
        all_item_emb = self.item_embeddings.weight[1:] 
        
        # logits: (B, N_items) - N_items æ˜¯æ’é™¤ padding åçš„æ•°é‡
        logits = torch.matmul(final_states, all_item_emb.transpose(0, 1))
        
        # âš ï¸ ä¿®æ­£ 1: å®šä¹‰ Loss å‡½æ•°æ—¶è®¾ç½® ignore_index=-1
        loss_fct = nn.CrossEntropyLoss(ignore_index=-1)

        # ç›®æ ‡ Label (éœ€è¦å°† Item ID è¿˜åŸä¸º 0-based ç´¢å¼•)
        labels_0based = labels - 1 

        # âš ï¸ ä¿®æ­£ 2: é’³åˆ¶æ‰€æœ‰å°äº -1 çš„æ ‡ç­¾ï¼ˆä»¥é˜²ä¸‡ä¸€ï¼‰
        # è™½ç„¶ Item ID æ˜¯éè´Ÿçš„ï¼Œä½†ä¸ºäº†å®‰å…¨ï¼Œæˆ‘ä»¬é’³åˆ¶è´Ÿå€¼
        labels_0based = torch.clamp(labels_0based, min=-1)

        # CrossEntropyLoss
        loss = loss_fct(logits, labels_0based) 
        return loss

    
# =============== 3. ä¸»ç¨‹åºå…¥å£ (è®­ç»ƒå¾ªç¯å’Œæå–) ===============

def train_and_extract_cf_embeddings(args):
    print(f"ğŸ”¹ è®­ç»ƒ SASRec æ¨¡å‹ä»¥æå– CF åµŒå…¥: {args.dataset}")
    
    # 1. æ•°æ®åŠ è½½
    data_path = os.path.join(args.root, f'{args.dataset}.train.jsonl')
    dataset = CFDataset(data_path, args.max_len)
    dataloader = DataLoader(
        dataset, 
        batch_size=args.batch_size, 
        shuffle=True, 
        num_workers=args.num_workers,
        collate_fn=CFDataset.collate_fn
    )
    
    # 2. æ¨¡å‹åˆå§‹åŒ–
    model = SimplifiedSASRec(
        n_items=dataset.n_items,
        max_len=args.max_len,
        d_model=args.d_model,
        n_layers=args.n_layers,
        n_heads=args.n_heads,
        dropout_rate=args.dropout_rate
    ).to(args.device)
    
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)
    
    # 3. è®­ç»ƒå¾ªç¯
    model.train()
    for epoch in range(args.num_epochs):
        total_loss = 0
        pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}", ncols=100)
        for batch in pbar:
            input_ids = batch['input_ids'].to(args.device)
            labels = batch['labels'].to(args.device)

            optimizer.zero_grad()
            outputs = model(input_ids)
            loss = model.calculate_loss(outputs, labels, input_ids)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            pbar.set_postfix(loss=f"{loss.item():.4f}")

        avg_loss = total_loss / len(dataloader)
        print(f"Epoch {epoch+1} è®­ç»ƒå®Œæˆ, Avg Loss: {avg_loss:.4f}")

    # 4. æå–å’Œä¿å­˜ Item åµŒå…¥ (E_cf)
    model.eval()
    with torch.no_grad():
        # æ’é™¤ Padding ID 0
        cf_embeddings = model.item_embeddings.weight[1:].cpu().numpy()

    # 5. ä¿å­˜
    emb_dir = os.path.join(args.root, "embeddings")
    os.makedirs(emb_dir, exist_ok=True)
    save_path = os.path.join(emb_dir, f"{args.dataset}.emb-cf-sasrec.npy")
    np.save(save_path, cf_embeddings)
    
    print(f"\nâœ… ååŒè¿‡æ»¤åµŒå…¥å·²ä¿å­˜è‡³: {save_path}")
    print(f"   E_cf ç»´åº¦: {cf_embeddings.shape}")
    print("\nğŸ‰ CF åµŒå…¥ç”Ÿæˆä»»åŠ¡å®Œæˆã€‚")

def _get_all_item_ids(args, dataset_obj):
    """
    æ ¹æ®æ•°æ®é›†çš„å®é™…æƒ…å†µè·å–æ‰€æœ‰ Item IDã€‚
    å‡è®¾æ‰€æœ‰ Item ID æ˜¯ä» 1 åˆ° N_total_items è¿ç»­çš„ã€‚
    æˆ‘ä»¬éœ€è¦çŸ¥é“çœŸå®çš„ N_total_itemsã€‚
    è¿™é‡Œä¸´æ—¶ä½¿ç”¨è®­ç»ƒé›†æœ€å¤§ ID æ¥ä¼°ç®—ï¼Œä½†å®é™…åº”ç”¨ä¸­åº”è¯¥è¯»å–ä¸€ä¸ªå…¨å±€ Item åˆ—è¡¨æ–‡ä»¶ã€‚
    
    ä¸ºäº†æ¼”ç¤ºå†·å¯åŠ¨ï¼Œæˆ‘ä»¬å‡è®¾æ€»å…±æœ‰ 900 ä¸ª Itemï¼Œæœ€å¤§çš„ ID æ˜¯ 900ã€‚
    """
    
    # âš ï¸ ä¸´æ—¶ä»£ç ï¼šå‡è®¾æœ€å¤§ ID å°±æ˜¯ 900
    if args.dataset == 'Musical_Instruments' and dataset_obj.n_items == 899:
        # å¦‚æœè®­ç»ƒé›†æœ€å¤§ ID æ˜¯ 899ï¼Œæˆ‘ä»¬å‡è®¾æ€» Item ID æ˜¯ 900
        N_total_items = 900
    else:
        # å¦åˆ™ä½¿ç”¨è®­ç»ƒé›†çš„ max_id (ä¸ç†æƒ³ï¼Œä½†æ¼”ç¤ºç”¨)
        N_total_items = dataset_obj.n_items - 1 
        
    # è¿”å› Item ID åˆ—è¡¨ (ä» 1 åˆ° N_total_items)
    return list(range(1, N_total_items + 1))
# =============== 4. ä¸»ç¨‹åºå…¥å£å’Œå‚æ•°è§£æ ===============

def parse_args():
    parser = argparse.ArgumentParser(description="Generate CF Embeddings using Simplified SASRec")
    parser.add_argument('--dataset', type=str, default='Musical_Instruments', help='æ•°æ®é›†åç§°')
    parser.add_argument('--root', type=str, default="../datasets", help='æ•°æ®é›†æ ¹ç›®å½•')
    parser.add_argument('--gpu_id', type=int, default=0, help='GPU ID')
    parser.add_argument('--batch_size', type=int, default=256)
    parser.add_argument('--max_len', type=int, default=50, help='åºåˆ—æœ€å¤§é•¿åº¦')
    parser.add_argument('--num_workers', type=int, default=4, help='DataLoader çº¿ç¨‹æ•°')
    
    # SASRec æ¨¡å‹å‚æ•°
    parser.add_argument('--d_model', type=int, default=512, help='åµŒå…¥ç»´åº¦ (D_cf)')
    parser.add_argument('--n_layers', type=int, default=2, help='Transformer å±‚æ•°')
    parser.add_argument('--n_heads', type=int, default=4, help='Attention å¤´æ•°')
    parser.add_argument('--dropout_rate', type=float, default=0.1)
    parser.add_argument('--lr', type=float, default=0.001, help='å­¦ä¹ ç‡')
    parser.add_argument('--num_epochs', type=int, default=30)

    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    args.root = os.path.join(args.root, args.dataset)
    os.makedirs(args.root, exist_ok=True)
    args.device = set_device(args.gpu_id)
    
    train_and_extract_cf_embeddings(args)