import html
import json
import os
import pickle
import re
import time

import torch
# import gensim
from transformers import AutoModel, AutoTokenizer
from sklearn.preprocessing import StandardScaler
import collections
from typing import Any, Dict, List, Optional, Tuple
# import openai



def get_res_batch(model_name, prompt_list, max_tokens, api_info):

    while True:
        try:
            res = openai.Completion.create(
                model=model_name,
                prompt=prompt_list,
                temperature=0.4,
                max_tokens=max_tokens,
                top_p=1,
                frequency_penalty=0,
                presence_penalty=0
            )
            output_list = []
            for choice in res['choices']:
                output = choice['text'].strip()
                output_list.append(output)

            return output_list

        except openai.error.AuthenticationError as e:
            print(e)
            openai.api_key = api_info["api_key_list"].pop()
            time.sleep(10)
        except openai.error.RateLimitError as e:
            print(e)
            if str(e) == "You exceeded your current quota, please check your plan and billing details.":
                openai.api_key = api_info["api_key_list"].pop()
                time.sleep(10)
            else:
                print('\nopenai.error.RateLimitError\nRetrying...')
                time.sleep(10)
        except openai.error.ServiceUnavailableError as e:
            print(e)
            print('\nopenai.error.ServiceUnavailableError\nRetrying...')
            time.sleep(10)
        except openai.error.Timeout:
            print('\nopenai.error.Timeout\nRetrying...')
            time.sleep(10)
        except openai.error.APIError as e:
            print(e)
            print('\nopenai.error.APIError\nRetrying...')
            time.sleep(10)
        except openai.error.APIConnectionError as e:
            print(e)
            print('\nopenai.error.APIConnectionError\nRetrying...')
            time.sleep(10)
        except Exception as e:
            print(e)
            return None




def check_path(path):
    if not os.path.exists(path):
        os.makedirs(path)


def set_device(gpu_id):
    if gpu_id == -1:
        return torch.device('cpu')
    else:
        return torch.device(
            'cuda:' + str(gpu_id) if torch.cuda.is_available() else 'cpu')

def load_plm(model_path='bert-base-uncased', kwargs=None):

    tokenizer = AutoTokenizer.from_pretrained(model_path, **kwargs)

    print("Load Model:", model_path)

    model = AutoModel.from_pretrained(model_path, low_cpu_mem_usage=True, **kwargs)
    return tokenizer, model

def load_json(file):
    with open(file, 'r') as f:
        data = json.load(f)
    return data

def clean_text(raw_text):
    if isinstance(raw_text, list):
        new_raw_text=[]
        for raw in raw_text:
            raw = html.unescape(raw)
            raw = re.sub(r'</?\w+[^>]*>', '', raw)
            raw = re.sub(r'["\n\r]*', '', raw)
            new_raw_text.append(raw.strip())
        cleaned_text = ' '.join(new_raw_text)
    else:
        if isinstance(raw_text, dict):
            cleaned_text = str(raw_text)[1:-1].strip()
        else:
            cleaned_text = raw_text.strip()
        cleaned_text = html.unescape(cleaned_text)
        cleaned_text = re.sub(r'</?\w+[^>]*>', '', cleaned_text)
        cleaned_text = re.sub(r'["\n\r]*', '', cleaned_text)
    index = -1
    while -index < len(cleaned_text) and cleaned_text[index] == '.':
        index -= 1
    index += 1
    if index == 0:
        cleaned_text = cleaned_text + '.'
    else:
        cleaned_text = cleaned_text[:index] + '.'
    if len(cleaned_text) >= 2000:
        cleaned_text = ''
    return cleaned_text

def load_pickle(filename):
    with open(filename, "rb") as f:
        return pickle.load(f)


def make_inters_in_order(inters):
    user2inters, new_inters = collections.defaultdict(list), list()
    for inter in inters:
        user, item, rating, timestamp = inter
        user2inters[user].append((user, item, rating, timestamp))
    for user in user2inters:
        user_inters = user2inters[user]
        user_inters.sort(key=lambda d: d[3])
        for inter in user_inters:
            new_inters.append(inter)
    return new_inters

def write_json_file(dic, file):
    print('Writing json file: ',file)
    with open(file, 'w') as fp:
        json.dump(dic, fp, indent=4)

def write_remap_index(unit2index, file):
    print('Writing remap file: ',file)
    with open(file, 'w') as fp:
        for unit in unit2index:
            fp.write(unit + '\t' + str(unit2index[unit]) + '\n')

# preprocessing/utils.py (å¢å¼ºç‰ˆ)

import html
import json
import os
import pickle
import re
import time
import argparse # å¯¼å…¥ argparse ç”¨äºç±»å‹æç¤º
from pathlib import Path # ä½¿ç”¨ Path å¯¹è±¡å¤„ç†è·¯å¾„
import joblib # ç”¨äºä¿å­˜ PCA æ¨¡å‹
import numpy as np
from PIL import Image, UnidentifiedImageError
from sklearn.decomposition import PCA
import torch
import collections
# (ç§»é™¤ä¸å†éœ€è¦çš„ transformers, openai ç­‰ç‰¹å®šåº“å¯¼å…¥ï¼Œè¿™äº›åº”åœ¨å„è‡ª encoder æ¨¡å—ä¸­)

# ====================================================
# ========= æ ¸å¿ƒé€šç”¨å‡½æ•° (æ¥è‡ªæ‚¨åŸæœ‰çš„ utils.py) =========
# ====================================================

def check_path(path):
    """(ä¿æŒä¸å˜) ç¡®ä¿ç›®å½•å­˜åœ¨"""
    # ä½¿ç”¨ Path å¯¹è±¡æ›´ä½³
    Path(path).mkdir(parents=True, exist_ok=True)

def set_device(gpu_id: int) -> torch.device:
    """(ä¿æŒä¸å˜) è®¾ç½® PyTorch è®¾å¤‡"""
    if gpu_id < 0:
        print("[INFO] Using CPU.")
        return torch.device('cpu')
    elif torch.cuda.is_available():
        device = torch.device(f'cuda:{gpu_id}')
        print(f"[INFO] Using CUDA device: {torch.cuda.get_device_name(device)}")
        return device
    else:
        print("[WARN] CUDA not available, falling back to CPU.")
        return torch.device('cpu')

def load_json(file: str) -> Any:
    """(ä¿æŒä¸å˜) æ›´å¥å£®åœ°åŠ è½½ JSON æ–‡ä»¶"""
    if not os.path.exists(file):
        print(f"[WARN] JSON file not found: {file}")
        return None
    try:
        with open(file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data
    except json.JSONDecodeError:
        print(f"[ERROR] Failed to decode JSON file: {file}")
        return None
    except Exception as e:
        print(f"[ERROR] Failed to load JSON file {file}: {e}")
        return None

def clean_text(raw_text: Any) -> str:
    """(ä¿æŒä¸å˜) æ¸…ç†æ–‡æœ¬ä¸­çš„ HTML æ ‡ç­¾ã€æ¢è¡Œç¬¦ç­‰"""
    text_to_clean = ""
    if isinstance(raw_text, list):
        text_to_clean = ' '.join(str(item) for item in raw_text)
    elif isinstance(raw_text, dict):
         # å°†å­—å…¸è½¬æ¢ä¸º "key1: value1, key2: value2" æ ¼å¼ (æˆ–æ ¹æ®éœ€è¦è°ƒæ•´)
        text_to_clean = ", ".join(f"{k}: {v}" for k, v in raw_text.items())
    elif isinstance(raw_text, (str, int, float)):
         text_to_clean = str(raw_text)
    else:
         return "" # å¯¹äºæ— æ³•å¤„ç†çš„ç±»å‹è¿”å›ç©º

    try:
        cleaned_text = html.unescape(text_to_clean)
        cleaned_text = re.sub(r'<[^>]+>', '', cleaned_text) # ç§»é™¤ HTML æ ‡ç­¾
        cleaned_text = re.sub(r'[\n\r]+', ' ', cleaned_text) # æ›¿æ¢æ¢è¡Œç¬¦ä¸ºç©ºæ ¼
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text) # åˆå¹¶å¤šä¸ªç©ºæ ¼
        cleaned_text = cleaned_text.replace('"', '').strip() # ç§»é™¤å¼•å·å¹¶å»é™¤é¦–å°¾ç©ºæ ¼
        # (ç§»é™¤æœ«å°¾å¥ç‚¹é€»è¾‘ï¼Œå¯èƒ½ä¸éœ€è¦æˆ–è¿‡äºç‰¹å®š)
        # index = -1 ...
    except Exception as e:
        # print(f"[WARN] Error cleaning text: {raw_text}. Error: {e}")
        cleaned_text = "" # å‡ºé”™æ—¶è¿”å›ç©ºå­—ç¬¦ä¸²

    # (ç§»é™¤é•¿åº¦é™åˆ¶é€»è¾‘ï¼Œè®©è°ƒç”¨è€…å†³å®š)
    # if len(cleaned_text) >= 2000: cleaned_text = ''
    return cleaned_text

def load_pickle(filename: str) -> Any:
    """(ä¿æŒä¸å˜) åŠ è½½ Pickle æ–‡ä»¶"""
    if not os.path.exists(filename):
         print(f"[WARN] Pickle file not found: {filename}")
         return None
    try:
        with open(filename, "rb") as f:
            return pickle.load(f)
    except Exception as e:
         print(f"[ERROR] Failed to load pickle file {filename}: {e}")
         return None

def write_json_file(dic: dict, file: str):
    """(ä¿æŒä¸å˜) å†™å…¥ JSON æ–‡ä»¶"""
    print(f'Writing json file: {file}')
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(file), exist_ok=True)
        with open(file, 'w', encoding='utf-8') as fp:
            json.dump(dic, fp, indent=4, ensure_ascii=False) # ä½¿ç”¨ ensure_ascii=False ä¿ç•™é ASCII å­—ç¬¦
    except Exception as e:
         print(f"[ERROR] Failed to write JSON file {file}: {e}")

def write_remap_index(unit2index: dict, file: str):
    """(ä¿æŒä¸å˜) å†™å…¥ remap æ–‡ä»¶ (ID æ˜ å°„)"""
    print(f'Writing remap file: {file}')
    try:
        os.makedirs(os.path.dirname(file), exist_ok=True)
        with open(file, 'w', encoding='utf-8') as fp:
            # æŒ‰ index æ’åºå†™å…¥ï¼Œä¿è¯ä¸€è‡´æ€§
            for unit, index in sorted(unit2index.items(), key=lambda item: int(item[1])):
                fp.write(f"{unit}\t{index}\n")
    except Exception as e:
        print(f"[ERROR] Failed to write remap file {file}: {e}")

# ========================================================
# ========= æ–°å¢ï¼šç”¨äº Embedding ç”Ÿæˆçš„é€šç”¨å‡½æ•° =========
# ========================================================

def get_id2item_dict(item2id_file: str) -> Dict[str, str]:
    """
    (æ–°å¢) ä» .item2id æ–‡ä»¶åŠ è½½ æ–°ID(str) -> åŸå§‹ID(str) çš„æ˜ å°„ã€‚
    """
    if not os.path.exists(item2id_file):
        raise FileNotFoundError(f"item2id æ–‡ä»¶æœªæ‰¾åˆ°: {item2id_file}")
    id2item = {}
    try:
        with open(item2id_file, "r", encoding='utf-8') as fp:
            for line_num, line in enumerate(fp):
                parts = line.strip().split("\t")
                if len(parts) == 2:
                    item, item_id = parts # item æ˜¯åŸå§‹ID, item_id æ˜¯æ–°ID (str)
                    id2item[item_id] = item
                elif line.strip(): # å¿½ç•¥ç©ºè¡Œï¼Œä½†å¯¹æ ¼å¼é”™è¯¯çš„è¡Œå‘å‡ºè­¦å‘Š
                     print(f"[WARN] item2id æ–‡ä»¶ç¬¬ {line_num+1} è¡Œæ ¼å¼é”™è¯¯: '{line.strip()}'")
        if not id2item:
            raise RuntimeError(f"æœªèƒ½ä» {item2id_file} åŠ è½½ä»»ä½• ID æ˜ å°„ã€‚")
    except Exception as e:
        print(f"[ERROR] è¯»å– item2id æ–‡ä»¶å¤±è´¥ ({item2id_file}): {e}")
        raise # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œå› ä¸ºè¿™æ˜¯å…³é”®æ•°æ®
    return id2item

def build_text_map(args: argparse.Namespace, id2item: Dict[str, str], item_meta: Dict[str, Dict]) -> Dict[str, str]:
    """
    (æ–°å¢) æ ¹æ® dataset_type ä» item_meta æ„å»º åŸå§‹ID(str) -> æ¸…ç†åæ–‡æœ¬(str) çš„æ˜ å°„ã€‚
    """
    if not item_meta:
         print("[WARN] item_meta ä¸ºç©ºï¼Œæ— æ³•æ„å»º text_mapã€‚")
         return {}

    # æ ¹æ® dataset_type é€‰æ‹©å­—æ®µ
    features = []
    if args.dataset_type == 'movielens':
        features = ['title', 'description', 'genres', 'year'] # æ·»åŠ  year
    elif args.dataset_type == 'amazon':
        features = ['title', 'description', 'brand', 'categories']
    else:
        print(f"[WARN] æœªçŸ¥çš„ dataset_type: {args.dataset_type}ï¼Œå°†å°è¯•ä½¿ç”¨é€šç”¨å­—æ®µ ['title', 'description']")
        features = ['title', 'description']

    print(f"å°†ä½¿ç”¨ä»¥ä¸‹å…ƒæ•°æ®å­—æ®µæ„å»ºæ–‡æœ¬: {features}")
    text_map = {}
    missing_meta_count = 0
    
    # éå† id2item æ¥ç¡®ä¿æˆ‘ä»¬åªå¤„ç†æœ‰æ•ˆçš„ item
    for new_id_str, orig_id in id2item.items():
        # item_meta çš„é”®æ˜¯æ–°ID(str)
        meta_data = item_meta.get(new_id_str) 
        if not meta_data:
             missing_meta_count += 1
             text_map[orig_id] = "N/A" # æˆ–è€… ""
             continue

        parts = []
        for f in features:
            if f in meta_data:
                val = meta_data[f]
                # å¯¹åˆ—è¡¨ç‰¹æ®Šå¤„ç† (ä¾‹å¦‚ genres)
                if isinstance(val, list):
                    val = ", ".join(clean_text(str(x)) for x in val if str(x).strip()) # æ¸…ç†åˆ—è¡¨å…ƒç´ 
                else:
                    val = clean_text(str(val)) # æ¸…ç†æ™®é€šå­—æ®µ
                
                if val: # åªæ·»åŠ éç©ºå­—æ®µ
                    # å¯ä»¥è€ƒè™‘æ·»åŠ å­—æ®µåå‰ç¼€ï¼Œä¾‹å¦‚ "Title: ... Brand: ..."
                    # parts.append(f"{f.capitalize()}: {val}") 
                    parts.append(val)
                    
        text = " ".join(parts).strip()
        text_map[orig_id] = text if text else "N/A" # ä¿è¯è‡³å°‘æ˜¯ "N/A"

    if missing_meta_count > 0:
         print(f"[WARN] {missing_meta_count} ä¸ª item åœ¨ item.json ä¸­ç¼ºå°‘å…ƒæ•°æ®ã€‚")
         
    return text_map

def find_first_image_path(original_item_id: str, images_info: Dict[str, List[str]], image_dir: str) -> Optional[str]:
    """(æ–°å¢) ä¸ºç»™å®š item æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå­˜åœ¨çš„å›¾åƒæ–‡ä»¶è·¯å¾„"""
    if not images_info: return None # å¦‚æœ images_info æœªåŠ è½½åˆ™è¿”å› None
    
    names = images_info.get(original_item_id, [])
    if not isinstance(names, list): names = []
    
    for name in names:
        if not isinstance(name, str) or not name: continue
        # è€ƒè™‘è·¯å¾„åˆ†éš”ç¬¦é—®é¢˜ï¼Œä½¿ç”¨ os.path.join
        fp = os.path.join(image_dir, name)
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”éç©º
        if os.path.exists(fp) and os.path.getsize(fp) > 0: 
            return fp
    return None # æœªæ‰¾åˆ°æˆ–æ‰€æœ‰æ–‡ä»¶éƒ½æ— æ•ˆ

def load_pil_image(img_path: Optional[str]) -> Optional[Image.Image]:
    """(æ–°å¢) å®‰å…¨åœ°åŠ è½½ PIL å›¾åƒï¼Œå¤±è´¥æ—¶è¿”å› None"""
    if img_path is None: return None
    try:
        img = Image.open(img_path)
        # ç¡®ä¿æ˜¯ RGB æ ¼å¼
        if img.mode != 'RGB':
             img = img.convert('RGB')
        return img
    except (UnidentifiedImageError, FileNotFoundError, OSError, Exception) as e: 
        # print(f"[WARN] Failed to load image {os.path.basename(img_path)}: {e}") # å¯èƒ½æ‰“å°è¿‡å¤š
        return None

def build_output_path(args: argparse.Namespace, modality_tag: str, model_tag: str) -> str:
    """(æ–°å¢) æ„å»ºæ ‡å‡†åŒ–çš„ embedding è¾“å‡ºè·¯å¾„"""
    emb_dir = os.path.join(args.save_root, args.dataset, "embeddings")
    check_path(emb_dir) # ä½¿ç”¨ check_path ç¡®ä¿ç›®å½•å­˜åœ¨
    
    # æ¸…ç† model_tag ä¸­çš„éæ³•å­—ç¬¦
    safe_model_tag = model_tag.split('/')[-1].replace('/', '-').replace('\\', '-')
    
    filename = f"{args.dataset}.emb-{modality_tag}-{safe_model_tag}.npy"
    return os.path.join(emb_dir, filename)

def apply_pca_and_save(original_embeddings: np.ndarray, args: argparse.Namespace, output_path: str) -> str:
    """
    (ä¿®æ”¹ç‰ˆ) å¯¹ embeddings åº”ç”¨ PCA å¹¶ä¿å­˜ã€‚
    å¦‚æœ pca_dim > 0ï¼Œåˆ™åŒæ—¶ä¿å­˜åŸå§‹æ–‡ä»¶å’Œ PCA é™ç»´åçš„ .npy æ–‡ä»¶ã€‚
    ä¸å†ä¿å­˜ .pca å’Œ .scaler æ¨¡å‹æ–‡ä»¶ã€‚
    è¿”å›æœ€ç»ˆä½¿ç”¨çš„æ–‡ä»¶è·¯å¾„ï¼ˆPCA åçš„è·¯å¾„ï¼Œå¦‚æœæ‰§è¡Œäº† PCAï¼‰ã€‚
    """
    pca_dim = getattr(args, 'pca_dim', 0) 

    if not isinstance(original_embeddings, np.ndarray) or original_embeddings.size == 0:
         print("[ERROR] è¾“å…¥çš„ embeddings æ— æ•ˆï¼Œæ— æ³•è¿›è¡Œ PCA æˆ–ä¿å­˜ã€‚")
         return "" 

    # --- æƒ…å†µ 1 & 2: ä¸æ‰§è¡Œ PCA æˆ–æ— æ³•æ‰§è¡Œ ---
    if pca_dim <= 0 or original_embeddings.shape[1] <= pca_dim:
        if pca_dim <= 0:
            print("pca_dim <= 0ï¼Œè·³è¿‡ PCAã€‚")
        else:
            print(f"åŸå§‹ç»´åº¦ ({original_embeddings.shape[1]}) <= ç›®æ ‡ç»´åº¦ ({pca_dim})ï¼Œè·³è¿‡ PCAã€‚")
        
        try:
            np.save(output_path, original_embeddings)
            print(f"âœ… åŸå§‹åµŒå…¥å·²ä¿å­˜è‡³: {output_path} (ç»´åº¦: {original_embeddings.shape})")
            return output_path # è¿”å›åŸå§‹è·¯å¾„
        except Exception as e:
            print(f"[ERROR] ä¿å­˜åŸå§‹åµŒå…¥å¤±è´¥ ({output_path}): {e}")
            return "" 

    # --- æƒ…å†µ 3: æ‰§è¡Œ PCA ---
    print(f"\nåº”ç”¨ PCA é™ç»´ï¼Œç›®æ ‡ç»´åº¦: {pca_dim}")
    try:
        # âœ… ç¬¬ä¸€æ­¥ï¼šå…ˆä¿å­˜åŸå§‹æ–‡ä»¶
        np.save(output_path, original_embeddings)
        print(f"âœ… åŸå§‹åµŒå…¥å·²ä¿å­˜è‡³: {output_path} (ç»´åº¦: {original_embeddings.shape})")

        # âœ… ç¬¬äºŒæ­¥ï¼šæ‰§è¡Œ PCA
        pca = PCA(n_components=pca_dim, svd_solver="auto", random_state=42)
        print("  -> æ ‡å‡†åŒ–æ•°æ® (StandardScaler)...")
        scaler = StandardScaler()
        scaled_embeddings = scaler.fit_transform(original_embeddings)
        
        print(f"  -> æ‰§è¡Œ PCA (n_components={pca_dim})...")
        reduced_emb = pca.fit_transform(scaled_embeddings).astype(np.float32)
        kept_variance = float(np.sum(pca.explained_variance_ratio_))

        # âœ… ç¬¬ä¸‰æ­¥ï¼šæ„å»º PCA å‘é‡æ–‡ä»¶è·¯å¾„
        output_path_obj = Path(output_path)
        base = output_path_obj.with_suffix("") # å»æ‰ .npy
        pca_vec_path_str = f"{base}-pca{pca_dim}.npy"
        
        # ğŸš¨ (ç§»é™¤) pca_model_path_str = f"{base}-pca{pca_dim}.pca"
        # ğŸš¨ (ç§»é™¤) scaler_model_path_str = f"{base}-pca{pca_dim}.scaler"

        # âœ… ç¬¬å››æ­¥ï¼šä¿å­˜ PCA å‘é‡æ–‡ä»¶
        np.save(pca_vec_path_str, reduced_emb)
        
        # ğŸš¨ (ç§»é™¤) joblib.dump(pca, pca_model_path_str)
        # ğŸš¨ (ç§»é™¤) joblib.dump(scaler, scaler_model_path_str) 

        print(f"âœ… PCA å‘é‡å·²ä¿å­˜: {pca_vec_path_str}  å½¢çŠ¶={reduced_emb.shape}  ä¿ç•™æ–¹å·®={kept_variance:.4f}")
        # ğŸš¨ (ç§»é™¤) print(f"âœ… PCA æ¨¡å‹å·²ä¿å­˜: ...")
        # ğŸš¨ (ç§»é™¤) print(f"âœ… Scaler æ¨¡å‹å·²ä¿å­˜: ...")
        
        # âœ… ç¬¬äº”æ­¥ï¼šè¿”å› PCA é™ç»´åçš„æ–‡ä»¶è·¯å¾„
        return pca_vec_path_str 

    except Exception as e:
        print(f"[ERROR] PCA å¤±è´¥: {e}")
        print(f"PCA å¤±è´¥ï¼Œä½†åŸå§‹åµŒå…¥å¯èƒ½å·²ä¿å­˜è‡³: {output_path}")
        return output_path if os.path.exists(output_path) else ""