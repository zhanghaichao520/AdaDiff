# preprocessing/generate_embeddings/cf_embedding.py

import os
import sys
import argparse
import json
import time
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
from tqdm import tqdm

# (ç”¨äºå¯¼å…¥ä¸Šçº§ç›®å½•çš„ utils)
try:
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from utils import load_json, set_device
except ImportError:
    print("è­¦å‘Š: æ— æ³•è‡ªåŠ¨å¯¼å…¥ utils.pyã€‚å°†ä½¿ç”¨æœ¬åœ°çš„ load_jsonã€‚")
    
    def load_json(file):
        if not os.path.exists(file): return {}
        with open(file, 'r') as f:
            return json.load(f)
            
    def set_device(gpu_id):
        if torch.cuda.is_available() and gpu_id >= 0:
            return torch.device(f'cuda:{gpu_id}')
        else:
            return torch.device('cpu')

# =================================================================
# ================== 1. æ•°æ®é›†å’ŒåŠ è½½å™¨ ==================
# =================================================================

class SASRecDataset(Dataset):
    """
    ç”¨äºåŠ è½½ .jsonl æ–‡ä»¶çš„ PyTorch æ•°æ®é›†ã€‚
    
    .jsonl æ ¼å¼: {"user": "0", "history": ["2803"], "target": "6913"}
    """
    def __init__(self, data_path, max_seq_len, n_items):
        self.data_path = data_path
        self.max_seq_len = max_seq_len
        self.n_items = n_items
        
        self.lines = []
        with open(data_path, 'r') as f:
            for line in f:
                self.lines.append(line.strip())

    def __len__(self):
        return len(self.lines)

    def __getitem__(self, idx):
        # 1. åŠ è½½æ•°æ®
        line = self.lines[idx]
        data = json.loads(line)
        
        # 2. è½¬æ¢ ID
        # (é‡è¦) æˆ‘ä»¬å°†æ‰€æœ‰ item_id + 1ã€‚
        # å› ä¸º 0 å·ç´¢å¼•å°†ä¿ç•™ç»™ [PAD] å¡«å……æ ‡è®°ã€‚
        history_ids = [int(i) + 1 for i in data["history"]]
        target_id = int(data["target"]) + 1
        
        # 3. æˆªæ–­/å¡«å……åºåˆ—
        
        # è·å–åºåˆ—
        seq = history_ids[-self.max_seq_len:]
        
        # è·å–çœŸå®é•¿åº¦ï¼ˆç”¨äºåç»­æ¨¡å‹æå–æœ€åä¸€ä¸ª itemï¼‰
        seq_len = len(seq)
        
        # å¡«å……
        padding_len = self.max_seq_len - seq_len
        seq = seq + [0] * padding_len # ä½¿ç”¨ 0 ä½œä¸º [PAD]
        
        # 4. è½¬æ¢ä¸º Tensors
        # seq æ˜¯ä¸€é•¿ä¸² 0-padded åºåˆ—
        # target æ˜¯å•ä¸€çš„ç›®æ ‡ item
        # seq_len æ˜¯ history çš„çœŸå®é•¿åº¦
        return torch.tensor(seq, dtype=torch.long), \
               torch.tensor(target_id, dtype=torch.long), \
               torch.tensor(seq_len, dtype=torch.long)

# =================================================================
# ================== 2. SASRec æ¨¡å‹ ==================
# =================================================================

class SASRecModel(nn.Module):
    def __init__(self, n_items, hidden_dim, max_seq_len, n_layers, n_heads, dropout=0.1):
        super(SASRecModel, self).__init__()
        self.n_items = n_items
        self.hidden_dim = hidden_dim
        
        # n_items + 1 (å› ä¸º 0 æ˜¯ [PAD] æ ‡è®°)
        self.item_embedding = nn.Embedding(self.n_items + 1, hidden_dim, padding_idx=0)
        
        # ä½ç½®åµŒå…¥
        self.position_embedding = nn.Embedding(max_seq_len, hidden_dim)
        
        self.emb_dropout = nn.Dropout(dropout)
        
        # Transformer ç¼–ç å™¨
        # (ä½¿ç”¨ batch_first=True ä»¥ä¾¿è¾“å…¥æ˜¯ [B, L, D])
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=n_heads,
            dim_feedforward=hidden_dim * 4,
            dropout=dropout,
            batch_first=True, 
            activation='gelu'
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)
        
        self.layer_norm = nn.LayerNorm(hidden_dim)

    def forward(self, item_seq, seq_lengths):
        """
        Args:
            item_seq (torch.Tensor): [B, L] 0-padded item id åºåˆ—
            seq_lengths (torch.Tensor): [B] æ¯ä¸ªåºåˆ—çš„çœŸå®é•¿åº¦

        Returns:
            torch.Tensor: [B, n_items+1] é¢„æµ‹ logits
        """
        # 1. åµŒå…¥
        item_emb = self.item_embedding(item_seq) # [B, L, D]
        
        # ä½ç½®åµŒå…¥
        # [L] -> [1, L] -> [B, L] -> [B, L, D]
        pos_ids = torch.arange(item_seq.size(1), device=item_seq.device).unsqueeze(0)
        pos_emb = self.position_embedding(pos_ids)
        
        x = item_emb + pos_emb
        x = self.emb_dropout(x) # [B, L, D]
        
        # 2. Transformer 
        # (åˆ›å»º padding mask)
        # item_seq == 0 çš„åœ°æ–¹æ˜¯ True, å…¶ä»–åœ°æ–¹æ˜¯ False
        padding_mask = (item_seq == 0) # [B, L]
        
        # Transformer æœŸæœ› True çš„åœ°æ–¹æ˜¯ "è¢«é®è”½çš„"
        transformer_out = self.transformer_encoder(
            x, 
            src_key_padding_mask=padding_mask
        ) # [B, L, D]
        
        transformer_out = self.layer_norm(transformer_out)
        
        # 3. æå–æœ€åä¸€ä¸ªæœ‰æ•ˆ item çš„è¡¨ç¤º
        # æˆ‘ä»¬éœ€è¦ä» [B, L, D] ä¸­æå– [B, D]
        # B = æ‰¹æ¬¡å¤§å°
        batch_indices = torch.arange(transformer_out.size(0), device=transformer_out.device)
        
        # çœŸå®é•¿åº¦ - 1 = æœ€åä¸€ä¸ª item çš„ç´¢å¼•
        # (å› ä¸º seq_len æ˜¯ 1-based, ç´¢å¼•æ˜¯ 0-based)
        last_item_indices = seq_lengths - 1 # [B]
        
        # ä½¿ç”¨é«˜çº§ç´¢å¼•
        last_item_emb = transformer_out[batch_indices, last_item_indices, :] # [B, D]
        
        # 4. è®¡ç®— Logits (é¢„æµ‹)
        # ä½¿ç”¨ "tied weights": é¢„æµ‹å±‚çš„æƒé‡å°±æ˜¯åµŒå…¥çŸ©é˜µçš„è½¬ç½®
        logits = last_item_emb @ self.item_embedding.weight.T # [B, D] @ [D, n_items+1] -> [B, n_items+1]
        
        return logits

# =================================================================
# ================== 3. ä¸»è®­ç»ƒæµç¨‹ ==================
# =================================================================

def main(args):
    # 1. è®¾ç½®
    device = set_device(args.gpu_id)
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 2. æ„å»ºè·¯å¾„
    data_dir = os.path.join(args.data_root, args.dataset)
    train_path = os.path.join(data_dir, f"{args.dataset}.train.jsonl")
    item_meta_path = os.path.join(data_dir, f"{args.dataset}.item.json")
    
    if not os.path.exists(train_path):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°è®­ç»ƒæ–‡ä»¶ {train_path}")
        sys.exit(1)
        
    print("åŠ è½½ item å…ƒæ•°æ®ä»¥è·å– n_items...")
    item_meta = load_json(item_meta_path)
    if not item_meta:
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æˆ–æ— æ³•åŠ è½½ item å…ƒæ•°æ® {item_meta_path}")
        sys.exit(1)
    
    n_items = len(item_meta)
    print(f"æ•°æ®é›†: {args.dataset}, ç‰©å“æ€»æ•°: {n_items}")

    # 3. åˆ›å»º Dataset å’Œ DataLoader
    print("åŠ è½½è®­ç»ƒæ•°æ®...")
    train_dataset = SASRecDataset(train_path, args.max_seq_len, n_items)
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=4,
        pin_memory=True
    )

    # 4. åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    model = SASRecModel(
        n_items=n_items,
        hidden_dim=args.hidden_dim,
        max_seq_len=args.max_seq_len,
        n_layers=args.n_layers,
        n_heads=args.n_heads,
        dropout=args.dropout
    ).to(device)
    
    # CrossEntropyLoss è‡ªåŠ¨å¤„ç† logits (ä¸éœ€è¦ softmax)
    # æˆ‘ä»¬é¢„æµ‹ [B, n_items+1]
    # ç›®æ ‡æ˜¯ [B] (ä¸” target ID å·²ç»æ˜¯ 1-based äº†)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    
    print("å¼€å§‹è®­ç»ƒ...")
    
    for epoch in range(1, args.epochs + 1):
        model.train()
        total_loss = 0
        
        pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{args.epochs}")
        
        for batch in pbar:
            # seq: [B, L], target: [B], seq_len: [B]
            seq, target, seq_len = [b.to(device) for b in batch]
            
            optimizer.zero_grad()
            
            # é¢„æµ‹ [B, n_items+1]
            logits = model(seq, seq_len)
            
            # è®¡ç®—æŸå¤± (target æ˜¯ [B])
            loss = criterion(logits, target)
            
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            pbar.set_postfix({"Loss": loss.item()})
            
        avg_loss = total_loss / len(train_loader)
        print(f"Epoch {epoch} å®Œæˆ. å¹³å‡æŸå¤±: {avg_loss:.4f}")

    print("è®­ç»ƒå®Œæˆ.")
    
    # 5. æå–å’Œä¿å­˜åµŒå…¥
    print("æ­£åœ¨æå–ç‰©å“åµŒå…¥...")
    model.eval()
    
    # æå–æ•´ä¸ªåµŒå…¥çŸ©é˜µ [n_items+1, D]
    embeddings = model.item_embedding.weight.data.cpu().numpy()
    
    # (é‡è¦) ç§»é™¤ 0 å·ç´¢å¼•çš„ [PAD] åµŒå…¥
    embeddings = embeddings[1:] # [n_items, D]
    
    print(f"æå–çš„åµŒå…¥ç»´åº¦: {embeddings.shape}")
    
    # 6. ä¿å­˜
    save_dir = os.path.join(data_dir, "embeddings")
    os.makedirs(save_dir, exist_ok=True)
    
    save_path = os.path.join(save_dir, f"{args.dataset}.emb-cf-sasrec.npy")
    np.save(save_path, embeddings)
    
    print(f"ğŸ‰ ååŒè¿‡æ»¤åµŒå…¥å·²ä¿å­˜åˆ°: {save_path}")


def parse_args():
    parser = argparse.ArgumentParser(description="ä½¿ç”¨ SASRec è®­ç»ƒååŒè¿‡æ»¤åµŒå…¥")
    
    # --- æ•°æ®å’Œè·¯å¾„ ---
    parser.add_argument('--dataset', type=str, required=True,
                        help='æ•°æ®é›†åç§° (ä¾‹å¦‚: Baby, ml-1m)')
    parser.add_argument('--data_root', type=str, default="../datasets",
                        help='æ•°æ®é›†çš„æ ¹ç›®å½•')
    
    # --- æ¨¡å‹è¶…å‚æ•° ---
    parser.add_argument('--hidden_dim', type=int, default=64,
                        help='åµŒå…¥å’Œæ¨¡å‹çš„éšè—ç»´åº¦')
    parser.add_argument('--max_seq_len', type=int, default=50,
                        help='åºåˆ—çš„æœ€å¤§é•¿åº¦')
    parser.add_argument('--n_layers', type=int, default=2,
                        help='Transformer ç¼–ç å™¨çš„å±‚æ•°')
    parser.add_argument('--n_heads', type=int, default=2,
                        help='Transformer çš„å¤´æ•°')
    parser.add_argument('--dropout', type=float, default=0.2,
                        help='Dropout æ¦‚ç‡')
    
    # --- è®­ç»ƒè¶…å‚æ•° ---
    parser.add_argument('--epochs', type=int, default=30,
                        help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=256,
                        help='æ‰¹é‡å¤§å°')
    parser.add_argument('--lr', type=float, default=0.001,
                        help='å­¦ä¹ ç‡')
    parser.add_argument('--weight_decay', type=float, default=0.0,
                        help='L2 æ­£åˆ™åŒ– (æƒé‡è¡°å‡)')
    
    # --- è®¾å¤‡ ---
    parser.add_argument('--gpu_id', type=int, default=0,
                        help='è¦ä½¿ç”¨çš„ GPU ID (å¦‚æœ < 0 åˆ™ä½¿ç”¨ CPU)')
    
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_args()
    main(args)