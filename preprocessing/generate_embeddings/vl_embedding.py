# preprocessing/generate_embeddings/vl_embedding.py
"""
ä½¿ç”¨ VLM (è¦–è¦ºèªè¨€æ¨¡å‹) æå–æ·±åº¦èåˆçš„å¤šæ¨¡æ…‹ Embeddingã€‚
ç„¡éœ€è¨“ç·´ï¼Œç›´æ¥åˆ©ç”¨é è¨“ç·´ VLM çš„èƒ½åŠ›ã€‚
"""

import os
import sys
import json
import argparse
import numpy as np
from PIL import Image, UnidentifiedImageError
from tqdm import tqdm
from typing import List, Dict, Any

import torch
# ğŸš¨ ç§»é™¤ nn, functional, DataLoader, Dataset ç­‰è¨“ç·´ç›¸é—œå°å…¥

from sklearn.decomposition import PCA
# âœ… å°å…¥ VLM ç›¸é—œåº«
from transformers import AutoProcessor, AutoModelForCausalLM # ä½¿ç”¨ AutoModel æ›´é€šç”¨
# from transformers import Qwen3VLForConditionalGeneration # æˆ–è€…ç›´æ¥æŒ‡å®š Qwen


# ----------------- utils (ä¿æŒä¸è®Š) -----------------
def load_json(p):
    try:
        with open(p, "r") as f:
            return json.load(f)
    except Exception as e:
        print(f"[ERR] load_json({p}): {e}")
        return None

def get_id2item_dict(item2id_file):
    if not os.path.exists(item2id_file):
        raise FileNotFoundError(f"item2id not found: {item2id_file}")
    id2item = {}
    with open(item2id_file, "r") as fp:
        for line in fp:
            try:
                item, item_id = line.strip().split("\t")
                id2item[item_id] = item
            except ValueError:
                continue
    if not id2item:
        raise RuntimeError("id2item is empty.")
    return id2item

def build_text_map(args, id2item):
    item_json = os.path.join(args.save_root, args.dataset, f"{args.dataset}.item.json")
    data = load_json(item_json)
    if not isinstance(data, dict):
        raise FileNotFoundError(f"Invalid item.json: {item_json}")

    if args.dataset_type == "amazon":
        fields = ["title", "description", "brand", "categories"]
    elif args.dataset_type == "movielens":
        fields = ["title", "description", "genres"]
    else:
        raise ValueError("--dataset_type must be amazon or movielens")

    text_map = {}
    for _, orig_id in id2item.items():
        v = data.get(orig_id, {})
        parts = []
        for f in fields:
            if f in v:
                val = v[f]
                if isinstance(val, list):
                    val = " ".join(str(x) for x in val)
                parts.append(str(val))
        text = " ".join(p.strip() for p in parts if str(p).strip())
        text_map[orig_id] = text if text.strip() else "N/A"
    return text_map

def find_first_image_path(original_item_id, images_info, image_dir):
    """(ä¿æŒä¸è®Š) ç‚ºçµ¦å®š item æŸ¥æ‰¾ç¬¬ä¸€å€‹å­˜åœ¨çš„åœ–åƒæ–‡ä»¶è·¯å¾‘"""
    names = images_info.get(original_item_id, [])
    if not isinstance(names, list):
        names = []
    for name in names:
        if not isinstance(name, str) or not name:
            continue
        fp = os.path.join(image_dir, name)
        if os.path.exists(fp):
            return fp
    return None

def load_pil_image(img_path):
    """å®‰å…¨åœ°åŠ è¼‰ PIL åœ–åƒï¼Œå¤±æ•—æ™‚è¿”å› None"""
    if img_path is None:
        return None
    try:
        # è½‰æ›ç‚º RGB ä»¥ç¢ºä¿é€šé“æ•¸ä¸€è‡´
        return Image.open(img_path).convert("RGB") 
    except (UnidentifiedImageError, FileNotFoundError, Exception) as e:
        # print(f"[è­¦å‘Š] ç„¡æ³•åŠ è¼‰åœ–åƒ {img_path}: {e}") # å¯èƒ½æ‰“å°éå¤š
        return None

# ----------------- VLM ç‰¹å¾µæå–æ ¸å¿ƒå‡½æ•¸ -----------------
def vlm_encode_batch(
    processor, 
    model, 
    texts: List[str], 
    images: List[Any], # List[PIL.Image or None]
    device: torch.device, 
    prompt_template: str = "Describe this item for recommendation: {}"
) -> np.ndarray:
    """
    ä½¿ç”¨ VLM è™•ç†ä¸€æ‰¹æ–‡æœ¬å’Œåœ–åƒï¼Œæå–èåˆå¾Œçš„ Embeddingã€‚
    
    Args:
        processor: VLM çš„è™•ç†å™¨ (AutoProcessor)
        model: VLM æ¨¡å‹ (AutoModelForCausalLM)
        texts: æ‰¹æ¬¡çš„æ–‡æœ¬åˆ—è¡¨
        images: æ‰¹æ¬¡çš„ PIL.Image å°è±¡åˆ—è¡¨ (ç¼ºå¤±åœ–åƒç”¨ None è¡¨ç¤º)
        device: æ¨¡å‹æ‰€åœ¨çš„è¨­å‚™
        prompt_template: ç”¨æ–¼åŒ…è£æ–‡æœ¬çš„æ¨¡æ¿ (å¯é¸)

    Returns:
        np.ndarray: èåˆå¾Œçš„ Embedding æ•¸çµ„ (batch_size, hidden_dim)
    """
    
    # æª¢æŸ¥è¼¸å…¥é•·åº¦æ˜¯å¦ä¸€è‡´
    if len(texts) != len(images):
        raise ValueError(f"æ–‡æœ¬ ({len(texts)}) å’Œåœ–åƒ ({len(images)}) çš„æ•¸é‡ä¸åŒ¹é…")
        
    batch_size = len(texts)
    
    # --- æº–å‚™ VLM è¼¸å…¥ ---
    # å¤§å¤šæ•¸ VLM è™•ç†å™¨æ¥å—æ–‡æœ¬åˆ—è¡¨å’Œ PIL åœ–åƒåˆ—è¡¨
    # æˆ‘å€‘éœ€è¦ç‚ºæ¯å€‹æ¨£æœ¬æ§‹å»ºè¼¸å…¥ï¼Œå¯èƒ½åŒ…å«åœ–åƒæˆ–ä¸åŒ…å«
    # ç‚ºäº†ç°¡åŒ–æ‰¹è™•ç†ï¼Œæˆ‘å€‘å‰µå»ºå…©å€‹åˆ—è¡¨ï¼šä¸€å€‹åŒ…å«æ‰€æœ‰æ–‡æœ¬ï¼ˆå¸¶ Promptï¼‰ï¼Œä¸€å€‹åŒ…å«æ‰€æœ‰åœ–åƒï¼ˆNone è¡¨ç¤ºç¼ºå¤±ï¼‰
    
    processed_texts = [prompt_template.format(t if t and t.strip() else "N/A") for t in texts]
    # å°æ–¼æ²’æœ‰åœ–åƒçš„æ¨£æœ¬ï¼Œå‚³é None çµ¦ processor é€šå¸¸å¯è¡Œ
    pil_images = images # images åˆ—è¡¨å·²ç¶“æ˜¯ PIL æˆ– None
    
    try:
        # ä½¿ç”¨ processor é€²è¡Œé è™•ç†
        # padding=True, return_tensors="pt" æ˜¯æ¨™æº–æ“ä½œ
        # truncation=True ä¹Ÿæ˜¯å¿…è¦çš„ï¼Œä½† max_length å–æ±ºæ–¼æ¨¡å‹ï¼Œè™•ç†å™¨é€šå¸¸æœ‰é»˜èªå€¼
        inputs = processor(
            text=processed_texts, 
            images=pil_images, 
            return_tensors="pt", 
            padding=True, 
            truncation=True 
            # max_length=... # å¯ä»¥è€ƒæ…®è¨­ç½®ä¸€å€‹æœ€å¤§é•·åº¦
        ).to(device)
    except Exception as e:
        print(f"\n[éŒ¯èª¤] VLM Processor è™•ç†å¤±æ•—: {e}")
        # è¿”å›é›¶å‘é‡ä½œç‚ºéŒ¯èª¤è™•ç†
        hidden_dim = model.config.hidden_size # å˜—è©¦ç²å–éš±è—å±¤ç¶­åº¦
        return np.zeros((batch_size, hidden_dim if hidden_dim else 4096), dtype=np.float32)

    # --- åŸ·è¡Œå‰å‘å‚³æ’­ä¸¦æå–éš±è—ç‹€æ…‹ ---
    with torch.no_grad():
        try:
            outputs = model(**inputs, output_hidden_states=True)
            last_hidden_states = outputs.hidden_states[-1] # (batch_size, seq_len, hidden_dim)
            
            # ç­–ç•¥ï¼šå–æœ€å¾Œä¸€å€‹ token çš„éš±è—ç‹€æ…‹
            # æ³¨æ„ï¼šé€™è£¡å‡è¨­æœ€å¾Œä¸€å€‹ token çš„ embedding æœ€èƒ½ä»£è¡¨æ•´é«”
            fused_embeddings = last_hidden_states[:, -1, :] # (batch_size, hidden_dim)
            
            return fused_embeddings.cpu().numpy().astype(np.float32)
            
        except Exception as e:
            print(f"\n[éŒ¯èª¤] VLM Forward pass æˆ–éš±è—ç‹€æ…‹æå–å¤±æ•—: {e}")
            hidden_dim = model.config.hidden_size
            return np.zeros((batch_size, hidden_dim if hidden_dim else 4096), dtype=np.float32)

# ----------------- VLM ç‰¹å¾µæå–æ ¸å¿ƒå‡½æ•¸ -----------------
def vlm_encode_one(processor, model, text, image, device, prompt_template):
    """
    å•æ ·æœ¬ç¼–ç ï¼šç”¨ chat_template ç”Ÿæˆå­—ç¬¦ä¸²ï¼Œå†ç”± processor æ‰“åŒ…æˆ mappingï¼Œé¿å… **Tensor é”™ä¼ ã€‚
    """
    # 1) ç»„è£… messages â€”â€” æœ‰å›¾å°±æ”¾ imageï¼Œæ²¡æœ‰å›¾å°±åªæ”¾ textï¼›ä¸è¦æ‰‹å†™ "<image>"ã€‚
    msg_content = []
    if image is not None:
        msg_content.append({"type": "image", "image": image})
    msg_content.append({"type": "text", "text": prompt_template.format(text if text and text.strip() else "N/A")})

    messages = [{"role": "user", "content": msg_content}]

    # 2) å…ˆç”¨æ¨¡æ¿ç”Ÿæˆâ€œå­—ç¬¦ä¸²â€ï¼Œç»å¯¹ä¸è¦ tokenize=True / return_tensors="pt"
    chat_str = processor.apply_chat_template(
        messages,
        tokenize=False,              # å…³é”®ï¼šè¿”å›å­—ç¬¦ä¸²è€Œä¸æ˜¯ tensor
        add_generation_prompt=False  # æˆ‘ä»¬åªå– hidden statesï¼Œä¸éœ€è¦ç”Ÿæˆæç¤º
    )

    # 3) å†ç”¨ processor æ‰“åŒ…æˆâ€œå­—å…¸â€ï¼ˆmappingï¼‰ï¼štext + image -> BatchFeature(dict)
    inputs = processor(
        text=[chat_str],             # åˆ—è¡¨å½¢å¼ï¼Œä¿æŒ batch ç»´
        images=[image] if image is not None else None,
        padding=True,
        return_tensors="pt"
    ).to(device)

    # 4) å‰å‘å¹¶å–éšè—æ€
    with torch.no_grad():
        outputs = model(**inputs, output_hidden_states=True)
        # (1) å…ˆåšæ± åŒ–
        last_hidden = outputs.hidden_states[-1].mean(dim=1)   # [1, H]ï¼Œå¯èƒ½æ˜¯ bf16
        # (2) æ˜¾å¼è½¬åˆ° float32 å†æ¬åˆ° CPUã€å†è½¬ numpy
        return last_hidden.to(dtype=torch.float32).squeeze(0).cpu().numpy()



def vlm_encode_batch(processor, model, texts, images, device, prompt_template):
    """
    å°æ‰¹é‡å°è£…ï¼šå†…éƒ¨é€æ ·æœ¬è°ƒç”¨ï¼Œå±è”½â€œæœ‰å›¾/æ— å›¾æ··æ‰¹â€å¸¦æ¥çš„ tokens/features ä¸åŒ¹é…ä¸ **Tensor é”™ä¼ ã€‚
    """
    assert len(texts) == len(images), "æ–‡æœ¬ä¸å›¾åƒæ•°é‡ä¸ä¸€è‡´"
    embs = []
    for t, img in zip(texts, images):
        emb = vlm_encode_one(processor, model, t, img, device, prompt_template)
        embs.append(emb)
    return np.stack(embs, axis=0)  # (batch, hidden_dim)


# ----------------- ä¸»æå–èˆ‡å°å‡ºé‚è¼¯ -----------------
def extract_vlm_embeddings(args):
    """ä¸»å‡½æ•¸ï¼šåŠ è¼‰æ•¸æ“šã€æ¨¡å‹ï¼Œæå–ä¸¦å°å‡º VLM Embedding"""
    
    # --- 1. åŠ è¼‰æ•¸æ“š ---
    processed_data_path = os.path.join(args.save_root, args.dataset)
    item2id_file = os.path.join(processed_data_path, f"{args.dataset}.item2id")
    id2item = get_id2item_dict(item2id_file)

    image_base_path = os.path.join(args.image_root, f"amazon{args.data_version}", "Images")
    image_dir = os.path.join(image_base_path, args.dataset)
    images_info_path = os.path.join(image_base_path, f"{args.dataset}_images_info.json")
    images_info = load_json(images_info_path) or {}

    text_map = build_text_map(args, id2item)

    # --- 2. åŠ è¼‰ VLM æ¨¡å‹å’Œè™•ç†å™¨ ---
    device = torch.device(args.device)
    print(f"[INFO] æ­£åœ¨åŠ è¼‰ VLM æ¨¡å‹: {args.vlm_model_name_or_path} åˆ° {device}")
    try:
        # å˜—è©¦ä½¿ç”¨ AutoModelForCausalLMï¼Œå®ƒé©ç”¨æ–¼å¾ˆå¤š Decoder-only çš„ VLM
        from transformers import AutoProcessor, Qwen3VLForConditionalGeneration

        from transformers import Qwen3VLForConditionalGeneration, AutoProcessor

        model = Qwen3VLForConditionalGeneration.from_pretrained(
            args.vlm_model_name_or_path,
            dtype="auto",
            device_map="auto",
            trust_remote_code=True
        )
        processor = AutoProcessor.from_pretrained(args.vlm_model_name_or_path, trust_remote_code=True)

        model.eval() # è¨­ç½®ç‚ºè©•ä¼°æ¨¡å¼
        print("[INFO] VLM æ¨¡å‹åŠ è¼‰æˆåŠŸã€‚")
    except Exception as e:
        print(f"[éŒ¯èª¤] åŠ è¼‰ VLM æ¨¡å‹æˆ–è™•ç†å™¨å¤±æ•—: {e}")
        print("è«‹ç¢ºä¿æ¨¡å‹åç¨±æ­£ç¢ºï¼Œä¸”å·²å®‰è£æ‰€æœ‰å¿…è¦çš„ä¾è³´åº« (å¯èƒ½éœ€è¦ 'pip install accelerate bitsandbytes')ã€‚")
        sys.exit(1)

    # --- 3. æº–å‚™æ‰€æœ‰ç‰©å“çš„æ–‡æœ¬å’Œåœ–åƒ ---
    sorted_ids = sorted(id2item.keys(), key=int)
    texts_all: List[str] = []
    images_all: List[Any] = [] # å­˜å„² PIL Image æˆ– None
    
    print("[INFO] æº–å‚™æ‰€æœ‰ç‰©å“çš„æ–‡æœ¬å’Œåœ–åƒ...")
    for mapped_id in tqdm(sorted_ids, desc="æº–å‚™æ•¸æ“š"):
        orig_id = id2item[mapped_id]
        texts_all.append(text_map.get(orig_id, "N/A"))
        img_path = find_first_image_path(orig_id, images_info, image_dir)
        images_all.append(load_pil_image(img_path)) # åŠ è¼‰ PIL åœ–åƒ

    # --- 4. åˆ†æ‰¹æ¬¡æå– Embedding ---
    all_embeddings = []
    total_items = len(sorted_ids)
    
    print(f"[INFO] é–‹å§‹ä½¿ç”¨æ‰¹æ¬¡å¤§å° {args.batch_size} æå– Embedding...")
    for i in tqdm(range(0, total_items, args.batch_size), desc="VLM Encoding"):
        batch_texts = texts_all[i : i + args.batch_size]
        batch_images = images_all[i : i + args.batch_size]
        
        batch_embeddings = vlm_encode_batch(
            processor, model, batch_texts, batch_images, device, args.prompt_template
        )

        all_embeddings.append(batch_embeddings)

    # --- 5. åŒ¯ç¸½ä¸¦ä¿å­˜ ---
    if not all_embeddings:
        print("[éŒ¯èª¤] æœªèƒ½ç”Ÿæˆä»»ä½• Embeddingã€‚")
        return

    final_embeddings_np = np.concatenate(all_embeddings, axis=0)
    
    # é©—è­‰æ•¸é‡æ˜¯å¦åŒ¹é…
    if final_embeddings_np.shape[0] != total_items:
         print(f"[è­¦å‘Š] æœ€çµ‚ Embedding æ•¸é‡ ({final_embeddings_np.shape[0]}) èˆ‡ç‰©å“ç¸½æ•¸ ({total_items}) ä¸åŒ¹é…ï¼")
         # å¯ä»¥è€ƒæ…®å¡«å……æˆ–æˆªæ–·ï¼Œä½†é€™è£¡å…ˆæ‰“å°è­¦å‘Š
         final_embeddings_np = final_embeddings_np[:total_items] # å˜—è©¦æˆªæ–·ä»¥åŒ¹é…

    out_dir = os.path.join(args.save_root, args.dataset, "embeddings")
    os.makedirs(out_dir, exist_ok=True)
    
    # æ–‡ä»¶ååŒ…å« VLM æ¨¡å‹åç¨±
    vlm_tag = args.vlm_model_name_or_path.split("/")[-1].replace("/", "-")
    out_base_name = f"{args.dataset}.emb-{args.export_tag}-{vlm_tag}.npy"
    out_base_path = os.path.join(out_dir, out_base_name)
    
    np.save(out_base_path, final_embeddings_np)
    print(f"\n[OK] saved fused VLM embeddings: {out_base_path}  shape={final_embeddings_np.shape}")

    # --- 6. (å¯é¸) PCA ---
    if args.pca_dim > 0 and args.pca_dim < final_embeddings_np.shape[1]:
        print(f"[INFO] Performing PCA -> {args.pca_dim} (whiten={args.whiten})")
        try:
            pca = PCA(n_components=args.pca_dim, whiten=args.whiten, svd_solver="auto", random_state=42)
            Zp = pca.fit_transform(final_embeddings_np).astype(np.float32)
            
            out_pca_name = f"{args.dataset}.emb-{args.export_tag}-{vlm_tag}-pca{args.pca_dim}.npy"
            out_pca_path = os.path.join(out_dir, out_pca_name)
            np.save(out_pca_path, Zp)
            print(f"[OK] saved PCA embeddings: {out_pca_path}  shape={Zp.shape}  explained_variance={pca.explained_variance_ratio_.sum():.4f}")
        except Exception as e:
            print(f"[éŒ¯èª¤] PCA å¤±æ•—: {e}")

# ----------------- argparser -----------------
def build_parser():
    ap = argparse.ArgumentParser("Extract Deep Fused Embeddings using VLM (No Training)")
    
    # --- æ•¸æ“šè·¯å¾‘åƒæ•¸ (èˆ‡ä¹‹å‰é¡ä¼¼) ---
    ap.add_argument("--data_version", type=str, default="14", choices=["14","18"])
    ap.add_argument("--dataset", type=str, required=True)
    ap.add_argument("--dataset_type", type=str, default="amazon", choices=["amazon","movielens"])
    ap.add_argument("--image_root", type=str, default="../datasets")
    ap.add_argument("--save_root", type=str, default="../datasets")

    # --- VLM æ¨¡å‹åƒæ•¸ ---
    ap.add_argument("--vlm_model_name_or_path", type=str, required=True, 
                        default="Qwen/Qwen3-VL-32B-Instruct",
                        help="è¦ä½¿ç”¨çš„ VLM æ¨¡å‹ Hugging Face ID æˆ–æœ¬åœ°è·¯å¾‘ (e.g., 'Qwen/Qwen3-VL-7B-Instruct', 'llava-hf/llava-1.5-7b-hf')")
    ap.add_argument("--model_cache_dir", type=str, default=None, help="Hugging Face æ¨¡å‹ç·©å­˜ç›®éŒ„ (å¯é¸)")
    ap.add_argument("--prompt_template", type=str, default="Represent this item for recommendation: {}", 
                        help="ç”¨æ–¼åŒ…è£ç‰©å“æ–‡æœ¬çš„ Prompt æ¨¡æ¿")

    # --- æå–èˆ‡å°å‡ºåƒæ•¸ ---
    ap.add_argument("--batch_size", type=int, default=16, help="VLM æ¨ç†çš„æ‰¹æ¬¡å¤§å° (æ ¹æ“šé¡¯å­˜èª¿æ•´)")
    ap.add_argument("--export_tag", type=str, default="vlm-fused", help="è¼¸å‡ºæ–‡ä»¶åä¸­çš„æ¨™ç±¤")
    ap.add_argument("--export_bs", type=int, default=1024, help="(æ­¤åƒæ•¸åœ¨æ­¤è…³æœ¬ä¸­æœªä½¿ç”¨ï¼Œä¿ç•™ä»¥å…¼å®¹)") # ä¿ç•™ä»¥é˜²æ··æ·†
    ap.add_argument("--pca_dim", type=int, default=0, help="PCA é™ç¶­ç¶­åº¦ (0è¡¨ç¤ºä¸é™ç¶­)")
    ap.add_argument("--whiten", action="store_true", help="PCA æ™‚æ˜¯å¦ç™½åŒ–")

    # --- è¨­å‚™ ---
    ap.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu")
    
    # ğŸš¨ ç§»é™¤æ‰€æœ‰è¨“ç·´ç›¸é—œåƒæ•¸ (epochs, lr, temperature, dropout ç­‰)
    
    return ap


def main():
    args = build_parser().parse_args()
    print(f"[CFG] VLM Model: {args.vlm_model_name_or_path}, Device: {args.device}")
    extract_vlm_embeddings(args)


if __name__ == "__main__":
    """
    ä½¿ç”¨ç¤ºä¾‹:
    
    python vl_embedding.py \
        --dataset Baby \
        --vlm_model_name_or_path Qwen/Qwen3-VL-7B-Instruct \
        --batch_size 32 \
        --export_tag qwen7b \
        --pca_dim 512 
        # --model_cache_dir /path/to/cache # (å¯é¸)
        # --device cuda:0 # (å¯é¸)
    """
    main()