# preprocessing/generate_embeddings/image_encoder.py

import os
import torch
import torch.nn.functional as F # å¯¼å…¥ F ç”¨äº normalize
from PIL import Image, UnidentifiedImageError
import numpy as np
from tqdm import tqdm
from transformers import CLIPProcessor, CLIPModel
from typing import Dict, List, Any
import sys # å¯¼å…¥ sys

# âœ… (æ ¸å¿ƒä¿®æ”¹) ä»çˆ¶ç›®å½•å¯¼å…¥å…±äº«å‡½æ•°
try:
    # æ·»åŠ çˆ¶ç›®å½• (preprocessing/) åˆ° Python è·¯å¾„
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    # ä» utils å¯¼å…¥éœ€è¦çš„å‡½æ•°
    from utils import load_json, find_first_image_path, load_pil_image # ä½¿ç”¨ utils ä¸­çš„ç‰ˆæœ¬
    print("[INFO] image_encoder: æˆåŠŸä»çˆ¶ç›®å½• utils.py å¯¼å…¥å…±äº«å‡½æ•°ã€‚")
except ImportError as e:
    print(f"å¯¼å…¥é”™è¯¯: {e}")
    print("é”™è¯¯: æ— æ³•ä»çˆ¶ç›®å½• (preprocessing/) å¯¼å…¥ utils.pyã€‚è¯·æ£€æŸ¥æ–‡ä»¶ç»“æ„ã€‚")


def generate_clip_image(args, id2item: Dict[str, str], images_info: Dict[str, List[str]], image_dir: str) -> np.ndarray:
    """
    ä½¿ç”¨æŒ‡å®šçš„ CLIP æ¨¡å‹æå–å›¾åƒåµŒå…¥ã€‚
    (å‡½æ•°ä½“ä¿æŒä¸å˜ï¼Œå› ä¸ºå®ƒå·²ç»ä¾èµ–äºä» utils å¯¼å…¥çš„å‡½æ•°)
    """
    print(f"ğŸ”¹ ä½¿ç”¨ CLIP æ¨¡å‹ç”Ÿæˆå›¾åƒåµŒå…¥: {args.clip_model_name}")
    device = args.device # device å·²åœ¨ main_generate.py ä¸­è®¾ç½®

    # --- 1. åŠ è½½ CLIP æ¨¡å‹å’Œå¤„ç†å™¨ ---
    print(f'åŠ è½½ Hugging Face CLIP æ¨¡å‹: {args.clip_model_name} ...')
    try:
        # ä½¿ç”¨ model_cache_dir å‚æ•°
        processor = CLIPProcessor.from_pretrained(args.clip_model_name, cache_dir=args.model_cache_dir)
        model = CLIPModel.from_pretrained(args.clip_model_name, cache_dir=args.model_cache_dir).to(device)
        model.eval()
        # å°è¯•æ›´å®‰å…¨åœ°è·å–ç»´åº¦
        embedding_dim = getattr(getattr(model, 'config', None), 'projection_dim', 512) # æä¾›é»˜è®¤å€¼
        print(f"æ¨¡å‹åŠ è½½æˆåŠŸï¼ŒåµŒå…¥ç»´åº¦: {embedding_dim}")
    except Exception as e:
        print(f"åŠ è½½ CLIP æ¨¡å‹æˆ–é¢„å¤„ç†å™¨å¤±è´¥: {e}")
        raise # é‡æ–°æŠ›å‡ºå¼‚å¸¸

    # --- 2. å‡†å¤‡å›¾åƒæ•°æ® (æŒ‰æ–°IDé¡ºåº) ---
    sorted_new_ids = sorted(id2item.keys(), key=int)
    all_pil_images = []
    print("å‡†å¤‡å›¾åƒæ•°æ® (åŠ è½½ PIL å¯¹è±¡)...")
    load_errors = 0
    for mapped_id_str in tqdm(sorted_new_ids, desc="æŸ¥æ‰¾å¹¶åŠ è½½å›¾åƒ"):
        original_item_id = id2item.get(mapped_id_str)
        img_path = None
        if original_item_id and image_dir: # ç¡®ä¿ image_dir æœ‰æ•ˆ
            # ä½¿ç”¨ utils.find_first_image_path
            img_path = find_first_image_path(original_item_id, images_info, image_dir)
        
        # ä½¿ç”¨ utils.load_pil_image
        pil_img = load_pil_image(img_path)
        if img_path and pil_img is None:
             load_errors += 1
        all_pil_images.append(pil_img)
        
    if load_errors > 0:
         print(f"[è­¦å‘Š] {load_errors} ä¸ªå›¾åƒæ–‡ä»¶æ— æ³•åŠ è½½ã€‚")

    # --- 3. åˆ†æ‰¹æ¬¡æå–ç‰¹å¾ ---
    embeddings = []
    total_items = len(sorted_new_ids)
    print(f"å¼€å§‹ä½¿ç”¨æ‰¹æ¬¡å¤§å° {args.batch_size} æå–å›¾åƒç‰¹å¾...")

    with torch.no_grad():
        for i in tqdm(range(0, total_items, args.batch_size), desc="CLIP Image Encoding"):
            batch_images = all_pil_images[i : i + args.batch_size]
            
            processed_batch = []
            for img in batch_images:
                if img is None:
                    try: # å°è¯•è·å– processor å®šä¹‰çš„å°ºå¯¸
                        if hasattr(processor, 'image_processor') and hasattr(processor.image_processor, 'size'):
                            size_info = processor.image_processor.size
                            if isinstance(size_info, dict): # å¤„ç† ViT-L/14@336px ç­‰æƒ…å†µ
                                img_size = size_info.get('shortest_edge', size_info.get('height', 224))
                            else: # å¤„ç†æ•´æ•°æƒ…å†µ
                                img_size = int(size_info)
                        else: # å›é€€åˆ°é»˜è®¤
                            img_size = 224
                    except:
                        img_size = 224 # æœ€ç»ˆå›é€€
                        
                    processed_batch.append(Image.new("RGB", (img_size, img_size), color=(0, 0, 0)))
                else:
                    processed_batch.append(img)
            
            try:
                # æ·»åŠ  error handling for processor call
                inputs = processor(images=processed_batch, return_tensors="pt", padding=True).to(device) # padding=True maybe needed
                image_features = model.get_image_features(**inputs)
                
                # L2 å½’ä¸€åŒ–
                image_features = F.normalize(image_features, p=2, dim=-1)
                
                embeddings.append(image_features.cpu())
            except Exception as e:
                print(f"\n[è­¦å‘Š] CLIP å›¾åƒç¼–ç æ‰¹æ¬¡ {i//args.batch_size} å¤±è´¥: {e}")
                embeddings.append(torch.zeros((len(batch_images), embedding_dim)))

    if not embeddings: # å¤„ç†å®Œå…¨å¤±è´¥çš„æƒ…å†µ
        raise RuntimeError("æœªèƒ½ç”Ÿæˆä»»ä½• CLIP å›¾åƒåµŒå…¥ã€‚")

    final_embeddings = torch.cat(embeddings, dim=0).numpy().astype(np.float32)

    # éªŒè¯æ•°é‡ (ä¸ text_encoder ä¿æŒä¸€è‡´)
    if final_embeddings.shape[0] != total_items:
         print(f"[è­¦å‘Š] è¾“å‡ºåµŒå…¥æ•°é‡ ({final_embeddings.shape[0]}) ä¸ç‰©å“æ•°é‡ ({total_items}) ä¸ç¬¦ï¼")
         target_len = total_items
         current_len = final_embeddings.shape[0]
         emb_dim = final_embeddings.shape[1]
         if current_len < target_len:
              print(" -> å°†ç”¨é›¶å‘é‡å¡«å……ã€‚")
              padding = np.zeros((target_len - current_len, emb_dim), dtype=np.float32)
              final_embeddings = np.concatenate([final_embeddings, padding], axis=0)
         else:
              print(" -> å°†æˆªæ–­å¤šä½™éƒ¨åˆ†ã€‚")
              final_embeddings = final_embeddings[:target_len]
         
    print(f"CLIP å›¾åƒåµŒå…¥ç»´åº¦: {final_embeddings.shape}")
    return final_embeddings

# ğŸš¨ (ç§»é™¤) ä¸å†éœ€è¦ main å‡½æ•°æˆ– argparseï¼Œå› ä¸ºè¿™ä¸ªæ–‡ä»¶ç°åœ¨æ˜¯æ¨¡å—
# if __name__ == "__main__":
#     args = ...
#     generate_clip_image(...)