# preprocessing/generate_embeddings/vlm_encoder.py

import os
import sys
import torch
from PIL import Image, UnidentifiedImageError
import numpy as np
from tqdm import tqdm
from transformers import AutoProcessor, AutoModelForCausalLM 
from typing import List, Dict, Any

# âœ… (æ ¸å¿ƒä¿®æ”¹) ä»çˆ¶ç›®å½•å¯¼å…¥å…±äº«å‡½æ•°
try:
    # æ·»åŠ çˆ¶ç›®å½• (preprocessing/) åˆ° Python è·¯å¾„
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from utils import load_json, find_first_image_path, load_pil_image # ä½¿ç”¨ utils ä¸­çš„ç‰ˆæœ¬
    print("[INFO] vlm_encoder: æˆåŠŸä»çˆ¶ç›®å½• utils.py å¯¼å…¥å…±äº«å‡½æ•°ã€‚")
except ImportError as e:
    print(f"å¯¼å…¥é”™è¯¯: {e}")
    print("é”™è¯¯: æ— æ³•ä»çˆ¶ç›®å½• (preprocessing/) å¯¼å…¥ utils.pyã€‚è¯·æ£€æŸ¥æ–‡ä»¶ç»“æ„ã€‚")
    sys.exit(1)

# ğŸš¨ (ç§»é™¤) ä¸å†éœ€è¦å¯¼å…¥æˆ–å®šä¹‰ common_utils ä¸­çš„å‡½æ•°
# try:
#     from .common_utils import load_json
# except ImportError: ...

# ğŸš¨ (ç§»é™¤) ä¸å†éœ€è¦åœ¨è¿™é‡Œå®šä¹‰ load_pil_image å’Œ find_first_image_path

# =================================================================
# ================== VLM ç‰¹å¾µæå–æ ¸å¿ƒå‡½æ•¸ (ä¿æŒä¸å˜) ==================
# =================================================================

def vlm_encode_batch(
    processor, 
    model, 
    texts: List[str], 
    images: List[Any], 
    device: torch.device, 
    prompt_template: str = "Represent this item for recommendation: {}"
) -> np.ndarray:
    """
    ä½¿ç”¨ VLM è™•ç†ä¸€æ‰¹æ–‡æœ¬å’Œåœ–åƒï¼Œæå–èåˆå¾Œçš„ Embedding (æœ€å¾Œ token)ã€‚
    (å‡½æ•°ä½“ä¿æŒä¸å˜)
    """
    if len(texts) != len(images):
        raise ValueError(f"æ–‡æœ¬ ({len(texts)}) å’Œåœ–åƒ ({len(images)}) çš„æ•¸é‡ä¸åŒ¹é…")
        
    batch_size = len(texts)
    # å°è¯•æ›´å®‰å…¨åœ°è·å– hidden_dim
    hidden_dim = getattr(getattr(model, 'config', None), 'hidden_size', 4096) # æä¾›é»˜è®¤å€¼
    
    processed_texts = [prompt_template.format(t if t and t.strip() else "N/A") for t in texts]
    pil_images = images 
    
    try:
        # æ·»åŠ  padding=True, truncation=True
        inputs = processor(
            text=processed_texts, 
            images=pil_images, 
            return_tensors="pt", 
            padding=True, # ç¡®ä¿æ‰¹å¤„ç†é•¿åº¦ä¸€è‡´
            truncation=True # ç¡®ä¿ä¸è¶…è¿‡æ¨¡å‹æœ€å¤§é•¿åº¦
        ).to(device)
    except Exception as e:
        print(f"\n[é”™è¯¯] VLM Processor å¤±è´¥: {e}")
        return np.zeros((batch_size, hidden_dim), dtype=np.float32)

    with torch.no_grad():
        try:
            # ç¡®ä¿æ¨¡å‹åœ¨ eval æ¨¡å¼
            model.eval() 
            outputs = model(**inputs, output_hidden_states=True)
            last_hidden_states = outputs.hidden_states[-1]
            # æ£€æŸ¥ hidden_states æ˜¯å¦æœ‰æ•ˆ
            if last_hidden_states is None or last_hidden_states.numel() == 0:
                 raise ValueError("æ¨¡å‹è¾“å‡ºäº†ç©ºçš„ hidden_statesã€‚")
                 
            fused_embeddings = last_hidden_states[:, -1, :]
            # L2 å½’ä¸€åŒ– (å¯é€‰ä½†æ¨è)
            fused_embeddings = F.normalize(fused_embeddings, p=2, dim=-1) 
            
            return fused_embeddings.cpu().numpy().astype(np.float32)
        except Exception as e:
            print(f"\n[é”™è¯¯] VLM Forward pass æˆ–éšè—çŠ¶æ€æå–å¤±è´¥: {e}")
            return np.zeros((batch_size, hidden_dim), dtype=np.float32)

# =================================================================
# ================== ä¸»æå–å‡½æ•¸ (ä¿æŒä¸å˜) ==================
# =================================================================

def generate_vlm_fused(args, id2item: Dict[str, str], text_map: Dict[str, str], images_info: Dict[str, List[str]], image_dir: str) -> np.ndarray:
    """
    ä½¿ç”¨ VLM æå–æ·±åº¦èåˆçš„å¤šæ¨¡æ…‹ Embeddingã€‚
    (å‡½æ•°ä½“ä¿æŒä¸å˜ï¼Œå› ä¸ºå®ƒå·²ç»ä¾èµ–äºä» utils å¯¼å…¥çš„å‡½æ•°)
    """
    print(f"ğŸ”¹ ä½¿ç”¨ VLM æ¨¡å‹ç”ŸæˆèåˆåµŒå…¥: {args.vlm_model_name_or_path}")
    device = args.device # device å·²ç”± main_generate è®¾ç½®

    # --- 1. åŠ è¼‰ VLM æ¨¡å‹å’Œè™•ç†å™¨ ---
    print(f'åŠ è½½ VLM æ¨¡å‹: {args.vlm_model_name_or_path} ...')
    try:
        # å¢åŠ  torch_dtype="auto" ä»¥ä¾¿è‡ªåŠ¨ä½¿ç”¨åŠç²¾åº¦ (å¦‚æœæ”¯æŒ)
        model = AutoModelForCausalLM.from_pretrained(
            args.vlm_model_name_or_path, 
            torch_dtype="auto", # ä½¿ç”¨ bfloat16 æˆ– float16
            device_map="auto", # è‡ªåŠ¨ GPU åˆ†é…
            trust_remote_code=True,
            cache_dir=args.model_cache_dir,
            # (å¯é€‰) å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œå°è¯• 4-bit é‡åŒ–åŠ è½½
            # load_in_4bit=True, 
            # bnb_4bit_compute_dtype=torch.bfloat16 
        )
        processor = AutoProcessor.from_pretrained(
            args.vlm_model_name_or_path,
            trust_remote_code=True,
            cache_dir=args.model_cache_dir
        )
        model.eval()
        print("VLM æ¨¡å‹åŠ è½½æˆåŠŸã€‚")
    except Exception as e:
        print(f"åŠ è½½ VLM æ¨¡å‹æˆ–å¤„ç†å™¨å¤±è´¥: {e}")
        raise

    # --- 2. å‡†å¤‡æ‰€æœ‰ç‰©å“çš„æ–‡æœ¬å’Œåœ–åƒ ---
    sorted_new_ids = sorted(id2item.keys(), key=int)
    texts_all: List[str] = []
    images_all: List[Any] = [] # PIL Image or None
    
    print("å‡†å¤‡æ–‡æœ¬å’Œåœ–åƒæ•°æ®...")
    load_errors = 0
    for mapped_id_str in tqdm(sorted_new_ids, desc="å‡†å¤‡æ•°æ®"):
        original_item_id = id2item.get(mapped_id_str)
        img_path = None # åˆå§‹åŒ–
        text = "N/A" # é»˜è®¤å€¼
        if original_item_id:
            text = text_map.get(original_item_id, "N/A")
            if image_dir and images_info: # åªæœ‰åœ¨éœ€è¦å›¾åƒæ—¶æ‰æŸ¥æ‰¾
                # ä½¿ç”¨ utils.find_first_image_path
                img_path = find_first_image_path(original_item_id, images_info, image_dir)
        else:
            print(f"[è­¦å‘Š] æ‰¾ä¸åˆ°æ–° ID {mapped_id_str} å¯¹åº”çš„åŸå§‹ IDï¼")

        texts_all.append(text)
        # ä½¿ç”¨ utils.load_pil_image
        pil_img = load_pil_image(img_path) 
        if img_path and pil_img is None: load_errors += 1
        images_all.append(pil_img)
        
    if load_errors > 0: print(f"[è­¦å‘Š] {load_errors} ä¸ªå›¾åƒæ–‡ä»¶æ— æ³•åŠ è½½ã€‚")

    # --- 3. åˆ†æ‰¹æ¬¡æå– Embedding ---
    all_embeddings = []
    total_items = len(sorted_new_ids)
    print(f"å¼€å§‹ä½¿ç”¨æ‰¹æ¬¡å¤§å° {args.batch_size} æå– VLM åµŒå…¥...")

    # (å‡å° batch size ä»¥é˜² OOM)
    effective_batch_size = min(args.batch_size, 16) # VLM é€šå¸¸éœ€è¦æ›´å°çš„ batch size
    if effective_batch_size != args.batch_size:
        print(f"[INFO] VLM batch size è°ƒæ•´ä¸º {effective_batch_size} ä»¥é€‚åº”æ˜¾å­˜ã€‚")

    for i in tqdm(range(0, total_items, effective_batch_size), desc="VLM Encoding"):
        batch_texts = texts_all[i : i + effective_batch_size]
        batch_images = images_all[i : i + effective_batch_size]
        
        batch_embeddings = vlm_encode_batch(
            processor, model, batch_texts, batch_images, device, args.vlm_prompt_template
        )
        all_embeddings.append(batch_embeddings)
        
        # (å¯é€‰) æ¸…ç† GPU ç¼“å­˜
        # if device.type == 'cuda':
        #     torch.cuda.empty_cache()

    # --- 4. æ±‡æ€» ---
    if not all_embeddings:
        raise RuntimeError("æœªèƒ½ç”Ÿæˆä»»ä½• VLM Embeddingã€‚")

    final_embeddings_np = np.concatenate(all_embeddings, axis=0)
    
    # éªŒè¯æ•°é‡ (ä¸ text_encoder ä¿æŒä¸€è‡´)
    if final_embeddings_np.shape[0] != total_items:
         print(f"[è­¦å‘Š] è¾“å‡º VLM åµŒå…¥æ•°é‡ ({final_embeddings_np.shape[0]}) ä¸ç‰©å“æ•°é‡ ({total_items}) ä¸ç¬¦ï¼")
         target_len = total_items
         current_len = final_embeddings_np.shape[0]
         emb_dim = final_embeddings_np.shape[1]
         if current_len < target_len:
              print(" -> å°†ç”¨é›¶å‘é‡å¡«å……ã€‚")
              padding = np.zeros((target_len - current_len, emb_dim), dtype=np.float32)
              final_embeddings_np = np.concatenate([final_embeddings_np, padding], axis=0)
         else:
              print(" -> å°†æˆªæ–­å¤šä½™éƒ¨åˆ†ã€‚")
              final_embeddings_np = final_embeddings_np[:target_len]
         
    print(f"VLM èåˆåµŒå…¥ç»´åº¦: {final_embeddings_np.shape}")
    return final_embeddings_np

# ğŸš¨ (ç§»é™¤) ä¸å†éœ€è¦ main æˆ– argparse
# if __name__ == "__main__":
#     args = ...
#     generate_vlm_fused(...)