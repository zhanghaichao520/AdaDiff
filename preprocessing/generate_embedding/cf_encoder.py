# preprocessing/generate_embeddings/cf_encoder.py

import os
import sys
import json
import time
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
from tqdm import tqdm

# âœ… (æ ¸å¿ƒä¿®æ”¹) ä»çˆ¶ç›®å½•å¯¼å…¥å…±äº«å‡½æ•°
try:
    # æ·»åŠ çˆ¶ç›®å½• (preprocessing/) åˆ° Python è·¯å¾„
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from utils import load_json, set_device # å¯¼å…¥éœ€è¦çš„å‡½æ•°
    print("[INFO] cf_encoder: æˆåŠŸä»çˆ¶ç›®å½• utils.py å¯¼å…¥å…±äº«å‡½æ•°ã€‚")
except ImportError as e:
    print(f"å¯¼å…¥é”™è¯¯: {e}")
    print("é”™è¯¯: æ— æ³•ä»çˆ¶ç›®å½• (preprocessing/) å¯¼å…¥ utils.pyã€‚è¯·æ£€æŸ¥æ–‡ä»¶ç»“æ„ã€‚")
    sys.exit(1)

# ğŸš¨ (ç§»é™¤) ä¸å†éœ€è¦å¯¼å…¥æˆ–å®šä¹‰ common_utils ä¸­çš„å‡½æ•°
# try:
#     from .common_utils import build_output_path 
# except ImportError: ...

# ğŸš¨ (ç§»é™¤) ä¸å†éœ€è¦åœ¨è¿™é‡Œé‡æ–°å®šä¹‰ load_json, set_device

# =================================================================
# ================== SASRec æ•°æ®é›†å’Œæ¨¡å‹ (ä¿æŒä¸å˜) =============
# =================================================================

class SASRecDataset(Dataset):
    # ... (ä»£ç ä¿æŒä¸å˜) ...
    def __init__(self, data_path, max_seq_len): 
        self.data_path = data_path
        self.max_seq_len = max_seq_len
        self.lines = []
        try:
            with open(data_path, 'r', encoding='utf-8') as f: # æ·»åŠ  encoding
                for line in f:
                    self.lines.append(line.strip())
        except FileNotFoundError:
             print(f"é”™è¯¯ï¼šSASRec è®­ç»ƒæ–‡ä»¶æœªæ‰¾åˆ°: {data_path}")
             raise # é‡æ–°æŠ›å‡ºï¼Œè®©è°ƒç”¨è€…çŸ¥é“
        except Exception as e:
             print(f"é”™è¯¯ï¼šè¯»å– SASRec è®­ç»ƒæ–‡ä»¶å¤±è´¥: {e}")
             raise

    def __len__(self):
        return len(self.lines)

    def __getitem__(self, idx):
        try:
            line = self.lines[idx]
            data = json.loads(line)
            history_ids = [int(i) + 1 for i in data["history"]] 
            target_id = int(data["target"]) + 1 
            seq = history_ids[-self.max_seq_len:]
            seq_len = len(seq)
            padding_len = self.max_seq_len - seq_len
            seq = seq + [0] * padding_len
            return torch.tensor(seq, dtype=torch.long), \
                   torch.tensor(target_id, dtype=torch.long), \
                   torch.tensor(seq_len, dtype=torch.long)
        except json.JSONDecodeError:
             print(f"è­¦å‘Šï¼šè§£æç¬¬ {idx} è¡Œ JSON å¤±è´¥: {line}")
             # è¿”å›ä¸€ä¸ªå ä½ç¬¦æˆ–å¼•å‘å¼‚å¸¸ï¼Œè¿™é‡Œé€‰æ‹©å ä½ç¬¦ï¼Œä½†å¯èƒ½å¯¼è‡´è®­ç»ƒé—®é¢˜
             # æ›´å¥å£®çš„æ–¹å¼æ˜¯åœ¨åŠ è½½æ—¶è¿‡æ»¤æ‰æ— æ•ˆè¡Œ
             return torch.zeros(self.max_seq_len, dtype=torch.long), \
                    torch.tensor(0, dtype=torch.long), \
                    torch.tensor(0, dtype=torch.long)
        except KeyError as e:
            print(f"è­¦å‘Šï¼šç¬¬ {idx} è¡Œç¼ºå°‘é”® {e}: {line}")
            return torch.zeros(self.max_seq_len, dtype=torch.long), \
                   torch.tensor(0, dtype=torch.long), \
                   torch.tensor(0, dtype=torch.long)


class SASRecModel(nn.Module):
    # ... (ä»£ç ä¿æŒä¸å˜) ...
    def __init__(self, n_items, hidden_dim, max_seq_len, n_layers, n_heads, dropout=0.1):
        super(SASRecModel, self).__init__()
        self.n_items = n_items; self.hidden_dim = hidden_dim
        self.item_embedding = nn.Embedding(self.n_items + 1, hidden_dim, padding_idx=0)
        self.position_embedding = nn.Embedding(max_seq_len, hidden_dim)
        self.emb_dropout = nn.Dropout(dropout)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim, nhead=n_heads, dim_feedforward=hidden_dim * 4,
            dropout=dropout, batch_first=True, activation='gelu', norm_first=True # æ·»åŠ  norm_first
        )
        # æ·»åŠ  LayerNorm
        encoder_norm = nn.LayerNorm(hidden_dim)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers, norm=encoder_norm) # åº”ç”¨ norm
        self.layer_norm = nn.LayerNorm(hidden_dim) # è¾“å‡ºå‰çš„ LayerNorm

    def forward(self, item_seq, seq_lengths):
        # è¾¹ç•Œæ£€æŸ¥ seq_lengths
        seq_lengths = torch.clamp(seq_lengths, min=1) # ç¡®ä¿æœ€å°é•¿åº¦ä¸º 1
        last_item_indices = seq_lengths - 1 # ç°åœ¨å®‰å…¨äº†

        item_emb = self.item_embedding(item_seq)
        pos_ids = torch.arange(item_seq.size(1), device=item_seq.device).unsqueeze(0)
        pos_emb = self.position_embedding(pos_ids)
        x = self.emb_dropout(item_emb + pos_emb)
        padding_mask = (item_seq == 0)
        # ç¡®ä¿ mask å’Œ input ç»´åº¦å…¼å®¹
        transformer_out = self.transformer_encoder(x, src_key_padding_mask=padding_mask) 
        transformer_out = self.layer_norm(transformer_out) # åº”ç”¨è¾“å‡º LayerNorm
        
        batch_indices = torch.arange(transformer_out.size(0), device=transformer_out.device)
        # last_item_indices = seq_lengths - 1 # ç§»åˆ°å‰é¢
        last_item_emb = transformer_out[batch_indices, last_item_indices, :]
        logits = last_item_emb @ self.item_embedding.weight.T
        return logits

# =================================================================
# ================== ä¸»è®­ç»ƒä¸æå–å‡½æ•° (ä¿æŒä¸å˜) ==================
# =================================================================

def train_and_extract_sasrec(args, n_items: int) -> np.ndarray:
    """
    è®­ç»ƒ SASRec æ¨¡å‹å¹¶æå–ç‰©å“åµŒå…¥ã€‚
    (å‡½æ•°ä½“ä¿æŒä¸å˜ï¼Œå› ä¸ºå®ƒå·²ç»ä¾èµ–äºä» utils å¯¼å…¥çš„å‡½æ•°)
    """
    print(f"ğŸ”¹ ä½¿ç”¨ SASRec è®­ç»ƒååŒè¿‡æ»¤åµŒå…¥...")
    device = args.device # device å·²ç”± main_generate è®¾ç½®

    # --- 1. æ„å»ºè·¯å¾„ ---
    data_dir = os.path.join(args.save_root, args.dataset) 
    train_path = os.path.join(data_dir, f"{args.dataset}.train.jsonl")
    
    if not os.path.exists(train_path):
        raise FileNotFoundError(f"é”™è¯¯: æ‰¾ä¸åˆ° SASRec è®­ç»ƒæ–‡ä»¶ {train_path}")

    # --- 2. åˆ›å»º Dataset å’Œ DataLoader ---
    print("åŠ è½½ SASRec è®­ç»ƒæ•°æ®...")
    try:
        # ä½¿ç”¨ getattr å®‰å…¨è·å– num_workers
        num_workers = getattr(args, 'num_workers', 0) 
        train_dataset = SASRecDataset(train_path, args.sasrec_max_seq_len) 
        train_loader = DataLoader(
            train_dataset, batch_size=args.batch_size, 
            shuffle=True, num_workers=num_workers, 
            pin_memory=(device.type == 'cuda')
        )
    except Exception as e:
        print(f"åˆ›å»º SASRec DataLoader å¤±è´¥: {e}")
        raise

    # --- 3. åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å’Œä¼˜åŒ–å™¨ ---
    try:
        model = SASRecModel(
            n_items=n_items,
            hidden_dim=args.sasrec_hidden_dim,
            max_seq_len=args.sasrec_max_seq_len,
            n_layers=args.sasrec_n_layers,
            n_heads=args.sasrec_n_heads,
            dropout=args.sasrec_dropout
        ).to(device)
    except Exception as e:
         print(f"åˆå§‹åŒ– SASRec æ¨¡å‹å¤±è´¥: {e}")
         raise
    
    criterion = nn.CrossEntropyLoss(ignore_index=0) # å¿½ç•¥ PAD ç›®æ ‡ (è™½ç„¶ç†è®ºä¸Š target ä¸åº”ä¸º 0)
    optimizer = torch.optim.Adam(
        model.parameters(), 
        lr=args.sasrec_lr, 
        weight_decay=args.sasrec_weight_decay
    )
    
    print("å¼€å§‹è®­ç»ƒ SASRec...")
    start_time = time.time()
    
    for epoch in range(1, args.sasrec_epochs + 1):
        model.train()
        total_loss = 0
        pbar = tqdm(train_loader, desc=f"SASRec Epoch {epoch}/{args.sasrec_epochs}")
        
        for batch in pbar:
            try:
                seq, target, seq_len = [b.to(device) for b in batch]
                
                # è·³è¿‡ seq_len ä¸º 0 çš„æ— æ•ˆæ ·æœ¬ (ç”± Dataset è¿”å›çš„å ä½ç¬¦å¯¼è‡´)
                valid_mask = (seq_len > 0)
                if not valid_mask.any(): continue # å¦‚æœæ•´ä¸ªæ‰¹æ¬¡éƒ½æ— æ•ˆ
                
                seq = seq[valid_mask]
                target = target[valid_mask]
                seq_len = seq_len[valid_mask]

                optimizer.zero_grad()
                logits = model(seq, seq_len)
                loss = criterion(logits, target)
                
                # æ£€æŸ¥ loss æ˜¯å¦æœ‰æ•ˆ
                if torch.isnan(loss) or torch.isinf(loss):
                     print(f"\n[è­¦å‘Š] Epoch {epoch}: æ£€æµ‹åˆ°æ— æ•ˆ Loss å€¼ ({loss.item()})ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡ã€‚")
                     continue # è·³è¿‡æ— æ•ˆæ‰¹æ¬¡

                loss.backward()
                # (å¯é€‰) æ¢¯åº¦è£å‰ª
                # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) 
                optimizer.step()
                
                total_loss += loss.item()
                pbar.set_postfix({"Loss": loss.item()})
            except Exception as e:
                 print(f"\n[è­¦å‘Š] è®­ç»ƒæ‰¹æ¬¡ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                 # å¯ä»¥é€‰æ‹© continue è·³è¿‡æ‰¹æ¬¡ï¼Œæˆ– raise ç»ˆæ­¢è®­ç»ƒ
                 continue # æš‚æ—¶è·³è¿‡

        avg_loss = total_loss / len(train_loader) if len(train_loader) > 0 else 0
        print(f"Epoch {epoch} å®Œæˆ. å¹³å‡æŸå¤±: {avg_loss:.4f}")

    train_duration = time.time() - start_time
    print(f"SASRec è®­ç»ƒå®Œæˆ. è€—æ—¶: {train_duration:.2f} ç§’.")
    
    # --- 4. æå–åµŒå…¥ ---
    print("æ­£åœ¨æå–ç‰©å“åµŒå…¥...")
    model.eval()
    
    try:
        with torch.no_grad():
            # æå–æ•´ä¸ªåµŒå…¥çŸ©é˜µ [n_items+1, D]
            embeddings = model.item_embedding.weight.data.cpu().numpy()
        
        # ç§»é™¤ 0 å·ç´¢å¼•çš„ [PAD] åµŒå…¥
        embeddings = embeddings[1:] # [n_items, D]
        
        print(f"æå–çš„ SASRec åµŒå…¥ç»´åº¦: {embeddings.shape}")
        
        # éªŒè¯æ•°é‡
        if embeddings.shape[0] != n_items:
             print(f"[è­¦å‘Š] æå–çš„åµŒå…¥æ•°é‡ ({embeddings.shape[0]}) ä¸é¢„æœŸç‰©å“æ•°é‡ ({n_items}) ä¸ç¬¦ï¼")
             # å°è¯•ä¿®å¤ï¼šå¦‚æœæ•°é‡å°‘äºé¢„æœŸï¼Œç”¨é›¶å¡«å……
             if embeddings.shape[0] < n_items:
                  print(f" -> å°†ç”¨é›¶å‘é‡å¡«å……è‡³ {n_items} ä¸ªã€‚")
                  padding = np.zeros((n_items - embeddings.shape[0], embeddings.shape[1]), dtype=embeddings.dtype)
                  embeddings = np.concatenate([embeddings, padding], axis=0)
             else: # å¦‚æœå¤šäºé¢„æœŸï¼ˆç†è®ºä¸Šä¸åº”å‘ç”Ÿï¼‰ï¼Œæˆªæ–­
                  print(f" -> å°†æˆªæ–­è‡³ {n_items} ä¸ªã€‚")
                  embeddings = embeddings[:n_items]
        
        return embeddings.astype(np.float32)
        
    except Exception as e:
         print(f"æå– SASRec åµŒå…¥å¤±è´¥: {e}")
         raise # é‡æ–°æŠ›å‡º

# ğŸš¨ (ç§»é™¤) ä¸å†éœ€è¦ main æˆ– argparse
# if __name__ == "__main__":
#     args = ...
#     train_and_extract_sasrec(...)