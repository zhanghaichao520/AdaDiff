# preprocessing/generate_embeddings/text_encoder.py

import torch
import numpy as np
from tqdm import tqdm
import time
from transformers import AutoTokenizer, AutoModel
from openai import OpenAI 
import os # å¯¼å…¥ os
import sys # å¯¼å…¥ sys

# âœ… (æ ¸å¿ƒä¿®æ”¹) ä»çˆ¶ç›®å½•å¯¼å…¥å…±äº«å‡½æ•°
try:
    # æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    # ä» utils å¯¼å…¥éœ€è¦çš„å‡½æ•° (å¯èƒ½ä¸éœ€è¦å…¨éƒ¨å¯¼å…¥)
    # from utils import clean_text # å¦‚æœéœ€è¦å†…éƒ¨æ¸…ç†
except ImportError as e:
    print(f"å¯¼å…¥é”™è¯¯: {e}")
    print("é”™è¯¯: æ— æ³•ä»çˆ¶ç›®å½• (preprocessing/) å¯¼å…¥ utils.pyã€‚")
    sys.exit(1)

# ğŸš¨ (ç§»é™¤) ä¸å†éœ€è¦åœ¨è¿™é‡Œå®šä¹‰ load_json, clean_text, set_device ç­‰

def generate_local_text(args, item_text_list) -> np.ndarray:
    """ä½¿ç”¨æœ¬åœ° Transformer æ¨¡å‹ç”Ÿæˆæ–‡æœ¬åµŒå…¥"""
    print(f"ğŸ”¹ ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆæ–‡æœ¬åµŒå…¥: {args.model_name_or_path}")
    
    # ç¡®ä¿ device æ¥è‡ª args
    device = getattr(args, 'device', torch.device('cpu')) 
    
    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, trust_remote_code=True, cache_dir=args.model_cache_dir)
    model = AutoModel.from_pretrained(args.model_name_or_path, trust_remote_code=True, cache_dir=args.model_cache_dir).to(device)
    model.eval()
    
    # (æ•°æ®å‡†å¤‡é€»è¾‘ - ä¿æŒä¸å˜æˆ–æ ¹æ®éœ€è¦è°ƒæ•´)
    items, texts = zip(*item_text_list)
    max_item_id = max(items) if items else -1
    order_texts = [[""]] * (max_item_id + 1)
    for item, text in zip(items, texts):
        order_texts[item] = text if text else [""]
    for i in range(len(order_texts)):
        if not order_texts[i]: order_texts[i] = [""] 
    final_texts = [" ".join(t) for t in order_texts]

    embeddings = []
    with torch.no_grad():
        for i in tqdm(range(0, len(final_texts), args.batch_size), desc="Local Text Encoding"):
            batch_texts = final_texts[i : i + args.batch_size]
            batch_texts = [t if t.strip() else "N/A" for t in batch_texts] 

            try:
                encoded = tokenizer(batch_texts, padding=True, truncation=True,
                                    return_tensors="pt", max_length=args.max_sent_len).to(device)
                outputs = model(**encoded)
                attn = encoded['attention_mask'].unsqueeze(-1)
                masked = outputs.last_hidden_state * attn
                mean_output = masked.sum(dim=1) / attn.sum(dim=1).clamp(min=1e-9) 
                embeddings.append(mean_output.cpu())
            except Exception as e:
                 print(f"\n[è­¦å‘Š] æœ¬åœ°ç¼–ç æ‰¹æ¬¡ {i//args.batch_size} å¤±è´¥: {e}")
                 # ä½¿ç”¨ getattr å®‰å…¨è·å– hidden_size
                 emb_dim = getattr(getattr(model, 'config', None), 'hidden_size', 768)
                 embeddings.append(torch.zeros((len(batch_texts), emb_dim)))

    if not embeddings: # å¤„ç†å®Œå…¨å¤±è´¥çš„æƒ…å†µ
         raise RuntimeError("æœªèƒ½ç”Ÿæˆä»»ä½•æœ¬åœ°æ–‡æœ¬åµŒå…¥ã€‚")
         
    embeddings = torch.cat(embeddings, dim=0).numpy().astype(np.float32)
    
    # éªŒè¯æ•°é‡
    if embeddings.shape[0] != len(final_texts):
         print(f"[è­¦å‘Š] æœ¬åœ°æ–‡æœ¬åµŒå…¥æ•°é‡ ({embeddings.shape[0]}) ä¸é¢„æœŸ ({len(final_texts)}) ä¸ç¬¦ï¼")
         # å¡«å……æˆ–æˆªæ–­ä»¥åŒ¹é…
         target_len = len(final_texts)
         current_len = embeddings.shape[0]
         emb_dim = embeddings.shape[1]
         if current_len < target_len: # å¡«å……
              print(" -> å°†ç”¨é›¶å‘é‡å¡«å……ã€‚")
              padding = np.zeros((target_len - current_len, emb_dim), dtype=np.float32)
              embeddings = np.concatenate([embeddings, padding], axis=0)
         else: # æˆªæ–­
              print(" -> å°†æˆªæ–­å¤šä½™éƒ¨åˆ†ã€‚")
              embeddings = embeddings[:target_len]

    print(f"æœ¬åœ°æ–‡æœ¬åµŒå…¥ç»´åº¦: {embeddings.shape}")
    return embeddings

def generate_api_text(args, item_text_list) -> np.ndarray:
    """ä½¿ç”¨ OpenAI API ç”Ÿæˆæ–‡æœ¬åµŒå…¥"""
    print(f"ğŸ”¹ ä½¿ç”¨ API æ¨¡å‹ç”Ÿæˆæ–‡æœ¬åµŒå…¥: {args.sent_emb_model}")
    try:
        from openai import OpenAI
    except ImportError:
        print("é”™è¯¯: 'openai' åº“æœªæ‰¾åˆ°ã€‚è¯·è¿è¡Œ: pip install openai")
        raise # é‡æ–°æŠ›å‡ºï¼Œè®©ä¸»è„šæœ¬çŸ¥é“ä¾èµ–ç¼ºå¤±

    client = OpenAI(api_key=args.openai_api_key, base_url=args.openai_base_url)

    # (æ•°æ®å‡†å¤‡é€»è¾‘ - ä¿æŒä¸å˜)
    items, texts = zip(*item_text_list)
    max_item_id = max(items) if items else -1
    order_texts = [[""]] * (max_item_id + 1)
    for item, text in zip(items, texts):
        order_texts[item] = text if text else [""]
    for i in range(len(order_texts)):
        if not order_texts[i]: order_texts[i] = [""] 
    final_texts = [" ".join(t) for t in order_texts]

    sent_embs = []
    api_emb_dim = args.api_emb_dim 
    if api_emb_dim <= 0: # å°è¯•æ ¹æ®æ¨¡å‹åçŒœæµ‹
        if 'large' in args.sent_emb_model: api_emb_dim = 3072
        elif 'small' in args.sent_emb_model: api_emb_dim = 1536
        else: api_emb_dim = 0 # æ— æ³•çŒœæµ‹æ—¶ä¿æŒ 0
        
    print(f"[INFO] é¢„æœŸ/çŒœæµ‹çš„ API ç»´åº¦: {api_emb_dim if api_emb_dim > 0 else 'è‡ªåŠ¨æ£€æµ‹'}")

    for i in tqdm(range(0, len(final_texts), args.batch_size), desc="API Text Encoding"):
        batch = final_texts[i : i + args.batch_size]
        batch = [t if t.strip() else "N/A" for t in batch]
        
        try:
            response = client.embeddings.create(model=args.sent_emb_model, input=batch)
            batch_embeddings = [np.array(d.embedding, dtype=np.float32) for d in response.data] # ç›´æ¥è½¬ numpy
            sent_embs.extend(batch_embeddings)
            
            if api_emb_dim <= 0 and batch_embeddings:
                api_emb_dim = len(batch_embeddings[0])
                print(f"\n[INFO] å®é™…æ£€æµ‹åˆ° API åµŒå…¥ç»´åº¦ä¸º: {api_emb_dim}")
                
        except Exception as e:
            print(f"\n[è­¦å‘Š] API è¯·æ±‚æ‰¹æ¬¡ {i//args.batch_size} å¤±è´¥: {e}")
            if api_emb_dim <= 0:
                 print("é”™è¯¯ï¼šAPI ç»´åº¦æœªçŸ¥ä¸”æœªåœ¨ --api_emb_dim ä¸­æŒ‡å®šï¼Œæ— æ³•åˆ›å»ºé›¶å‘é‡ã€‚")
                 api_emb_dim = 1024 # æœ€åçš„é»˜è®¤å›é€€
                 print(f"è­¦å‘Šï¼šå°†å‡è®¾ç»´åº¦ä¸º {api_emb_dim}ã€‚")
                 
            sent_embs.extend([np.zeros(api_emb_dim, dtype=np.float32) for _ in batch])
            time.sleep(1)

    if not sent_embs: # å¤„ç†å®Œå…¨å¤±è´¥
         raise RuntimeError("æœªèƒ½ç”Ÿæˆä»»ä½• API æ–‡æœ¬åµŒå…¥ã€‚")

    # å°è¯•å°† list of numpy arrays è½¬æ¢ä¸ºå•ä¸ª numpy array
    try:
        sent_embs = np.stack(sent_embs, axis=0)
    except ValueError as e:
         print(f"é”™è¯¯ï¼šæ— æ³•å°† API è¿”å›çš„åµŒå…¥å †å æˆæ•°ç»„ ({e})ã€‚å¯èƒ½ç»´åº¦ä¸ä¸€è‡´ã€‚")
         # å°è¯•æ‰¾å‡ºä¸ä¸€è‡´çš„ç»´åº¦
         dims = [emb.shape for emb in sent_embs if isinstance(emb, np.ndarray)]
         print(f"æ£€æµ‹åˆ°çš„ç»´åº¦: {set(dims)}")
         # é€‰æ‹©å¡«å……æˆ–æŠ¥é”™ï¼Œè¿™é‡ŒæŠ¥é”™
         raise RuntimeError("API è¿”å›çš„åµŒå…¥ç»´åº¦ä¸ä¸€è‡´ã€‚") from e
         
    args.api_emb_dim = api_emb_dim # æ›´æ–° args

    # éªŒè¯æ•°é‡
    if sent_embs.shape[0] != len(final_texts):
         print(f"[è­¦å‘Š] API è¾“å‡ºåµŒå…¥æ•°é‡ ({sent_embs.shape[0]}) ä¸é¢„æœŸ ({len(final_texts)}) ä¸ç¬¦ï¼")
         # å¡«å……æˆ–æˆªæ–­
         target_len = len(final_texts)
         current_len = sent_embs.shape[0]
         emb_dim = sent_embs.shape[1]
         if current_len < target_len:
              print(" -> å°†ç”¨é›¶å‘é‡å¡«å……ã€‚")
              padding = np.zeros((target_len - current_len, emb_dim), dtype=np.float32)
              sent_embs = np.concatenate([sent_embs, padding], axis=0)
         else:
              print(" -> å°†æˆªæ–­å¤šä½™éƒ¨åˆ†ã€‚")
              sent_embs = sent_embs[:target_len]

    print(f"API æ–‡æœ¬åµŒå…¥ç»´åº¦: {sent_embs.shape}")
    return sent_embs