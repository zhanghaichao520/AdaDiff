#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
UniGenRec Fusion Module - Cross-Modal Attention (SOTA)
------------------------------------------------------
åŠŸèƒ½ï¼š
1. è¯»å–é¢„æå–çš„ Text Embedding (Qwen/T5) å’Œ Image Embedding (CLIP)ã€‚
2. è®­ç»ƒ Cross-Modal Attention ç½‘ç»œ (Text Queries Image)ã€‚
3. å°†èåˆåçš„ Embedding å¯¼å‡ºä¸º .npy æ–‡ä»¶ï¼Œä¾› RQ-VAE ä½¿ç”¨ã€‚

æ¶æ„ï¼š
Query = Text, Key/Value = Image
Loss = InfoNCE(Fused, Text) + InfoNCE(Fused, Image)
"""

import os
import sys
import argparse
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
from tqdm import tqdm

# --- å°è¯•å¯¼å…¥ utils (è·¯å¾„å…¼å®¹æ€§å¤„ç†) ---
try:
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from utils import set_device, check_path, apply_pca_and_save
    print("[INFO] æˆåŠŸä» utils.py å¯¼å…¥å…±äº«å‡½æ•°ã€‚")
except ImportError:
    try:
        sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        from utils import set_device, check_path, apply_pca_and_save
        print("[INFO] æˆåŠŸä»çˆ¶ç›®å½•å¯¼å…¥ utils.pyã€‚")
    except ImportError:
        print("[ERROR] æ— æ³•æ‰¾åˆ° utils.pyï¼Œè¯·ç¡®ä¿æ–‡ä»¶ç»“æ„æ­£ç¡®ã€‚")
        sys.exit(1)


# =================================================================
# 1. æ¨¡å‹å®šä¹‰: Cross-Modal Attention Fusion
# =================================================================
class CrossModalAttentionFusion(nn.Module):
    def __init__(self, in_dim_text, in_dim_image, out_dim=512, num_heads=8, dropout=0.1):
        super().__init__()
        self.out_dim = out_dim
        
        # 1. ç‹¬ç«‹æŠ•å½±å±‚ï¼šå°†ä¸åŒç»´åº¦çš„å›¾æ–‡æ˜ å°„åˆ°åŒä¸€ç»´åº¦
        self.text_proj = nn.Linear(in_dim_text, out_dim)
        self.img_proj = nn.Linear(in_dim_image, out_dim)
        
        # 2. äº¤å‰æ³¨æ„åŠ›å±‚ (Cross Attention)
        # batch_first=True -> (Batch, Seq, Dim)
        self.cross_attn = nn.MultiheadAttention(embed_dim=out_dim, num_heads=num_heads, dropout=dropout, batch_first=True)
        
        # 3. å‰é¦ˆç½‘ç»œ (FFN)
        self.ffn = nn.Sequential(
            nn.Linear(out_dim, out_dim * 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(out_dim * 4, out_dim)
        )
        
        # 4. Norm å±‚
        self.norm1 = nn.LayerNorm(out_dim)
        self.norm2 = nn.LayerNorm(out_dim)
        self.norm_out = nn.LayerNorm(out_dim)

    def forward(self, txt, img):
        """
        txt: (Batch, in_dim_text)
        img: (Batch, in_dim_image)
        """
        # --- A. æŠ•å½±å¹¶å¢åŠ åºåˆ—ç»´åº¦ (Batch, 1, Dim) ---
        q = self.text_proj(txt).unsqueeze(1) 
        k_v = self.img_proj(img).unsqueeze(1) 
        
        # --- B. Cross Attention: Text queries Image ---
        # attn_output: (Batch, 1, Dim)
        attn_output, _ = self.cross_attn(query=q, key=k_v, value=k_v)
        
        # --- C. æ®‹å·® + FFN ---
        # æ®‹å·®åŠ åœ¨ Query (æ–‡æœ¬) ä¸Š -> "å¢å¼ºæ–‡æœ¬"
        h = self.norm1(q + attn_output)
        h = self.norm2(h + self.ffn(h))
        
        # --- D. è¾“å‡º ---
        # ç§»é™¤åºåˆ—ç»´åº¦ -> (Batch, Dim)
        return self.norm_out(h.squeeze(1))

# =================================================================
# 2. Loss å‡½æ•°: InfoNCE
# =================================================================
def info_nce_loss(features_a, features_b, temperature=0.07):
    """è®¡ç®—ä¸¤ä¸ªç‰¹å¾é›†ä¹‹é—´çš„å¯¹æ¯”æŸå¤±"""
    # å½’ä¸€åŒ–
    a = F.normalize(features_a, dim=-1)
    b = F.normalize(features_b, dim=-1)
    
    # ç›¸ä¼¼åº¦çŸ©é˜µ (Batch, Batch)
    logits = torch.matmul(a, b.T) / temperature
    
    # æ ‡ç­¾æ˜¯å¯¹è§’çº¿ (self-supervised: i-th item in A matches i-th item in B)
    labels = torch.arange(a.shape[0], device=a.device)
    
    return F.cross_entropy(logits, labels)

# =================================================================
# 3. è¾…åŠ©å‡½æ•°: è·¯å¾„æ„å»º
# =================================================================
def build_input_path(base_dir, dataset, modality, model_tag, pca_dim):
    """è‡ªåŠ¨æ„å»ºè¾“å…¥ .npy æ–‡ä»¶è·¯å¾„"""
    emb_dir = os.path.join(base_dir, dataset, "embeddings")
    # æ¸…ç†æ ‡ç­¾
    safe_tag = model_tag.split('/')[-1].replace('/', '-').replace('\\', '-')
    pca_suffix = f"-pca{pca_dim}" if pca_dim > 0 else ""
    filename = f"{dataset}.emb-{modality}-{safe_tag}{pca_suffix}.npy"
    return os.path.join(emb_dir, filename)

def clean_tag_name(tag):
    return tag.split('/')[-1].replace('/', '-').replace(' ', '').lower()

# =================================================================
# 4. ä¸»æµç¨‹
# =================================================================
def main():
    parser = argparse.ArgumentParser("UniGenRec Attention Fusion Training")
    
    # --- è¾“å…¥å‚æ•° ---
    parser.add_argument('--dataset', type=str, required=True, help='æ•°æ®é›†åç§° (e.g., Baby)')
    parser.add_argument('--text_model_tag', type=str, required=True, help='æ–‡æœ¬æ¨¡å‹æ ‡ç­¾ (e.g., text-embedding-3-large)')
    parser.add_argument('--image_model_tag', type=str, required=True, help='å›¾åƒæ¨¡å‹æ ‡ç­¾ (e.g., clip-vit-base-patch32)')
    parser.add_argument('--save_root', type=str, default='../datasets', help='æ•°æ®æ ¹ç›®å½•')
    
    # --- PCA é€‰é¡¹ (é’ˆå¯¹è¾“å…¥) ---
    parser.add_argument('--text_pca_dim', type=int, default=0, help='è¾“å…¥æ–‡æœ¬æ˜¯å¦å·² PCA')
    parser.add_argument('--image_pca_dim', type=int, default=0, help='è¾“å…¥å›¾åƒæ˜¯å¦å·² PCA')

    # --- æ¨¡å‹ä¸è®­ç»ƒå‚æ•° ---
    parser.add_argument('--fusion_out_dim', type=int, default=512, help='èåˆè¾“å‡ºç»´åº¦ (å»ºè®®ä¸ RQ-VAE è¾“å…¥ä¸€è‡´)')
    parser.add_argument('--num_heads', type=int, default=8, help='Attention å¤´æ•°')
    parser.add_argument('--epochs', type=int, default=15, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=1024, help='Batch Size')
    parser.add_argument('--lr', type=float, default=1e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--temp', type=float, default=0.07, help='InfoNCE æ¸©åº¦')
    parser.add_argument('--gpu_id', type=int, default=0, help='GPU ID')
    parser.add_argument('--output_tag', type=str, default='attn-fusion', help='è¾“å‡ºæ–‡ä»¶åæ ‡ç­¾')

    args = parser.parse_args()
    device = set_device(args.gpu_id)

    # -----------------------------------------------------------
    # Step 1: åŠ è½½æ•°æ®
    # -----------------------------------------------------------
    print(f"\n[1/4] åŠ è½½è¾“å…¥ Embedding ({args.dataset})...")
    try:
        text_path = build_input_path(args.save_root, args.dataset, "text", args.text_model_tag, args.text_pca_dim)
        image_path = build_input_path(args.save_root, args.dataset, "image", args.image_model_tag, args.image_pca_dim)
        
        print(f"  -> Text: {os.path.basename(text_path)}")
        print(f"  -> Image: {os.path.basename(image_path)}")
        
        T_data = np.load(text_path).astype(np.float32)
        I_data = np.load(image_path).astype(np.float32)

        if T_data.shape[0] != I_data.shape[0]:
            raise ValueError(f"æ•°é‡ä¸åŒ¹é…: Text={T_data.shape[0]}, Image={I_data.shape[0]}")
        
        print(f"  -> æ•°æ®åŠ è½½æˆåŠŸã€‚æ ·æœ¬æ•°: {T_data.shape[0]}")
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        sys.exit(1)

    # -----------------------------------------------------------
    # Step 2: åˆå§‹åŒ–æ¨¡å‹
    # -----------------------------------------------------------
    print(f"\n[2/4] åˆå§‹åŒ– Cross-Modal Attention æ¨¡å‹...")
    model = CrossModalAttentionFusion(
        in_dim_text=T_data.shape[1],
        in_dim_image=I_data.shape[1],
        out_dim=args.fusion_out_dim,
        num_heads=args.num_heads
    ).to(device)
    
    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=1e-4)
    
    # æ•°æ®é›†
    dataset = TensorDataset(torch.from_numpy(T_data), torch.from_numpy(I_data))
    loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True)

    # -----------------------------------------------------------
    # Step 3: è®­ç»ƒå¾ªç¯
    # -----------------------------------------------------------
    print(f"\n[3/4] å¼€å§‹è®­ç»ƒ (Epochs: {args.epochs})...")
    model.train()
    
    for epoch in range(args.epochs):
        total_loss = 0
        pbar = tqdm(loader, desc=f"Epoch {epoch+1}/{args.epochs}")
        
        for b_txt, b_img in pbar:
            b_txt, b_img = b_txt.to(device), b_img.to(device)
            
            # Forward
            fused = model(b_txt, b_img) # (B, out_dim)
            
            # è·å–æŠ•å½±åçš„åŸå§‹ç‰¹å¾ (ä½œä¸º Target)
            target_txt = model.text_proj(b_txt)
            target_img = model.img_proj(b_img)
            
            # Loss è®¡ç®—
            # 1. Fidelity: èåˆååº”è¯¥ä¾ç„¶åƒæ–‡æœ¬ (ä¿çœŸ)
            loss_txt = info_nce_loss(fused, target_txt, args.temp)
            
            # 2. Injection: èåˆååº”è¯¥åŒ…å«å›¾åƒä¿¡æ¯ (æ³¨å…¥)
            loss_img = info_nce_loss(fused, target_img, args.temp)
            
            # æ€» Loss: ç¨å¾®ä¾§é‡æ–‡æœ¬ä¿çœŸåº¦ï¼Œå› ä¸ºæ–‡æœ¬é€šå¸¸æ˜¯ä¸»å¯¼æ¨¡æ€
            loss = 0.7 * loss_txt + 0.3 * loss_img
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            pbar.set_postfix(loss=f"{loss.item():.4f}")
            
    # -----------------------------------------------------------
    # Step 4: å¯¼å‡ºä¸ä¿å­˜
    # -----------------------------------------------------------
    print(f"\n[4/4] å¯¼å‡ºèåˆ Embedding...")
    model.eval()
    export_loader = DataLoader(dataset, batch_size=args.batch_size * 2, shuffle=False)
    
    all_fused = []
    with torch.no_grad():
        for b_txt, b_img in tqdm(export_loader, desc="Exporting"):
            b_txt, b_img = b_txt.to(device), b_img.to(device)
            out = model(b_txt, b_img)
            all_fused.append(out.cpu().numpy())
            
    final_emb = np.concatenate(all_fused, axis=0)
    
    # æ„å»ºè¾“å‡ºè·¯å¾„
    emb_dir = os.path.join(args.save_root, args.dataset, "embeddings")
    t_tag = clean_tag_name(args.text_model_tag)
    i_tag = clean_tag_name(args.image_model_tag)
    
    # æ–‡ä»¶å: Baby.emb-fused-attn-fusion-text_tag-image_tag.npy
    filename = f"{args.dataset}.emb-fused-{args.output_tag}-{t_tag}-{i_tag}.npy"
    save_path = os.path.join(emb_dir, filename)
    
    # ä¿å­˜ (apply_pca_and_save ä¼šå¤„ç†ä¿å­˜é€»è¾‘ï¼Œå¦‚æœä¸é™ç»´ç›´æ¥å­˜)
    # è¿™é‡Œçš„ args.pca_dim æŒ‡çš„æ˜¯ *æœ€ç»ˆè¾“å‡º* æ˜¯å¦è¦å†é™ç»´ï¼Œé€šå¸¸ä¸éœ€è¦äº†ï¼Œå› ä¸ºæ¨¡å‹è¾“å‡ºå°±æ˜¯ 512
    saved_path = apply_pca_and_save(final_emb, argparse.Namespace(pca_dim=0), save_path)
    
    print(f"ğŸ‰ å®Œæˆï¼èåˆæ–‡ä»¶å·²ä¿å­˜: {saved_path}")
    print(f"ğŸ‘‰ ä¸‹ä¸€æ­¥: è¯·å°†æ­¤æ–‡ä»¶è·¯å¾„é…ç½®ä¸º RQ-VAE çš„è¾“å…¥ã€‚")

if __name__ == "__main__":
    main()