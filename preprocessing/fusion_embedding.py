#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šæ¨¡æ€èåˆ V3 - é»„é‡‘æ ‡å‡†å¯¹é½ (ç‹¬ç«‹è®­ç»ƒè„šæœ¬)
(V3.1 - è‡ªåŠ¨è·¯å¾„æ„å»º)

èŒè´£ï¼š
1. æ ¹æ®æ¨¡å‹æ ‡ç­¾è‡ªåŠ¨æ„å»ºè¾“å…¥ .npy æ–‡ä»¶è·¯å¾„ã€‚
2. è®­ç»ƒ GatedFusion å¤´ (L(H<->T) æŸå¤±)ã€‚
3. å¯¼å‡ºèåˆåçš„ H å‘é‡ã€‚
"""

import os
import sys
import argparse
import numpy as np
from tqdm import tqdm

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, TensorDataset
from sklearn.decomposition import PCA
import joblib 

# (æ ¸å¿ƒ) ä»çˆ¶ç›®å½•å¯¼å…¥ utils.py
try:
    # å‡è®¾æ­¤è„šæœ¬ä½äº preprocessing/ ç›®å½•ä¸‹
    sys.path.append(os.path.dirname(os.path.abspath(__file__))) 
    from utils import set_device, apply_pca_and_save, check_path
    print("[INFO] æˆåŠŸä» utils.py å¯¼å…¥å…±äº«å‡½æ•°ã€‚")
except ImportError as e:
    # å¦‚æœ utils.py åœ¨ä¸Šä¸€çº§ (ä¾‹å¦‚ preprocessing/utils.py)
    try:
        sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        from utils import set_device, apply_pca_and_save, check_path
        print("[INFO] æˆåŠŸä»çˆ¶ç›®å½• utils.py å¯¼å…¥å…±äº«å‡½æ•°ã€‚")
    except ImportError:
        print(f"å¯¼å…¥é”™è¯¯: {e}")
        print("é”™è¯¯: æ— æ³•ä» (preprocessing/) ç›®å½•å¯¼å…¥ utils.pyã€‚")
        sys.exit(1)

# =================================================================
# ================== 1. èåˆå¤´æ¨¡å‹ (GatedFusion) ==================
# =================================================================
class GatedFusion(nn.Module):
    """
    é—¨æ§èåˆå¤´ (ä¿®æ”¹ç‰ˆ - æ”¯æŒä¸åŒè¾“å…¥ç»´åº¦)ã€‚
    """
    # âœ… (ä¿®æ”¹) __init__ ç­¾å
    def __init__(self, in_dim_text: int, in_dim_image: int, mid_dim: int, out_dim: int, dropout=0.1):
        super().__init__()
        self.in_dim_text = in_dim_text
        self.in_dim_image = in_dim_image
        self.in_dim_concat = in_dim_text + in_dim_image # æ‹¼æ¥åçš„ç»´åº¦
        self.out_dim = out_dim
        
        # âœ… (ä¿®æ”¹) LayerNorm ä½œç”¨äºæ‹¼æ¥åçš„ç»´åº¦
        self.ln = nn.LayerNorm(self.in_dim_concat) 
        
        # âœ… (ä¿®æ”¹) MLP è·¯å¾„å¤„ç†æ‹¼æ¥åçš„ç»´åº¦
        self.fc1 = nn.Linear(self.in_dim_concat, mid_dim)
        self.fc2 = nn.Linear(mid_dim, out_dim)
        self.dropout = nn.Dropout(dropout)

        # âœ… (ä¿®æ”¹) Gate è¾“å…¥ä¹Ÿæ˜¯æ‹¼æ¥åçš„ç»´åº¦
        self.gate = nn.Linear(self.in_dim_concat, 2) 
        
        # âœ… (ä¿®æ”¹) ç‹¬ç«‹æŠ•å½±å±‚ï¼Œè¾“å…¥ç»´åº¦ä¸åŒ
        self.proj_text = nn.Linear(self.in_dim_text, out_dim, bias=False) 
        self.proj_image = nn.Linear(self.in_dim_image, out_dim, bias=False) 

        self.res_scale = nn.Parameter(torch.tensor(0.5))

    def forward(self, txt: torch.Tensor, img: torch.Tensor):
        # txt: (B, D_text), img: (B, D_image)
        
        # æ‹¼æ¥ (ç°åœ¨ç»´åº¦æ˜¯ D_T + D_I)
        x = torch.cat([txt, img], dim=-1)    # (B, D_T + D_I)
        x_n = self.ln(x)                     # (B, D_T + D_I)
        
        # 1. é—¨æ§è·¯å¾„
        g = torch.sigmoid(self.gate(x_n))    # (B, 2)
        # (ç‹¬ç«‹æŠ•å½±)
        t_proj = self.proj_text(txt)         # (B, D_out)
        i_proj = self.proj_image(img)        # (B, D_out)
        gated = g[:, :1] * t_proj + g[:, 1:] * i_proj # (B, D_out)

        # 2. MLP æ®‹å·®è·¯å¾„ (è¾“å…¥æ˜¯ x_n)
        h = self.fc2(F.gelu(self.fc1(x_n)))
        h = self.dropout(h)                  # (B, D_out)

        out = gated + self.res_scale * h
        out = F.normalize(out, dim=-1)
        return out

# =================================================================
# ================== 2. æŸå¤±å‡½æ•° ==================
# =================================================================
def info_nce_from_pairs(anchor, positive, temperature):
    # ... (info_nce_from_pairs å‡½æ•°çš„ä»£ç ä¿æŒä¸å˜) ...
    anchor = F.normalize(anchor, dim=-1); positive = F.normalize(positive, dim=-1)
    logits = torch.matmul(anchor, positive.t()) / temperature
    labels = torch.arange(anchor.shape[0], device=anchor.device)
    return F.cross_entropy(logits, labels)

# =================================================================
# ================== 3. ä¸»è®­ç»ƒä¸æå–å‡½æ•° ==================
# =================================================================
def train_fusion_head(args, T_emb: np.ndarray, I_emb: np.ndarray):
    """
    è®­ç»ƒ GatedFusion æ¨¡å‹å¤´ (ä¿®æ”¹ç‰ˆ - å¤„ç†ä¸åŒç»´åº¦è¾“å…¥)ã€‚
    """
    device = args.device
    
    # --- 1. éªŒè¯è¾“å…¥ ---
    if T_emb.shape[0] != I_emb.shape[0]:
        raise ValueError(f"æ–‡æœ¬ ({T_emb.shape[0]}) å’Œå›¾åƒ ({I_emb.shape[0]}) ç‰©å“æ•°é‡ä¸åŒ¹é…ï¼")
    # ğŸš¨ (ç§»é™¤) ä¸å†éœ€è¦æ£€æŸ¥ç»´åº¦æ˜¯å¦ç›¸ç­‰
    # if T_emb.shape[1] != I_emb.shape[1]: ... 
    
    # âœ… (ä¿®æ”¹) è·å–ä¸¤ä¸ªè¾“å…¥çš„ç»´åº¦
    in_dim_text = T_emb.shape[1]
    in_dim_image = I_emb.shape[1]
    
    # ç¡®å®šè¾“å‡ºç»´åº¦ (å¦‚æœæœªæŒ‡å®šæˆ–ä¸º 0ï¼Œåˆ™é»˜è®¤ä¸ºæ–‡æœ¬ç»´åº¦)
    out_dim = args.fusion_out_dim if args.fusion_out_dim > 0 else in_dim_text 
    # ç¡®å®šä¸­é—´å±‚ç»´åº¦ (å¯ä»¥åŸºäºè¾“å…¥æˆ–è¾“å‡ºç»´åº¦)
    mid_dim = max(out_dim, (in_dim_text + in_dim_image) // 2) # ä¾‹å¦‚å–æ‹¼æ¥ç»´åº¦çš„ä¸€åŠ
    
    print(f"[INFO] è®­ç»ƒ GatedFusionï¼šText Dim={in_dim_text}, Image Dim={in_dim_image}, Mid Dim={mid_dim}, Output Dim={out_dim}")

    # --- 2. âœ… (ä¿®æ”¹) åˆå§‹åŒ–æ¨¡å‹ (ä¼ å…¥ä¸¤ä¸ª in_dim) ---
    fusion_head = GatedFusion(
        in_dim_text=in_dim_text,     # <--- ä¼ å…¥æ–‡æœ¬ç»´åº¦
        in_dim_image=in_dim_image,   # <--- ä¼ å…¥å›¾åƒç»´åº¦
        mid_dim=mid_dim,
        out_dim=out_dim,
        dropout=args.fusion_dropout
    ).to(device)

    # --- 3. å‡†å¤‡è®­ç»ƒ (ä¸å˜) ---
    optimizer = torch.optim.AdamW(fusion_head.parameters(), lr=args.fusion_lr, weight_decay=args.fusion_weight_decay)
    scaler = torch.cuda.amp.GradScaler(enabled=(device.type == 'cuda' and args.amp))
    print("åˆ›å»º DataLoader...")
    dataset = TensorDataset(torch.from_numpy(T_emb).float(), torch.from_numpy(I_emb).float())
    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=(device.type == 'cuda'))

    # --- 4. è®­ç»ƒèåˆå¤´ (ä¸å˜) ---
    print(f"å¼€å§‹è®­ç»ƒ GatedFusion å¤´... (Epochs: {args.fusion_epochs}, LR: {args.fusion_lr})")
    best_loss = float("inf")
    ckpt_dir = os.path.join(args.save_root, args.dataset, "embeddings", "checkpoints")
    check_path(ckpt_dir)
    best_ckpt_path = os.path.join(ckpt_dir, f"fusion_head_{args.output_tag}.pt")

    for epoch in range(args.fusion_epochs):
        # ... (è®­ç»ƒå¾ªç¯å†…éƒ¨é€»è¾‘å®Œå…¨ä¸å˜ï¼Œå› ä¸º L(H<->T) ä»ç„¶é€‚ç”¨) ...
        fusion_head.train()
        pbar = tqdm(dataloader, desc=f"Train Fusion Epoch {epoch+1}/{args.fusion_epochs}")
        epoch_loss = 0.0
        for batch_T, batch_I in pbar:
            T, I = batch_T.to(device), batch_I.to(device)
            with torch.amp.autocast('cuda', enabled=(device.type == 'cuda' and args.amp)):
                H = fusion_head(T, I)  # (B, out_dim)

                # æ–‡æœ¬/è§†è§‰å„è‡ªæŠ•å½±åˆ°åŒä¸€ out_dimï¼ˆå¦‚ 512ï¼‰
                T_proj = F.normalize(fusion_head.proj_text(T), dim=-1)
                I_proj = F.normalize(fusion_head.proj_image(I), dim=-1)

                # æ–‡æœ¬æŒ‡å¯¼èåˆï¼ˆå·²æœ‰æ€æƒ³ï¼‰
                L_ht = info_nce_from_pairs(H, T_proj.detach(), temperature=args.fusion_temperature)

                # ğŸ”¥ æ–‡æœ¬æŒ‡å¯¼è§†è§‰ï¼ˆæ ¸å¿ƒï¼‰
                L_it = info_nce_from_pairs(I_proj, T_proj.detach(), temperature=args.fusion_temperature)
                L_kd = 1 - (I_proj * T_proj.detach()).sum(dim=-1).mean()  # ä¹Ÿå¯æ¢æˆ F.mse_loss

                # èåˆå¸æ”¶è§†è§‰
                L_hi = info_nce_from_pairs(H, I_proj.detach(), temperature=args.fusion_temperature)

                loss = 1.0 * L_ht + 1.0 * L_it + 0.5 * L_hi + 0.1 * L_kd


            optimizer.zero_grad(set_to_none=True)
            scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()
            epoch_loss += loss.item()
            pbar.set_postfix(loss=f"{loss.item():.4f}")
        avg = epoch_loss / len(dataloader) if len(dataloader) > 0 else 0
        print(f"[E{epoch+1}] èåˆå¤´è®­ç»ƒ Loss = {avg:.4f}")
        if avg < best_loss:
            best_loss = avg; torch.save(fusion_head.state_dict(), best_ckpt_path)
            print(f"  -> ä¿å­˜æœ€ä½³èåˆå¤´åˆ°: {best_ckpt_path}")

    # --- 5. åŠ è½½æœ€ä½³æ¨¡å‹ (ä¸å˜) ---
    if os.path.exists(best_ckpt_path):
        print(f"åŠ è½½æœ€ä½³èåˆå¤´: {best_ckpt_path}")
        fusion_head.load_state_dict(torch.load(best_ckpt_path, map_location=device))
    else: print("[è­¦å‘Š] æœªæ‰¾åˆ°æœ€ä½³èåˆå¤´æ¨¡å‹ï¼Œä½¿ç”¨æœ€å epoch æƒé‡ã€‚")
    fusion_head.eval()
    return fusion_head

def export_fused_embeddings(args, fusion_head: GatedFusion, T_emb: np.ndarray, I_emb: np.ndarray) -> np.ndarray:
    # ... (export_fused_embeddings å‡½æ•°çš„ä»£ç ä¿æŒä¸å˜) ...
    print("\nèåˆå¤´è®­ç»ƒå®Œæˆã€‚å¼€å§‹å¯¼å‡ºæ‰€æœ‰ç‰©å“çš„ Embedding...")
    device = args.device; fusion_head.eval()
    dataset = TensorDataset(torch.from_numpy(T_emb).float(), torch.from_numpy(I_emb).float())
    inference_loader = DataLoader(dataset, batch_size=args.batch_size * 2, shuffle=False, num_workers=args.num_workers)
    all_feats = []
    with torch.no_grad():
        for batch_T, batch_I in tqdm(inference_loader, desc="Exporting Fused Embeddings"):
            T, I = batch_T.to(device), batch_I.to(device)
            H = fusion_head(T, I); all_feats.append(H.cpu())
    if not all_feats: raise RuntimeError("æœªèƒ½ç”Ÿæˆä»»ä½•èåˆ Embeddingã€‚")
    Z = torch.cat(all_feats, dim=0).numpy().astype(np.float32)
    if Z.shape[0] != T_emb.shape[0]:
       print(f"[è­¦å‘Š] è¾“å‡ºåµŒå…¥æ•°é‡ ({Z.shape[0]}) ä¸ç‰©å“æ•°é‡ ({T_emb.shape[0]}) ä¸ç¬¦ï¼å°†ä¿®å¤ã€‚")
       target_len = T_emb.shape[0]; current_len = Z.shape[0]; emb_dim = Z.shape[1]
       if current_len < target_len: Z = np.concatenate([Z, np.zeros((target_len - current_len, emb_dim), dtype=np.float32)], axis=0)
       else: Z = Z[:target_len]
    print(f"èåˆåµŒå…¥æå–å®Œæˆã€‚æœ€ç»ˆç»´åº¦: {Z.shape}")
    return Z


# =================================================================
# ================== 4. è¾…åŠ©å‡½æ•°å’Œä¸»ç¨‹åºå…¥å£ ==================
# =================================================================

def build_input_path(base_dir: str, dataset: str, modality: str, model_tag: str, pca_dim: int) -> str:
    """(æ–°å¢) è¾…åŠ©å‡½æ•°ï¼šæ ¹æ®ç»„ä»¶æ„å»ºè¾“å…¥çš„ .npy æ–‡ä»¶è·¯å¾„"""
    emb_dir = os.path.join(base_dir, dataset, "embeddings")
    
    # æ¸…ç† model_tag (ä»¥é˜²ä¸‡ä¸€)
    safe_model_tag = model_tag.split('/')[-1].replace('/', '-').replace('\\', '-')
    
    pca_suffix = f"-pca{pca_dim}" if pca_dim > 0 else ""
    filename = f"{dataset}.emb-{modality}-{safe_model_tag}{pca_suffix}.npy"
    return os.path.join(emb_dir, filename)


def parse_args():
    ap = argparse.ArgumentParser("V3.1: ç‹¬ç«‹çš„å¤šæ¨¡æ€èåˆè®­ç»ƒè„šæœ¬ (è‡ªåŠ¨è·¯å¾„æ„å»º)")
    
    # --- âœ… (ä¿®æ”¹) å¿…éœ€çš„è¾“å…¥å‚æ•° ---
    ap.add_argument('--dataset', type=str, required=True, help='æ•°æ®é›†åç§° (e.g., Baby)')
    ap.add_argument('--text_model_tag', type=str, required=True, 
                        help='æ–‡æœ¬åµŒå…¥çš„æ¨¡å‹æ ‡ç­¾ (ä¾‹å¦‚: "text-embedding-3-large")')
    ap.add_argument('--image_model_tag', type=str, required=True, 
                        help='å›¾åƒåµŒå…¥çš„æ¨¡å‹æ ‡ç­¾ (ä¾‹å¦‚: "clip-vit-base-patch32")')
    
    # --- âœ… (æ–°å¢) å¯é€‰çš„è¾“å…¥ PCA å‚æ•° ---
    ap.add_argument('--text_pca_dim', type=int, default=0,
                        help='(å¯é€‰) è¾“å…¥çš„æ–‡æœ¬åµŒå…¥æ˜¯å¦ç»è¿‡ PCAã€‚å¦‚æœæ˜¯, æŒ‡å®šç»´åº¦ (e.g., 512)ã€‚')
    ap.add_argument('--image_pca_dim', type=int, default=0,
                        help='(å¯é€‰) è¾“å…¥çš„å›¾åƒåµŒå…¥æ˜¯å¦ç»è¿‡ PCAã€‚å¦‚æœæ˜¯, æŒ‡å®šç»´åº¦ (e.g., 512)ã€‚')

    # ğŸš¨ (ç§»é™¤) --text_emb_path å’Œ --image_emb_path
    
    # --- è·¯å¾„ ---
    ap.add_argument('--save_root', type=str, default='../datasets', help='ä¿å­˜é¢„å¤„ç†æ•°æ®çš„æ ¹ç›®å½• (ç”¨äºæŸ¥æ‰¾ embeddings å’Œä¿å­˜ checkpoints/è¾“å‡º)')
    
    # --- è¾“å‡ºæ§åˆ¶ ---
    ap.add_argument('--output_tag', type=str, default='fused-gold', 
                        help='è¾“å‡ºæ–‡ä»¶åä¸­çš„æ ‡ç­¾ (e.g., "fused-gold-v1")')
    
    # --- è®­ç»ƒå‚æ•° (ä¿æŒä¸å˜) ---
    ap.add_argument('--fusion_epochs', type=int, default=10, help='èåˆå¤´è®­ç»ƒè½®æ•°')
    ap.add_argument('--fusion_lr', type=float, default=5e-4, help='èåˆå¤´å­¦ä¹ ç‡')
    ap.add_argument('--fusion_weight_decay', type=float, default=0.01, help='èåˆå¤´æƒé‡è¡°å‡')
    ap.add_argument('--fusion_temperature', type=float, default=0.07, help='H<->T å¯¹é½æ¸©åº¦')
    ap.add_argument('--fusion_out_dim', type=int, default=512, help='èåˆå¤´è¾“å‡ºç»´åº¦ (0 è¡¨ç¤ºä¸è¾“å…¥ç›¸åŒ)')
    ap.add_argument('--fusion_dropout', type=float, default=0.1, help='èåˆå¤´ Dropout')
    ap.add_argument('--amp', action='store_true', help='å¯ç”¨ AMP æ··åˆç²¾åº¦è®­ç»ƒ')

    # --- é€šç”¨å‚æ•° (ä¿æŒä¸å˜) ---
    ap.add_argument('--batch_size', type=int, default=4096, help='è®­ç»ƒå’Œæ¨ç†çš„æ‰¹å¤„ç†å¤§å°')
    ap.add_argument('--num_workers', type=int, default=16, help='DataLoader num_workers')
    ap.add_argument('--pca_dim', type=int, default=0, help='(å¯é€‰) å¯¹*æœ€ç»ˆ*èåˆåµŒå…¥åº”ç”¨ PCA (<=0 ä¸é™ç»´)')
    ap.add_argument('--gpu_id', type=int, default=0, help='GPU ID (<0 ä½¿ç”¨ CPU)')
    
    return ap.parse_args()


def main():
    args = parse_args()
    args.device = set_device(args.gpu_id)
    print(f"[CFG] èåˆè„šæœ¬å¯åŠ¨: {args.dataset}")

    # --- 1. âœ… (ä¿®æ”¹) è‡ªåŠ¨æ„å»ºè¾“å…¥è·¯å¾„ ---
    try:
        text_emb_path = build_input_path(
            args.save_root, args.dataset, 
            "text", args.text_model_tag, args.text_pca_dim
        )
        # å‡è®¾å›¾åƒæ¨¡æ€æ ‡ç­¾ä¸º "image"
        image_emb_path = build_input_path(
            args.save_root, args.dataset, 
            "image", args.image_model_tag, args.image_pca_dim
        )
        
        print(f"[CFG] æ–‡æœ¬è¾“å…¥ (è‡ªåŠ¨æ„å»º): {text_emb_path}")
        print(f"[CFG] å›¾åƒè¾“å…¥ (è‡ªåŠ¨æ„å»º): {image_emb_path}")

        # --- 2. åŠ è½½æ•°æ® ---
        print("åŠ è½½æ–‡æœ¬ Embedding...")
        T_emb = np.load(text_emb_path)
        print(f"  -> å½¢çŠ¶: {T_emb.shape}")
        
        print("åŠ è½½å›¾åƒ Embedding...")
        I_emb = np.load(image_emb_path)
        print(f"  -> å½¢çŠ¶: {I_emb.shape}")
        
    except FileNotFoundError as e:
        print(f"\nâŒ é”™è¯¯: æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ {e}")
        print("è¯·æ£€æŸ¥ --dataset, --save_root, --text_model_tag, --image_model_tag, --text_pca_dim, --image_pca_dim å‚æ•°æ˜¯å¦æ­£ç¡®ã€‚")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ é”™è¯¯: åŠ è½½ .npy æ–‡ä»¶å¤±è´¥: {e}")
        sys.exit(1)

    # --- 3. è®­ç»ƒèåˆå¤´ ---
    trained_fusion_head = train_fusion_head(args, T_emb, I_emb)
    
    # --- 4. å¯¼å‡ºèåˆåµŒå…¥ ---
    fused_embeddings = export_fused_embeddings(args, trained_fusion_head, T_emb, I_emb)

    # --- 5. âœ… (ä¿®æ”¹) æ„å»ºæ›´å…·æè¿°æ€§çš„è¾“å‡ºè·¯å¾„å¹¶ä¿å­˜ ---
    emb_dir = os.path.join(args.save_root, args.dataset, "embeddings")
    check_path(emb_dir)
    
    # æ„å»ºåŒ…å«æºä¿¡æ¯çš„æ–‡ä»¶å
    # ä¾‹å¦‚: Baby.emb-fused-gold-(T_text-embedding-3-large-pca512+I_clip-vit-base-patch32).npy
    # âœ… æ–°ç‰ˆè§„èŒƒåŒ–å‘½å
    def clean_tag(tag: str):
        """æ¸…ç†æ¨¡å‹åï¼Œé¿å…è·¯å¾„ç¬¦å·å’Œç©ºæ ¼"""
        return tag.split('/')[-1].replace('/', '-').replace('\\', '-').replace(' ', '').lower()

    text_tag = clean_tag(args.text_model_tag)
    image_tag = clean_tag(args.image_model_tag)

    # æ–‡ä»¶å: Baby.emb-fused-textmodel-imagemodel.npy
    output_filename = f"{args.dataset}.emb-fused-{text_tag}-{image_tag}.npy"
    output_path = os.path.join(emb_dir, output_filename)

    
    # apply_pca_and_save ä¼šå¤„ç†æœ€ç»ˆçš„ PCA é€»è¾‘å¹¶ä¿å­˜
    final_output_path = apply_pca_and_save(fused_embeddings, args, output_path)
    
    print(f"\nğŸ‰ èåˆä»»åŠ¡å®Œæˆï¼æœ€ç»ˆ Embedding å·²ä¿å­˜è‡³: {final_output_path}")


if __name__ == "__main__":
    """
    ä½¿ç”¨ç¤ºä¾‹:
    
    # ç¤ºä¾‹ 1: èåˆä¸¤ä¸ªåŸå§‹ç»´åº¦çš„ CLIP åµŒå…¥
    python preprocessing/train_fusion_model.py \
        --dataset Baby \
        --text_model_tag "clip-vit-base-patch32" \
        --image_model_tag "clip-vit-base-patch32" \
        --output_tag "fused-gold-clip" \
        --fusion_epochs 10 \
        --batch_size 4096 \
        --fusion_out_dim 512 \
        --pca_dim 0 \
        --amp
        
    # ç¤ºä¾‹ 2: èåˆä¸€ä¸ª PCA é™ç»´è¿‡çš„ text-embedding-3-large å’Œä¸€ä¸ªåŸå§‹ç»´åº¦çš„ CLIP å›¾åƒ
    python preprocessing/train_fusion_model.py \
        --dataset Baby \
        --text_model_tag "text-embedding-3-large" \
        --text_pca_dim 512 \
        --image_model_tag "clip-vit-base-patch32" \
        --image_pca_dim 0 \
        --output_tag "fused-gold-T_api_pca+I_clip" \
        --fusion_epochs 10 \
        --fusion_out_dim 512 \
        --pca_dim 0 \
        --amp
    """
    main()