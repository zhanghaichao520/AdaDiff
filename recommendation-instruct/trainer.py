# æ–‡ä»¶è·¯å¾„: instruction_peft_trainer.py
import torch
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM, get_linear_schedule_with_warmup
from peft import LoraConfig, get_peft_model, TaskType
import logging
import argparse
import os
from pathlib import Path
import math
import pprint # ç”¨äºæ‰“å°é…ç½®

# å‡è®¾ instruction_dataset.py åœ¨åŒä¸€ç›®å½•ä¸‹æˆ–å¯å¯¼å…¥
try:
    from dataset import InstructionDataset, DataCollatorForInstructionTuning
except ImportError:
    print("é”™è¯¯ï¼šæ— æ³•å¯¼å…¥ instruction_dataset.py")
    raise

# âœ… å¯¼å…¥æ–°çš„å·¥å…·å‡½æ•°
try:
    from utils import load_instruction_config, setup_instruction_logging
except ImportError:
    print("é”™è¯¯ï¼šæ— æ³•å¯¼å…¥ instruction_utils.py")
    raise

# ä½¿ç”¨ root logger (ç”± setup_instruction_logging é…ç½®)
logger = logging.getLogger(__name__) # è·å–å½“å‰æ¨¡å—çš„ logger

# ============================================
#            è®­ç»ƒä¸è¯„ä¼°é€»è¾‘ (ä¿æŒä¸å˜)
# ============================================
def train_instruction_epoch(model, train_loader, optimizer, scheduler, device):
    """æ‰§è¡Œä¸€ä¸ªæŒ‡ä»¤å¾®è°ƒè®­ç»ƒå‘¨æœŸ"""
    model.train()
    total_loss = 0.0
    for batch in tqdm(train_loader, desc="Training"):
        # DataCollator å·²å°†æ•°æ®è½¬ä¸º Tensorï¼Œåªéœ€ç§»åŠ¨è®¾å¤‡
        batch = {k: v.to(device) for k, v in batch.items()}
        
        optimizer.zero_grad()
        
        # æ¨¡å‹ forward (PEFT æ¨¡å‹ä¼šè‡ªåŠ¨å¤„ç†)
        # å¯¹äº CausalLMï¼ŒHugging Face æ¨¡å‹ä¼šè‡ªåŠ¨è®¡ç®—æŸå¤±
        outputs = model(**batch)
        loss = outputs.loss
        
        if loss is None:
            logger.warning("æ¨¡å‹æœªè¿”å›æŸå¤±ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡ã€‚è¯·æ£€æŸ¥æ¨¡å‹é…ç½®å’Œ DataCollatorã€‚")
            continue
            
        loss.backward()
        optimizer.step()
        scheduler.step() # é€šå¸¸å­¦ä¹ ç‡è°ƒåº¦å™¨åœ¨æ¯ä¸ª step æ›´æ–°
        
        total_loss += loss.item()
        
    return total_loss / len(train_loader)

@torch.no_grad()
def evaluate_loss(model, eval_loader, device) -> float:
    """åœ¨è¯„ä¼°é›†ä¸Šç®€å•è®¡ç®—æŸå¤± (ä½œä¸º Perplexity çš„ä»£ç†)"""
    model.eval()
    total_loss = 0.0
    total_tokens = 0 # ç”¨äºæ›´ç²¾ç¡®åœ°è®¡ç®— Perplexity

    for batch in tqdm(eval_loader, desc="Evaluating"):
        batch = {k: v.to(device) for k, v in batch.items()}
        
        outputs = model(**batch)
        loss = outputs.loss

        if loss is not None:
             # è®¡ç®—æ‰¹æ¬¡ä¸­ç”¨äºè®¡ç®—æŸå¤±çš„ token æ•°é‡
             valid_labels = batch['labels'] != -100
             num_valid_tokens = valid_labels.sum().item()
             # ç´¯åŠ åŠ æƒæŸå¤± (loss * num_tokens)
             total_loss += loss.item() * num_valid_tokens
             total_tokens += num_valid_tokens
        else:
             logger.warning("è¯„ä¼°æ—¶æ¨¡å‹æœªè¿”å›æŸå¤±ã€‚")

    if total_tokens == 0:
        logger.warning("è¯„ä¼°é›†ä¸Šæ²¡æœ‰æœ‰æ•ˆçš„ token ç”¨äºè®¡ç®—æŸå¤±ã€‚")
        return float('inf')
        
    avg_loss = total_loss / total_tokens
    # å¯ä»¥é€‰æ‹©è¿”å› avg_loss æˆ– perplexity
    # perplexity = math.exp(avg_loss)
    # return perplexity
    return avg_loss # è¿”å›å¹³å‡æŸå¤±æ›´ç›´æ¥

# ============================================
#                ä¸»ç¨‹åºå…¥å£ (å·²ä¿®æ”¹)
# ============================================

def main():
    parser = argparse.ArgumentParser(description="åŸºäº PEFT (LoRA) çš„æ™ºèƒ½æŒ‡ä»¤å¾®è°ƒè®­ç»ƒè„šæœ¬")

    # --- âœ… ç®€åŒ–å‘½ä»¤è¡Œå‚æ•° ---
    parser.add_argument('--dataset_name', type=str, required=True, help='æ•°æ®é›†åç§° (e.g., Baby)')
    parser.add_argument('--base_model_alias', type=str, required=True, help='é…ç½®æ–‡ä»¶ä¸­å®šä¹‰çš„æ¨¡å‹åˆ«å (e.g., gpt2-medium)')

    # --- ä¿ç•™å…³é”®çš„å¯è¦†ç›–è¶…å‚æ•° (è®¾ä¸º None è¡¨ç¤ºé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶) ---
    parser.add_argument('--epochs', type=int, default=None, help='è®­ç»ƒè½®æ•° (è¦†ç›–é…ç½®)')
    parser.add_argument('--batch_size', type=int, default=None, help='è®­ç»ƒæ‰¹é‡å¤§å° (è¦†ç›–é…ç½®)')
    parser.add_argument('--eval_batch_size', type=int, default=None, help='è¯„ä¼°æ‰¹é‡å¤§å° (è¦†ç›–é…ç½®)')
    parser.add_argument('--lr', type=float, default=None, help='å­¦ä¹ ç‡ (è¦†ç›–é…ç½®)')
    parser.add_argument('--max_seq_len', type=int, default=None, help='æœ€å¤§åºåˆ—é•¿åº¦ (è¦†ç›–é…ç½®)')
    parser.add_argument('--lora_r', type=int, default=None, help='LoRA çš„ç§© (è¦†ç›–é…ç½®)')
    parser.add_argument('--lora_alpha', type=int, default=None, help='LoRA çš„ alpha (è¦†ç›–é…ç½®)')
    parser.add_argument('--device', type=str, default=None, help='è®­ç»ƒè®¾å¤‡ (è¦†ç›–é…ç½®, e.g., cuda:1)')
    # ä½ å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ æ›´å¤šå¯è¦†ç›–çš„å‚æ•°

    args = parser.parse_args()

    # === ğŸš€ æ ¸å¿ƒæ”¹åŠ¨ï¼šä½¿ç”¨å·¥å…·å‡½æ•°åŠ è½½å’Œå¤„ç†é…ç½® ===
    try:
        config = load_instruction_config(args.dataset_name, args.base_model_alias, args)
    except (FileNotFoundError, ValueError) as e:
        logging.error(f"åŠ è½½æˆ–å¤„ç†é…ç½®å¤±è´¥: {e}")
        return # ç»ˆæ­¢ç¨‹åº

    # === è®¾ç½®æ—¥å¿— (ä½¿ç”¨ config ä¸­æ¨å¯¼å‡ºçš„è·¯å¾„) ===
    setup_instruction_logging(config['log_path'])

    # æ‰“å°æœ€ç»ˆä½¿ç”¨çš„é…ç½®
    logger.info("=" * 30 + " æœ€ç»ˆé…ç½® " + "=" * 30)
    config_str = pprint.pformat(config)
    logger.info("\n" + config_str)
    logger.info("=" * (60 + len(" æœ€ç»ˆé…ç½® ")))

    # --- åç»­æµç¨‹ä½¿ç”¨ config ä¸­çš„å€¼ ---
    device = torch.device(config['device'] if torch.cuda.is_available() and 'cuda' in config['device'] else "cpu")
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # 1. åŠ è½½ Tokenizer (ä½¿ç”¨ config ä¸­çš„è·¯å¾„)
    tokenizer_path = config['token_params']['tokenizer_path']
    logger.info(f"åŠ è½½ Tokenizer ä»: {tokenizer_path}")
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            tokenizer_path,
            trust_remote_code=True,
            cache_dir=config['paths'].get('model_cache_dir') # ä½¿ç”¨ç¼“å­˜è·¯å¾„
        )
    except Exception as e:
        logger.error(f"åŠ è½½ Tokenizer å¤±è´¥: {e}")
        return

    # 2. åŠ è½½åŸºç¡€æ¨¡å‹ (ä½¿ç”¨ config ä¸­çš„è·¯å¾„)
    base_model_path = config['base_model_path']
    logger.info(f"åŠ è½½åŸºç¡€æ¨¡å‹ä»: {base_model_path}")
    try:
        model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            trust_remote_code=True,
            cache_dir=config['paths'].get('model_cache_dir') # ä½¿ç”¨ç¼“å­˜è·¯å¾„
            # torch_dtype=torch.bfloat16
        ).to(device)
    except Exception as e:
        logger.error(f"åŠ è½½åŸºç¡€æ¨¡å‹å¤±è´¥: {e}")
        return

    # 3. é…ç½®å¹¶åº”ç”¨ PEFT (ä½¿ç”¨ config ä¸­çš„ peft_params)
    logger.info("é…ç½® PEFT (LoRA)...")
    peft_cfg = config['peft_params']
    lora_config = LoraConfig(
        r=peft_cfg['lora_r'],
        lora_alpha=peft_cfg['lora_alpha'],
        lora_dropout=peft_cfg['lora_dropout'],
        bias="none",
        task_type=TaskType.CAUSAL_LM,
        target_modules=peft_cfg.get('target_modules') # ä»é…ç½®è¯»å– target_modules
    )
    logger.info("åº”ç”¨ PEFT åˆ°åŸºç¡€æ¨¡å‹...")
    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()

    # 4. åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨ (ä½¿ç”¨ config ä¸­çš„è·¯å¾„å’Œå‚æ•°)
    logger.info("åŠ è½½æ•°æ®é›†...")
    try:
        train_dataset = InstructionDataset(config['train_jsonl'], tokenizer)
        valid_dataset = InstructionDataset(config['valid_jsonl'], tokenizer)
    except FileNotFoundError:
        logger.error("è®­ç»ƒæˆ–éªŒè¯æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥è·¯å¾„ã€‚")
        return

    data_collator = DataCollatorForInstructionTuning(tokenizer, max_length=config['token_params']['max_seq_len'])

    train_cfg = config['training_params']
    train_loader = DataLoader(
        train_dataset,
        batch_size=train_cfg['batch_size'],
        collate_fn=data_collator,
        shuffle=True,
        num_workers=4
    )
    valid_loader = DataLoader(
        valid_dataset,
        batch_size=train_cfg['eval_batch_size'],
        collate_fn=data_collator,
        shuffle=False,
        num_workers=4
    )

    # 5. åˆå§‹åŒ–ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ (ä½¿ç”¨ config ä¸­çš„å‚æ•°)
    optimizer = optim.AdamW(
        model.parameters(),
        lr=train_cfg['lr'],
        weight_decay=train_cfg['weight_decay']
    )

    num_training_steps = len(train_loader) * train_cfg['epochs'] // train_cfg['gradient_accumulation_steps']
    num_warmup_steps = int(num_training_steps * train_cfg['warmup_ratio'])
    scheduler = get_linear_schedule_with_warmup(
        optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps
    )

    # 6. è®­ç»ƒå¾ªç¯ (é€»è¾‘ä¸å˜, ä½¿ç”¨ config ä¸­çš„å‚æ•°)
    best_val_loss = float('inf')
    output_dir = config['output_dir'] # ä½¿ç”¨ config ä¸­çš„è¾“å‡ºè·¯å¾„

    logger.info("å¼€å§‹æŒ‡ä»¤å¾®è°ƒ...")
    for epoch in range(1, train_cfg['epochs'] + 1):
        logger.info(f"--- Epoch {epoch}/{train_cfg['epochs']} ---")

        # (è®­ç»ƒå’Œè¯„ä¼°å‡½æ•°è°ƒç”¨ä¸å˜)
        avg_train_loss = train_instruction_epoch(model, train_loader, optimizer, scheduler, device)
        logger.info(f"Epoch {epoch} è®­ç»ƒå®Œæˆ. å¹³å‡è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}")

        avg_val_loss = evaluate_loss(model, valid_loader, device)
        logger.info(f"Epoch {epoch} è¯„ä¼°å®Œæˆ. å¹³å‡éªŒè¯æŸå¤±: {avg_val_loss:.4f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹ (é€»è¾‘ä¸å˜)
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            logger.info(f"ğŸš€ æ–°çš„æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}ã€‚ä¿å­˜ PEFT adapter åˆ° {output_dir}...")
            model.save_pretrained(output_dir)
            tokenizer.save_pretrained(output_dir)

    logger.info("æŒ‡ä»¤å¾®è°ƒå®Œæˆï¼")
    logger.info(f"æœ€ä½³ PEFT adapter ä¿å­˜åœ¨: {output_dir}")

if __name__ == "__main__":
    main()