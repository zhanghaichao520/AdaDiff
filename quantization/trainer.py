# /quantization/trainer.py (æœ€ç»ˆä¿®æ­£ç‰ˆ - å¤ç”¨ TensorDataset)

import os
import json
import logging
import torch
import numpy as np
from tqdm import tqdm
# âœ… å¯¼å…¥ TensorDataset
from torch.utils.data import DataLoader, Subset, TensorDataset 
from sklearn.model_selection import train_test_split
import utils # å‡è®¾ utils.py åœ¨å¯å¯¼å…¥è·¯å¾„
from collections import defaultdict
# ğŸš¨ (ç§»é™¤) ä¸å†éœ€è¦ MultiModalDataset æˆ– EmbeddingDataset
# from dataset import EmbeddingDataset, MultiModalDataset 

class Trainer:
    """
    é€šç”¨é‡åŒ–å™¨ Trainer (å¤ç”¨ TensorDataset)
    """

    def __init__(self, config: dict, model: torch.nn.Module, device: torch.device):
        self.config = config
        self.model = model.to(device)
        self.device = device
        self.model_name = config.get("model_name", model.__class__.__name__)
        # âœ… ä»æ­£ç¡®çš„ config key è¯»å–æ¨¡å‹é…ç½®
        self.model_cfg = config.get(self.model_name.lower(), {}) 
        self.train_cfg = self.model_cfg.get("training_params", {})
        self.common_cfg = config.get("common", {}) # è·å– common é…ç½®
        self.logger = logging.getLogger(f"Trainer[{self.model_name}]")

    # ====================================================
    # ğŸ”¹ 1. é€šç”¨è¨“ç·´é‚è¼¯ (æ™ºèƒ½èª¿åº¦å…¥å£)
    # ====================================================
    
    # (ç­¾åå·²ä¿®æ­£ï¼šæ¥æ”¶ embeddings_data)
    def fit(self, embeddings_data, ckpt_dir):
        """
        é€šç”¨çš„ fit æ–¹æ³•ï¼Œæ¥æ”¶å®é™…çš„ embedding æ•°æ® (numpy array æˆ– tuple)ã€‚
        """
        # (è°ƒåº¦é€»è¾‘ä¸å˜)
        if self.model_name.lower() in ['opq']: # æ·»åŠ å…¶ä»– one-shot æ¨¡å‹
            return self._fit_one_shot(embeddings_data, ckpt_dir)
        else: # RQVAE, MM_RQVAE, VQVAE, RKMEANS ç­‰
            return self._fit_iterative(embeddings_data, ckpt_dir)

    # ====================================================
    # ğŸ”¹ 1a. å…§éƒ¨æ–¹æ³•ï¼šè¿­ä»£å¼è¨“ç·´ (æ ¸å¿ƒä¿®æ”¹)
    # ====================================================
    
    # (ç­¾åå·²ä¿®æ­£ï¼šæ¥æ”¶ embeddings_data)
    def _fit_iterative(self, embeddings_data, ckpt_dir):
        """å¤„ç†éœ€è¦è¿­ä»£è®­ç»ƒçš„æ¨¡å‹ (æ¥æ”¶ numpy æ•°æ®)ã€‚"""
        self.logger.info(f"å¼€å§‹è¿­ä»£å¼è®­ç»ƒ ({self.model_name})...")

        # âœ… (æ ¸å¿ƒä¿®æ”¹) æ ¹æ® embeddings_data åˆ›å»º TensorDataset
        is_multimodal = isinstance(embeddings_data, tuple)
        dataset = None # åˆå§‹åŒ–
        try:
            if is_multimodal:
                embeddings_T, embeddings_I = embeddings_data
                tensor_T = torch.from_numpy(embeddings_T).float()
                tensor_I = torch.from_numpy(embeddings_I).float()
                dataset = TensorDataset(tensor_T, tensor_I) # ä½¿ç”¨ TensorDataset
                self.logger.info("åˆ›å»º TensorDataset (å¤šæ¨¡æ€ T+I)...")
            else:
                # å•æ¨¡æ€ï¼Œå‡è®¾ embeddings_data æ˜¯ numpy array
                tensor_data = torch.from_numpy(embeddings_data).float()
                dataset = TensorDataset(tensor_data) # åªåŒ…å«ä¸€ä¸ªå¼ é‡
                self.logger.info("åˆ›å»º TensorDataset (å•æ¨¡æ€)...")
        except Exception as e:
            self.logger.error(f"åˆ›å»º Dataset å¤±è´¥: {e}", exc_info=True)
            raise # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œå› ä¸ºæ— æ³•ç»§ç»­

        # (åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†)
        try:
            # ä½¿ç”¨ common config ä¸­çš„ test_size æˆ–é»˜è®¤ 0.05
            test_size = self.common_cfg.get('validation_split', 0.05)
            if test_size > 0:
                 train_idx, val_idx = train_test_split(list(range(len(dataset))), 
                                                       test_size=test_size, 
                                                       random_state=self.common_cfg.get('seed', 42))
                 self.logger.info(f"æ•°æ®é›†å·²åˆ’åˆ†ä¸º {1-test_size:.0%} è®­ç»ƒ / {test_size:.0%} éªŒè¯")
            else:
                 self.logger.info("æœªé…ç½®éªŒè¯é›†åˆ’åˆ† (validation_split <= 0)ï¼Œå°†ä½¿ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒã€‚")
                 train_idx = list(range(len(dataset)))
                 val_idx = []
                 
        except ValueError as e:
            self.logger.warning(f"æ— æ³•åˆ’åˆ†éªŒè¯é›† ({e})ï¼Œå°†ä½¿ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒã€‚")
            train_idx = list(range(len(dataset)))
            val_idx = []

        # (åˆ›å»º DataLoader)
        batch_size = self.train_cfg.get("batch_size", 1024) 
        # ä» common config è·å– num_workers
        num_workers = self.common_cfg.get('num_workers', 0) 
        train_loader = DataLoader(Subset(dataset, train_idx), batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)
        val_loader = DataLoader(Subset(dataset, val_idx), batch_size=batch_size, num_workers=num_workers, pin_memory=True) if val_idx else None
        self.logger.info(f"DataLoader: batch_size={batch_size}, num_workers={num_workers}")
        
        # (ä¼˜åŒ–å™¨é€»è¾‘ä¸å˜)
        params_to_optimize = list(filter(lambda p: p.requires_grad, self.model.parameters()))
        optimizer_name = self.train_cfg.get("optimizer", "AdamW")
        lr = float(self.train_cfg.get("lr", 1e-4))
        wd = float(self.train_cfg.get("weight_decay", 0.0))
        optimizer = None
        if params_to_optimize:
            try:
                optimizer_class = getattr(torch.optim, optimizer_name)
                optimizer = optimizer_class(params_to_optimize, lr=lr, weight_decay=wd)
                self.logger.info(f"ä¼˜åŒ–å™¨: {optimizer_name}, LR: {lr}, WeightDecay: {wd}")
            except AttributeError:
                 self.logger.error(f"æ— æ•ˆçš„ä¼˜åŒ–å™¨åç§°: {optimizer_name}")
                 raise
        else:
            self.logger.info("æ¨¡å‹æ²¡æœ‰å¯è®­ç»ƒå‚æ•°ï¼Œä¸åˆ›å»ºä¼˜åŒ–å™¨ã€‚")

        # (è®­ç»ƒå¾ªç¯é€»è¾‘...)
        best_loss, best_epoch = float("inf"), 0
        num_epochs = self.train_cfg.get("epochs", 100)
        best_path = os.path.join(ckpt_dir, f"{self.model_name}_best.pth")
        os.makedirs(os.path.dirname(best_path), exist_ok=True) 

        pbar = tqdm(range(num_epochs), desc=f"Training {self.model_name}", ncols=120)
        for epoch in pbar:
            self.model.train()
            epoch_loss_sum = defaultdict(float) # ä½¿ç”¨ defaultdict ç®€åŒ–
            # --- è®­ç»ƒå¾ªç¯ ---
            for batch in train_loader:
                loss_dict = {} 
                # âœ… (æ ¸å¿ƒä¿®æ”¹) æ­£ç¡®è§£åŒ… batch
                if is_multimodal:
                    batch_T, batch_I = batch # TensorDataset ä¼šè¿”å›å…ƒç»„
                    batch_T, batch_I = batch_T.to(self.device), batch_I.to(self.device)
                    outputs = self.model(xs_T=batch_T, xs_I=batch_I)
                    loss_dict = self.model.compute_loss(outputs, xs_T=batch_T, xs_I=batch_I)
                else:
                    # å•æ¨¡æ€ TensorDataset è¿”å›åªæœ‰ä¸€ä¸ªå…ƒç´ çš„å…ƒç»„
                    batch_xs = batch[0].to(self.device) 
                    outputs = self.model(xs=batch_xs)
                    # ç¡®ä¿ compute_loss æ¥æ”¶æ­£ç¡®çš„å‚æ•°å (xs)
                    loss_dict = self.model.compute_loss(outputs, xs=batch_xs) 
                
                loss_total = loss_dict.get("loss_total", torch.tensor(0.0, device=self.device))

                if optimizer and hasattr(loss_total, 'requires_grad') and loss_total.requires_grad:
                    optimizer.zero_grad()
                    loss_total.backward()
                    optimizer.step()
                
                for key, val in loss_dict.items():
                    item_val = val.item() if isinstance(val, torch.Tensor) else float(val) 
                    epoch_loss_sum[key] += item_val

            # è®¡ç®—å¹³å‡æŸå¤±
            num_batches = len(train_loader)
            avg_losses = {k: v / num_batches for k, v in epoch_loss_sum.items()}

            # --- éªŒè¯å¾ªç¯ ---
            avg_val_loss = float('inf') 
            if val_loader: 
                self.model.eval()
                val_loss_sum = 0.0
                with torch.no_grad():
                    for batch in val_loader:
                        val_loss_dict = {} 
                        # âœ… (æ ¸å¿ƒä¿®æ”¹) æ­£ç¡®è§£åŒ… batch
                        if is_multimodal:
                            batch_T, batch_I = batch
                            batch_T, batch_I = batch_T.to(self.device), batch_I.to(self.device)
                            outputs = self.model(xs_T=batch_T, xs_I=batch_I)
                            val_loss_dict = self.model.compute_loss(outputs, xs_T=batch_T, xs_I=batch_I)
                        else:
                            batch_xs = batch[0].to(self.device)
                            outputs = self.model(xs=batch_xs)
                            val_loss_dict = self.model.compute_loss(outputs, xs=batch_xs)
                            
                        loss_val = val_loss_dict.get('loss_total', torch.tensor(0.0, device=self.device))
                        val_loss_sum += loss_val.item() if isinstance(loss_val, torch.Tensor) else float(loss_val)
                        
                avg_val_loss = val_loss_sum / len(val_loader) if len(val_loader) > 0 else float('inf')
            
            # (æ›´æ–°è¿›åº¦æ¡å’Œæ—¥å¿—)
            postfix_str = f"TrL={avg_losses.get('loss_total', 0):.4f}"
            if val_loader:
                 postfix_str += f"|VL={avg_val_loss:.4f}"
                 # æ·»åŠ æ›´å¤šæŸå¤±ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                 if 'loss_recon' in avg_losses: postfix_str += f"|Rec={avg_losses['loss_recon']:.4f}"
                 if 'loss_latent' in avg_losses: postfix_str += f"|Lat={avg_losses['loss_latent']:.4f}"
                 if 'loss_recon_T' in avg_losses: postfix_str += f"|RecT={avg_losses['loss_recon_T']:.4f}"
                 if 'loss_recon_I' in avg_losses: postfix_str += f"|RecI={avg_losses['loss_recon_I']:.4f}"
            pbar.set_postfix_str(postfix_str)

            # (ä¿å­˜æœ€ä½³æ¨¡å‹ - æ ¹æ®éªŒè¯é›†æˆ–è®­ç»ƒé›†)
            current_eval_loss = avg_val_loss if val_loader else avg_losses.get('loss_total', float('inf'))
            # å¢åŠ ä¸€ç‚¹å®¹å¿åº¦ï¼Œé˜²æ­¢å› ä¸ºæµ®ç‚¹æ•°ç²¾åº¦é—®é¢˜ä¸ä¿å­˜
            if current_eval_loss < best_loss - 1e-6: 
                best_loss = current_eval_loss
                best_epoch = epoch + 1
                if optimizer: # åªæœ‰å¯è®­ç»ƒçš„æ¨¡å‹æ‰ä¿å­˜
                    try:
                        torch.save(self.model.state_dict(), best_path)
                    except Exception as e:
                         self.logger.error(f"ä¿å­˜æ¨¡å‹å¤±è´¥: {e}", exc_info=True)


        pbar.close()
        self.logger.info("=" * 100)
        self.logger.info(f"ğŸ è¿­ä»£å¼è®­ç»ƒå®Œæˆ [{self.model_name}]")
        if val_loader:
            self.logger.info(f"ğŸ“‰ æœ€ä½³éªŒè¯é›† Loss: {best_loss:.6f} (åœ¨ Epoch {best_epoch})")
        else:
             final_train_loss = avg_losses.get('loss_total', float('inf'))
             self.logger.info(f"ğŸ“‰ æœ€ç»ˆè®­ç»ƒé›† Loss: {final_train_loss:.6f}")
             
        if optimizer: self.logger.info(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: {best_path}")
        self.logger.info("=" * 100)
        
        # å³ä½¿æ²¡æœ‰ä¼˜åŒ–å™¨ï¼ˆå¦‚RKMeansï¼‰ï¼Œä¹Ÿè¿”å›è·¯å¾„ä½œä¸ºå®Œæˆä¿¡å·
        # å¦‚æœæ²¡æœ‰ä¿å­˜ï¼ˆä¾‹å¦‚ epochs=0 æˆ–ä»æœªæ”¹è¿›ï¼‰ï¼Œbest_path å¯èƒ½ä¸å­˜åœ¨ï¼Œè¿”å› None
        return best_path if os.path.exists(best_path) else None 


    # ====================================================
    # ğŸ”¹ 1b. å…§éƒ¨æ–¹æ³•ï¼šä¸€æ¬¡æ€§æ“¬åˆ
    # ====================================================
    
    # (ç­¾åå·²ä¿®æ­£ï¼šæ¥æ”¶ embeddings_data)
    def _fit_one_shot(self, embeddings_data, ckpt_dir: str) -> str:
        """å¤„ç†ä¸€æ¬¡æ€§æ‹Ÿåˆçš„æ¨¡å‹ (æ¥æ”¶ numpy æ•°æ®)ã€‚"""
        self.logger.info(f"å¼€å§‹ one-shot æ‹Ÿåˆ ({self.model_name})...")
        self.model.train() 
        
        full_data_tensor = None
        if isinstance(embeddings_data, tuple):
             self.logger.warning("One-shot æ¨¡å‹æ¥æ”¶åˆ°å¤šæ¨¡æ€è¾“å…¥ï¼Œå°†åªä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡æ€ (æ–‡æœ¬)ã€‚")
             full_data_tensor = torch.from_numpy(embeddings_data[0]).float().to(self.device)
        else:
             full_data_tensor = torch.from_numpy(embeddings_data).float().to(self.device)
        
        # (è°ƒç”¨æ¨¡å‹è¿›è¡Œæ‹Ÿåˆ)
        try:
            if hasattr(self.model, 'fit') and callable(getattr(self.model, 'fit')):
                 self.logger.info("è°ƒç”¨ model.fit()...")
                 self.model.fit(full_data_tensor) 
            else:
                 self.logger.info("è°ƒç”¨ model forward()...")
                 self.model(full_data_tensor) 
        except Exception as e:
            self.logger.error(f"One-shot æ‹Ÿåˆå¤±è´¥: {e}", exc_info=True)
            raise # é‡æ–°æŠ›å‡ºï¼Œè®© main.py çŸ¥é“å¤±è´¥äº†
        
        # ä½¿ç”¨ç©ºæ–‡ä»¶ä½œä¸ºä¿¡å·ï¼Œå› ä¸ºæ¨¡å‹çŠ¶æ€å¯èƒ½åœ¨å†…éƒ¨æˆ–ä¸å¯ä¿å­˜
        fitted_signal_path = os.path.join(ckpt_dir, f"{self.model_name}_fitted.signal")
        os.makedirs(os.path.dirname(fitted_signal_path), exist_ok=True) 
        with open(fitted_signal_path, 'w') as f: f.write('fitted') # å†™å…¥å†…å®¹è¡¨ç¤ºå®Œæˆ

        self.logger.info("=" * 100)
        self.logger.info(f"ğŸ One-shot æ‹Ÿåˆå®Œæˆ [{self.model_name}]")
        self.logger.info(f"ğŸ’¾ æ‹Ÿåˆå®Œæˆä¿¡å·å·²åˆ›å»º: {fitted_signal_path}")
        self.logger.info("=" * 100)
        # æ³¨æ„ï¼šå¯¹äº one-shotï¼Œè¿”å›ä¿¡å·æ–‡ä»¶è·¯å¾„ï¼Œè€Œä¸æ˜¯æ¨¡å‹è·¯å¾„
        return fitted_signal_path 

    # ====================================================
    # ğŸ”¹ 2. é€šç”¨ç¢¼æœ¬ç”Ÿæˆé‚è¼¯ (æ ¸å¿ƒä¿®æ”¹)
    # ====================================================
    
    # (ç­¾åå·²ä¿®æ­£ï¼šæ¥æ”¶ embeddings_data å’Œå®Œæ•´ output_path)
    @torch.no_grad()
    def predict(self, embeddings_data, output_path): 
        """ç”Ÿæˆç æœ¬ (æ¥æ”¶ numpy æ•°æ®)"""
        self.logger.info(f"å¼€å§‹ç”Ÿæˆç æœ¬ ({self.model_name}) -> {output_path}")
        self.model.eval()
        
        # âœ… (æ ¸å¿ƒä¿®æ”¹) æ ¹æ® embeddings_data åˆ›å»º TensorDataset
        is_multimodal = isinstance(embeddings_data, tuple)
        dataset = None
        try:
            if is_multimodal:
                embeddings_T, embeddings_I = embeddings_data
                tensor_T = torch.from_numpy(embeddings_T).float()
                tensor_I = torch.from_numpy(embeddings_I).float()
                dataset = TensorDataset(tensor_T, tensor_I)
            else:
                tensor_data = torch.from_numpy(embeddings_data).float()
                dataset = TensorDataset(tensor_data)
        except Exception as e:
             self.logger.error(f"ä¸º predict åˆ›å»º Dataset å¤±è´¥: {e}", exc_info=True)
             return None # è¿”å› None è¡¨ç¤ºå¤±è´¥
            
        # (åˆ›å»º DataLoader)
        # ä½¿ç”¨ common config ä¸­çš„ predict_batch_size æˆ–é»˜è®¤ 2048
        pred_batch_size = self.common_cfg.get('predict_batch_size', 2048) 
        num_workers = self.common_cfg.get('num_workers', 0)
        loader = DataLoader(dataset, batch_size=pred_batch_size, shuffle=False, num_workers=num_workers)
        
        all_codes = []

        try:
            for batch in tqdm(loader, desc="ç¼–ç ä¸­"):
                codes = None 
                # âœ… (æ ¸å¿ƒä¿®æ”¹) æ­£ç¡®è§£åŒ… batch å¹¶è°ƒç”¨æ¨¡å‹
                if is_multimodal:
                    batch_T, batch_I = batch
                    batch_T, batch_I = batch_T.to(self.device), batch_I.to(self.device)
                    # ä¼˜å…ˆè°ƒç”¨ get_codes
                    if hasattr(self.model, "get_codes"):
                        codes = self.model.get_codes(xs_T=batch_T, xs_I=batch_I)
                    # å¦åˆ™å°è¯• encode + quantizer
                    elif hasattr(self.model, "encode") and hasattr(self.model, "quantizer"): 
                         z_e = self.model.encode(xs_T=batch_T, xs_I=batch_I)
                         _, _, codes = self.model.quantizer(z_e) 
                    else: raise ValueError(f"{self.model_name} ç¼ºå°‘ get_codes æˆ– encode+quantizer æ–¹æ³•")
                else: # å•æ¨¡æ€
                    batch_xs = batch[0].to(self.device)
                    if hasattr(self.model, "get_codes"):
                        codes = self.model.get_codes(xs=batch_xs)
                    elif hasattr(self.model, "encode"):
                         output = self.model.encode(xs=batch_xs)
                         # å…¼å®¹ä¸åŒæ¨¡å‹çš„ encode è¾“å‡º
                         if isinstance(output, torch.Tensor) and output.dtype in [torch.int, torch.long]:
                              codes = output # å‡è®¾ encode ç›´æ¥è¿”å› codes (e.g., OPQ)
                         elif hasattr(self.model, 'quantizer'): 
                              z_e = output
                              _, _, codes = self.model.quantizer(z_e)
                         else: # å°è¯•å°†è¾“å‡ºç›´æ¥ä½œä¸º codes
                              codes = output 
                    else: raise ValueError(f"{self.model_name} ç¼ºå°‘ get_codes/encode æ–¹æ³•")
                
                if codes is not None and isinstance(codes, torch.Tensor):
                     all_codes.append(codes.detach().cpu().numpy().astype(np.int64))
                else:
                     self.logger.warning("æ¨¡å‹æœªè¿”å›æœ‰æ•ˆçš„ codes tensorï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡ã€‚")

        except Exception as e:
             self.logger.error(f"ç”Ÿæˆ codes æ—¶å‡ºé”™: {e}", exc_info=True)
             return None # è¿”å› None è¡¨ç¤ºå¤±è´¥


        if not all_codes:
             self.logger.error("æœªèƒ½ç”Ÿæˆä»»ä½• codesã€‚æ— æ³•ä¿å­˜ç æœ¬ã€‚")
             return None 

        base_codes = np.vstack(all_codes)
        self.logger.info(f"åŸºç¡€ç æœ¬ç”Ÿæˆå®Œæ¯•ï¼Œå½¢çŠ¶: {base_codes.shape}")

        # (æ·»åŠ å»é‡å±‚é€»è¾‘ä¸å˜)
        final_codes = base_codes # é»˜è®¤å€¼
        try: # å°†æŸ¥æ‰¾é…ç½®çš„æ“ä½œæ”¾å…¥ try-except
            # ä½¿ç”¨ .get é¿å… KeyError
            model_specific_cfg = self.config.get(self.model_name.lower(), {})
            model_params = model_specific_cfg.get("model_params", {})
            
            if model_params.get('has_dup_layer', True): # é»˜è®¤ True
                self.logger.info("å°†æ„å»ºå»é‡å±‚ã€‚")
                # å†æ¬¡ä½¿ç”¨ .get è·å– codebook_size
                vocab_size = model_params.get("codebook_size") 
                if vocab_size is None or vocab_size <= 0: 
                    self.logger.error("æ— æ³•è·å–æœ‰æ•ˆçš„ 'codebook_size'ï¼Œæ— æ³•æ„å»ºå»é‡å±‚ã€‚")
                    # é€‰æ‹©æ˜¯ç»§ç»­ï¼ˆæ— å»é‡å±‚ï¼‰è¿˜æ˜¯æŠ¥é”™é€€å‡º
                    # return None # æŠ¥é”™é€€å‡º
                    self.logger.warning("å°†ç»§ç»­ï¼Œä½†ä¸æ·»åŠ å»é‡å±‚ã€‚")
                else:
                    dedup = utils.build_dedup_layer(base_codes, vocab_size)
                    final_codes = np.concatenate([base_codes, dedup], axis=1)
                    self.logger.info(f"æ·»åŠ å»é‡å±‚åç»´åº¦: {final_codes.shape}")
            else:
                self.logger.info("é…ç½®ä¸­ 'has_dup_layer' è®¾ä¸º Falseï¼Œä¸æ„å»ºå»é‡å±‚ã€‚")
                final_codes = base_codes
        except KeyError as e:
             self.logger.error(f"è¯»å–é…ç½®æ—¶å‡ºé”™ (KeyError: {e})ï¼Œæ— æ³•ç¡®å®šæ˜¯å¦æ·»åŠ å»é‡å±‚ã€‚")
             # é€‰æ‹©ç»§ç»­è¿˜æ˜¯æŠ¥é”™
             self.logger.warning("å°†ç»§ç»­ï¼Œä½†ä¸æ·»åŠ å»é‡å±‚ã€‚")
             final_codes = base_codes
        except Exception as e:
             self.logger.error(f"å¤„ç†å»é‡å±‚æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
             self.logger.warning("å°†ç»§ç»­ï¼Œä½†ä¸æ·»åŠ å»é‡å±‚ã€‚")
             final_codes = base_codes


        # (ä¿å­˜é€»è¾‘ä¸å˜ï¼Œä½¿ç”¨ä¼ å…¥çš„ output_path)
        try:
            os.makedirs(os.path.dirname(output_path), exist_ok=True) 
            np.save(output_path, final_codes)
            
            # (å¯é€‰ JSON ä¿å­˜)
            json_path = output_path.replace(".npy", ".codebook.json") 
            json_dict = {str(i): " ".join([f"<L{l}_{v}>" for l, v in enumerate(row)]) 
                         for i, row in enumerate(final_codes)}
            with open(json_path, "w") as f: json.dump(json_dict, f, indent=2)

            self.logger.info(f"âœ… ç æœ¬ä¿å­˜å®Œæˆï¼Œæœ€ç»ˆå½¢çŠ¶: {final_codes.shape}ï¼Œå·²ä¿å­˜è‡³: {output_path} (åŠ .json)")
            return final_codes # è¿”å›ç”Ÿæˆçš„ç æœ¬
        except Exception as e:
             self.logger.error(f"ä¿å­˜ç æœ¬å¤±è´¥: {e}", exc_info=True)
             return None # è¿”å› None è¡¨ç¤ºå¤±è´¥