# /quantization/trainer.py (åš´æ ¼éµå®ˆä½ çš„è¨­ç½®ï¼Œä¸¦åŠ å…¥æ™ºèƒ½èª¿åº¦)

import os
import json
import logging
import torch
import numpy as np
from tqdm import tqdm
from torch.utils.data import DataLoader, Subset
from sklearn.model_selection import train_test_split

# å‡è¨­ä½ çš„ dataset.py å’Œ utils.py éƒ½åœ¨å¯å°å…¥çš„è·¯å¾‘
from dataset import EmbeddingDataset
import utils

class Trainer:
    """
    ğŸŒ Universal Trainer for Quantization Models
    Supports both supervised (RQ-VAE) and unsupervised (RKMEANS, OPQ) paradigms.
    """

    def __init__(self, config: dict, model: torch.nn.Module, device: torch.device):
        self.config = config
        self.model = model.to(device)
        self.device = device
        self.model_name = config.get("model_name", model.__class__.__name__)
        self.model_cfg = config.get(self.model_name, {})
        self.train_cfg = self.model_cfg.get("training_params", {})
        self.logger = logging.getLogger(f"Trainer[{self.model_name}]")

    # ====================================================
    # ğŸ”¹ 1. é€šç”¨è¨“ç·´é‚è¼¯ (æ™ºèƒ½èª¿åº¦å…¥å£)
    # ====================================================
    def fit(self, embeddings_path, ckpt_dir):
        """
        é€šç”¨çš„ fit æ–¹æ³•ï¼Œç¾åœ¨æ˜¯ä¸€å€‹æ™ºèƒ½èª¿åº¦å™¨ã€‚
        å®ƒæœƒæ ¹æ“šæ¨¡å‹åç¨±ï¼Œè‡ªå‹•é¸æ“‡æ­£ç¢ºçš„è¨“ç·´æµç¨‹ã€‚
        """
        # âœ… æ ¸å¿ƒæ”¹å‹•ï¼šåœ¨é€™è£¡é€²è¡Œç°¡å–®ã€ç›´è§€çš„åˆ¤æ–·å’Œåˆ†æ´¾
        # æœªä¾†å¦‚æœæ–°å¢ PQ æ¨¡å‹ï¼Œåªéœ€åœ¨æ­¤åˆ—è¡¨ä¸­åŠ å…¥ 'pq' å³å¯
        if self.model_name in ['opq']:
            return self._fit_one_shot(embeddings_path, ckpt_dir)
        else:
            return self._fit_iterative(embeddings_path, ckpt_dir)

    # ====================================================
    # ğŸ”¹ 1a. å…§éƒ¨æ–¹æ³•ï¼šè¿­ä»£å¼è¨“ç·´ (ä½ åŸä¾†çš„ fit é‚è¼¯)
    # ====================================================
    def _fit_iterative(self, embeddings_path, ckpt_dir):
        """è™•ç†éœ€è¦è¿­ä»£è¨“ç·´çš„æ¨¡å‹ (å¦‚ VQ-VAE, RKMEANS)ã€‚"""
        self.logger.info(f"æª¢æ¸¬åˆ°è¿­ä»£å¼æ¨¡å‹ï¼Œé–‹å§‹è¨“ç·´å¾ªç’°...")

        dataset = EmbeddingDataset(embeddings_path)
        # âœ… å¢åŠ äº†é©—è­‰é›†ä¾†åšæ¨¡å‹é¸æ“‡ï¼Œæ›´ç§‘å­¸
        train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=0.05, random_state=42)
        train_loader = DataLoader(Subset(dataset, train_idx), batch_size=self.train_cfg.get("batch_size", 1024), shuffle=True)
        val_loader = DataLoader(Subset(dataset, val_idx), batch_size=self.train_cfg.get("batch_size", 1024))
        
        # âœ… åªæœ‰åœ¨æ¨¡å‹æœ‰å¯è¨“ç·´åƒæ•¸æ™‚æ‰å‰µå»ºå„ªåŒ–å™¨
        params_to_optimize = list(filter(lambda p: p.requires_grad, self.model.parameters()))
        optimizer = torch.optim.AdamW(params_to_optimize, lr=self.train_cfg.get("lr", 1e-4)) if params_to_optimize else None

        best_loss, best_epoch = float("inf"), 0
        num_epochs = self.train_cfg.get("epochs", 100)
        best_path = os.path.join(ckpt_dir, f"{self.model_name}_best.pth")

        pbar = tqdm(range(num_epochs), desc=f"Training {self.model_name}", ncols=120)
        for epoch in pbar:
            self.model.train()
            epoch_loss_sum = {}
            for batch in train_loader:
                batch = batch.to(self.device)
                outputs = self.model(batch)
                loss_dict = self.model.compute_loss(outputs, batch)
                loss_total = loss_dict.get("loss_total", 0)

                # åƒ…å°å¯å¾®ä¸”æœ‰å„ªåŒ–å™¨çš„æ¨¡å‹åŸ·è¡Œ backward
                if optimizer and hasattr(loss_total, 'requires_grad') and loss_total.requires_grad:
                    optimizer.zero_grad()
                    loss_total.backward()
                    optimizer.step()
                
                for key, val in loss_dict.items():
                    epoch_loss_sum[key] = epoch_loss_sum.get(key, 0.0) + float(val.item())

            avg_losses = {k: v / len(train_loader) for k, v in epoch_loss_sum.items()}

            # âœ… åŸ·è¡Œé©—è­‰
            self.model.eval()
            val_loss = 0
            with torch.no_grad():
                for batch in val_loader:
                    batch = batch.to(self.device)
                    outputs = self.model(batch)
                    val_loss += self.model.compute_loss(outputs, batch).get('loss_total', 0)
            avg_val_loss = val_loss / len(val_loader) if len(val_loader) > 0 else 0
            
            postfix_str = f"train_loss={avg_losses.get('loss_total', 0):.4f}, val_loss={avg_val_loss:.4f}, best_val_loss={best_loss:.4f}"
            pbar.set_postfix_str(postfix_str)

            # âœ… æ ¹æ“šé©—è­‰é›†æå¤±ä¿å­˜æœ€å„ªæ¨¡å‹
            if avg_val_loss < best_loss:
                best_loss, best_epoch = avg_val_loss, epoch + 1
                if optimizer: # åªæœ‰å¯è¨“ç·´çš„æ¨¡å‹æ‰éœ€è¦ä¿å­˜
                    torch.save(self.model.state_dict(), best_path)

        pbar.close()
        self.logger.info("=" * 100)
        self.logger.info(f"ğŸ è¿­ä»£å¼è¨“ç·´å®Œæˆ [{self.model_name}]")
        self.logger.info(f"ğŸ“‰ æœ€ä½³é©—è­‰é›† Loss: {best_loss:.6f} (åœ¨ Epoch {best_epoch})")
        if optimizer: self.logger.info(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³: {best_path}")
        self.logger.info("=" * 100)
        return best_path

    # ====================================================
    # ğŸ”¹ 1b. å…§éƒ¨æ–¹æ³•ï¼šä¸€æ¬¡æ€§æ“¬åˆ
    # ====================================================
    def _fit_one_shot(self, embeddings_path: str, ckpt_dir: str) -> str:
        """è™•ç†ä¸€æ¬¡æ€§æ“¬åˆçš„æ¨¡å‹ (å¦‚ OPQ)ã€‚"""
        self.logger.info(f"æª¢æ¸¬åˆ° one-shot æ¨¡å‹ï¼Œé–‹å§‹ä¸€æ¬¡æ€§æ“¬åˆ...")
        self.model.train()
        
        full_dataset = EmbeddingDataset(embeddings_path)
        full_data_tensor = full_dataset.embeddings.to(self.device)
        
        # ç›´æ¥å°‡å…¨éƒ¨æ•¸æ“šå–‚çµ¦æ¨¡å‹çš„ forward ä¾†è§¸ç™¼æ“¬åˆ
        # å‡è¨­ one-shot æ¨¡å‹çš„ forward æœƒè™•ç†é€™å€‹é‚è¼¯
        self.model(full_data_tensor)
        
        best_path = os.path.join(ckpt_dir, f"{self.model_name}_fitted.pth")
        torch.save({}, best_path) # ä¿å­˜ä¸€å€‹ç©ºå­—å…¸ä½œç‚ºå®Œæˆä¿¡è™Ÿ
        
        self.logger.info("=" * 100)
        self.logger.info(f"ğŸ One-shot æ“¬åˆå®Œæˆ [{self.model_name}]")
        self.logger.info(f"ğŸ’¾ æ“¬åˆå®Œæˆä¿¡è™Ÿå·²ä¿å­˜è‡³: {best_path}")
        self.logger.info("=" * 100)
        return best_path

    # ====================================================
    # ğŸ”¹ 2. é€šç”¨ç¢¼æœ¬ç”Ÿæˆé‚è¼¯ (ä½ çš„åŸç‰ˆ predictï¼Œå®Œå…¨ä¸è®Š)
    # ====================================================
    def predict(self, embeddings_path: str, codebook_dir: str):
        self.logger.info(f"å¼€å§‹ç”Ÿæˆç æœ¬ ({self.model_name}) ...")
        self.model.eval()

        dataset = EmbeddingDataset(embeddings_path)
        # å¢åŠ  batch_size ä»¥åŠ é€Ÿæ¨è«–
        loader = DataLoader(dataset, batch_size=self.train_cfg.get("batch_size", 2048) * 2)
        all_codes = []

        with torch.no_grad():
            for batch in tqdm(loader, desc="ç¼–ç ä¸­"):
                batch = batch.to(self.device)
                if hasattr(self.model, "get_codes"):
                    codes = self.model.get_codes(batch)
                elif hasattr(self.model, "encode"):
                    codes = self.model.encode(batch)
                else:
                    raise ValueError(f"{self.model_name} ç¼ºå°‘ get_codes/encode æ–¹æ³•")
                all_codes.append(codes.detach().cpu().numpy().astype(np.int64))

        base_codes = np.vstack(all_codes)
        self.logger.info(f"åŸºç¤ç¢¼æœ¬ç”Ÿæˆå®Œç•¢ï¼Œå½¢ç‹€: {base_codes.shape}")

        # === âœ… é—œéµæ”¹å‹•ï¼šæ ¹æ“š config æ±ºå®šæ˜¯å¦æ·»åŠ å»é‡å±¤ ===
        model_params = self.model_cfg.get("model_params", {})
        # ä½¿ç”¨ .get('has_dup_layer', True) ç¢ºä¿å¦‚æœ config ä¸­æ²’æœ‰é€™å€‹éµï¼Œé è¨­è¡Œç‚ºæ˜¯æ·»åŠ å»é‡å±¤
        if model_params.get('has_dup_layer', True):
            self.logger.info("é…ç½®ä¸­ 'has_dup_layer' ç‚º True æˆ–æœªè¨­ç½®ï¼Œå°‡æ§‹å»ºå»é‡å±¤ã€‚")
            vocab_size = model_params.get("codebook_size", 1024)
            dedup = utils.build_dedup_layer(base_codes, vocab_size)
            final_codes = np.concatenate([base_codes, dedup], axis=1)
        else:
            self.logger.info("é…ç½®ä¸­ 'has_dup_layer' è¨­ç½®ç‚º Falseï¼Œå°‡ä¸æ§‹å»ºå»é‡å±¤ã€‚")
            final_codes = base_codes
        # =======================================================

        os.makedirs(codebook_dir, exist_ok=True)
        dataset_name = self.config["dataset_name"]
        model_tag = self.model_name.lower()
        # æª”åæ ¼å¼: {dataset_name}.{model_name}.codebook
        prefix = os.path.join(codebook_dir, f"{dataset_name}.{model_tag}.codebook")

        np.save(f"{prefix}.npy", final_codes)
        
        # ä¿å­˜ JSON æ ¼å¼ (å¯é¸)
        json_path = f"{prefix}.json"
        json_dict = {str(i): " ".join([f"<L{l}_{v}>" for l, v in enumerate(row)]) for i, row in enumerate(final_codes)}
        with open(json_path, "w") as f: json.dump(json_dict, f, indent=2)

        self.logger.info(f"âœ… ç æœ¬ä¿å­˜å®Œæˆï¼Œæœ€çµ‚å½¢ç‹€: {final_codes.shape}ï¼Œå·²ä¿å­˜è‡³: {prefix}.(npy/json)")
        return final_codes